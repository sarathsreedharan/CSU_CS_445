{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0663e287",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd256197",
   "metadata": {},
   "source": [
    "*Adapted from various sources, e.g., https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html and https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff3bb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d29b8",
   "metadata": {},
   "source": [
    "## Transformers 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0963a",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7e834",
   "metadata": {},
   "source": [
    "Previously we talked about different types of attention mechanisms.  Attention mechanisms are just neural networks, usually standard feed-forward neural networks, that compute alignment scores between different parts of the input based on metrics of how they match.\n",
    "\n",
    "The simplest one is *dot product attention*, which is just $s_th_i$, the dot product of the hidden state $s$ associated with some element $y_t$ of the output, and the hidden state $h$ associated with some element of the input $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16a8d8",
   "metadata": {},
   "source": [
    "Since dot product attention (and scaled dot product attention) don't depend on a trained weight matrix to adjust the values, we can easily plot what the effect of this will be on toy samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f01e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5584ed90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.19203896, 0.07064719, 0.07064719],\n",
       "        [0.07064719, 0.19203896, 0.07064719],\n",
       "        [0.07064719, 0.07064719, 0.19203896]]),\n",
       " <matplotlib.image.AxesImage at 0x3b13df2e0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "h = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "softmax(s@h),plt.matshow(softmax(s@h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5649e",
   "metadata": {},
   "source": [
    "Consider two languages, English, which has S-V-O word order (\"man bites dog\"), and Irish, which has V-S-O (bites-man-dog) word order.  A trained attention matrix between two equivalent 3-word sentences might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff875a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.07064719, 0.19203896, 0.07064719],\n",
       "        [0.19203896, 0.07064719, 0.07064719],\n",
       "        [0.07064719, 0.07064719, 0.19203896]]),\n",
       " <matplotlib.image.AxesImage at 0x14755ba60>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "h = np.array([[0,1,0],[1,0,0],[0,0,1]])\n",
    "softmax(s@h),plt.matshow(softmax(s@h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfdae6",
   "metadata": {},
   "source": [
    "With some randomness added to the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d28900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.07643087, 0.18161548, 0.08981243],\n",
       "        [0.13736404, 0.11569462, 0.09288636],\n",
       "        [0.08800219, 0.0763772 , 0.14181683]]),\n",
       " <matplotlib.image.AxesImage at 0x38bccb8e0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = np.array([[1,0,0],[0,1,0],[0,0,1]])+np.random.uniform(-.5, .5, size=(3, 3))\n",
    "h = np.array([[0,1,0],[1,0,0],[0,0,1]])+np.random.uniform(-.5, .5, size=(3, 3))\n",
    "softmax(s@h),plt.matshow(softmax(s@h),vmax=np.max(softmax(s@h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669a09d",
   "metadata": {},
   "source": [
    "Using scaled dot product attention instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d61350d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.04412738, 0.10485575, 0.05185323],\n",
       "        [0.07930716, 0.06679632, 0.05362796],\n",
       "        [0.05080809, 0.04409639, 0.08187798]]),\n",
       " <matplotlib.image.AxesImage at 0x38b4722e0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "softmax(s@h)/np.sqrt(3),plt.matshow(softmax(s@h)/np.sqrt(3),vmax=np.max(softmax(s@h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e92c8c",
   "metadata": {},
   "source": [
    "As mentioned, attention mechanisms are a whole category of algorithms.  One subcategory is **self-attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4abda",
   "metadata": {},
   "source": [
    "Given that attention operates over sequences, we can train attention mechanisms to align elements *within* the input sequence by simply setting the input and output sequence to the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de2f9c",
   "metadata": {},
   "source": [
    "In this episode of \"Stupid Machine Learning Tricks\" we will examine how and why this works.  Hey, we can do this with autoencoders.  Why not attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ccdea",
   "metadata": {},
   "source": [
    "a.k.a. **intra-attention**, self-attention relates different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.\n",
    "\n",
    "[This paper](https://arxiv.org/pdf/1601.06733.pdf) uses self-attention in an LSTM for machine reading.\n",
    "\n",
    "<img src=\"images/cheng2016-fig1.png\" width=600>\n",
    "The current word is in red and the size of the blue shade indicates the activation level. (Image source: Cheng et al., 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7388b",
   "metadata": {},
   "source": [
    "## Soft vs. Hard Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158b9ab",
   "metadata": {},
   "source": [
    "Turning again to vision problems, attention can also be applied here.\n",
    "\n",
    "In [this paper](http://proceedings.mlr.press/v37/xuc15.pdf), they apply an attention mechanism to images to generate captions.  Alignments can be computed between image patches and words.  When a trained CNN is passed over the images, filters and patches multiply to create feature maps.  Then a LSTM decoder consumes the feature maps to produce descriptive words one by one, where the weights are learned through attention. The visualization of the attention weights clearly demonstrates which regions of the image the model is paying attention to so as it generates certain words.\n",
    "\n",
    "<img src=\"images/xu2015-fig6b.png\">\n",
    "“A woman is throwing a frisbee in a park.” (Image source: Fig. 6(b) in Xu et al. 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f34ef7b",
   "metadata": {},
   "source": [
    "**Soft** vs. **hard** attention is whether the attention mechanism has access to the whole image or just a patch at a time.\n",
    "\n",
    "In **soft** attention, the alignment weights are learned and placed \"softly\" over all patches in the source image.  The the model is smooth and differentiable, but expensive when the source input is large.\n",
    "\n",
    "In **hard** attention, only one patch of the image is attended to at a time.  This takes less calculation at inference time, but the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de770d3",
   "metadata": {},
   "source": [
    "## Key, Value, Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3c81b",
   "metadata": {},
   "source": [
    "[\"Attention Is All You Need\"](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) (Vaswani, et al., 2017) presented a lot of improvements to soft attention to make it possible to do `seq2seq` modeling *without* recurrent network units. The proposed “transformer” model is entirely built on the self-attention mechanisms without using sequence-aligned recurrent architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f64527",
   "metadata": {},
   "source": [
    "The major component in the transformer is *multi-headed self-attention*. This is a self-attention mechanism in that both source and target are the same sequence, but \"multi-headed\" in that it allows attention to be paid to multiple elements of the source/target sequence for every single element of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028a975",
   "metadata": {},
   "source": [
    "The transformer views the encoded representation of the input as a set of key-value pairs, ($K$,$V$), both of dimension $n$ (input sequence length).  Since this is self-attention, and the input becomes both \"source\" and \"target\", \"source\" and \"target\" become \"key\" and \"value\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d569a",
   "metadata": {},
   "source": [
    "For example, in neural machine translation, both the key and value are the encode hidden states at each timestep.  Therefore key and value at the last time step will be the sentence embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e32950",
   "metadata": {},
   "source": [
    "In the decoder, the output at the previous timestep is compressed into a *query* ($Q$ of dimension $m$) and the next output is produced by mapping this query and the set of keys and values using *scaled dot-product attention*.\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{n}})V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461e24e",
   "metadata": {},
   "source": [
    "Compare to standard scaled dot product attention: $$\\text{Attention}(s,h) = \\text{softmax}(\\frac{s_t^Th_i}{\\sqrt n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba30e71",
   "metadata": {},
   "source": [
    "The current input (encoder hidden state $h_i$) becomes the \"key\" $K$, while the previous output (decoder hidden state $s_t$) becomes the \"query\" $Q$.  This product is scaled and softmaxed then multiplied by the target $V$.  In this way we should be able to capture how much information is relevant to both this input and the output we have so far, and then take that and multiply it by the target.  Because this is self-attention, the target is the same as the source. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09945b7b",
   "metadata": {},
   "source": [
    "The below example shows how to calculate the scaled dot product attention for a sample sentence: `\"The animal didn't cross the street because it was too tired.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0816a040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i = {\"<SOS>\" : 0,\n",
    "       \"<EOS>\" : 1,\n",
    "       \"the\" : 2,\n",
    "       \"animal\" : 3,\n",
    "       \"did\" : 4,\n",
    "       \"n't\" : 5,\n",
    "       \"cross\" : 6,\n",
    "       \"street\" : 7,\n",
    "       \"because\" : 8,\n",
    "       \"it\" : 9,\n",
    "       \"was\" : 10,\n",
    "       \"too\" : 11,\n",
    "       \"tired\" : 12,\n",
    "       \".\" : 13,\n",
    "       \"wide\" : 14\n",
    "      }\n",
    "\n",
    "sentence = \"<SOS> the animal did n't cross the street because it was too tired . <EOS>\"\n",
    "indices = np.array([w2i[word] for word in sentence.split()])\n",
    "\n",
    "x = np.zeros((indices.size, indices.max()+1))\n",
    "x[np.arange(indices.size),indices] = 1\n",
    "y = x\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060eae18",
   "metadata": {},
   "source": [
    "Because the encoder and decoder are neural nets, they will be initialized as weight matrices.  We're not doing any training here so the weights aren't optimized.  You can think of this as calculating the scaled dot product self-attention in the first epoch of training, before any weight update is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "849a7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_encoder_weights = np.random.uniform(-1,1,size=(14,128))\n",
    "dummy_decoder_weights = np.random.uniform(-1,1,size=(128,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c53b20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 15)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = (x@dummy_encoder_weights).T\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49d85434",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = []\n",
    "\n",
    "for t in range(0,len(sentence.split())):\n",
    "    query = x[t]@dummy_decoder_weights.T\n",
    "    attention = softmax(query.T @ keys)\n",
    "    attentions.append(attention * keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eba2bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = np.array(attentions)\n",
    "attentions = np.sum(attentions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06816a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x38aca48e0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6bb171",
   "metadata": {},
   "source": [
    "Attention at this point is fairly random.  After training, the values should be optimized and we should see something much more like this:\n",
    "\n",
    "<img src=\"images/animal-street.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d86c2e",
   "metadata": {},
   "source": [
    "In the first sentence, the attention weights should be optimized to correlate \"it\" and \"animal\" more than \"it\" and street (because animals can get tired, and here and elsewhere in the data, we should find collocations of \"animal\" or animal words and \"tired\").\n",
    "\n",
    "In the second, the opposite is true (because streets can be wide)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d1870",
   "metadata": {},
   "source": [
    "**Speculate**: What might happen if the sentence were `\"The animal didn't cross the street because it was too busy.\"`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce261a",
   "metadata": {},
   "source": [
    "## Multi-Headed Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359fcef",
   "metadata": {},
   "source": [
    "<img src=\"images/multi-head-attention.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a543d3fc",
   "metadata": {},
   "source": [
    "Rather than only computing the attention once, the multi-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c22f0",
   "metadata": {},
   "source": [
    "From Vaswani et al., 2017: \"_multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this._\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6c15c",
   "metadata": {},
   "source": [
    "Anyway, multi-headed attention is calculated as\n",
    "\n",
    "$$\\text{MultiHead}(Q,K,V) = [\\text{head}_1;...;\\text{head}_h]W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$, and $W_i^Q$, $W_i^K$, $W_i^V$, and $W^O$ are learnable weight matrices associated with the query, key, and value at each timestep, and the overall output, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad809d1",
   "metadata": {},
   "source": [
    "## Transfomer Encoder\n",
    "\n",
    "<img src=\"images/transformer-encoder.png\" width=400>\n",
    "\n",
    "The transformer encoder uses a stack of 6 identical encoders. Why 6?\n",
    "\n",
    "🤷‍♂️\n",
    "==\n",
    "\n",
    "It seemed to work best (Vaswani et al., 2017).\n",
    "\n",
    "* Each layer has a multi-headed self-attention layer and a simple position-wise fully connected feed-forward network.\n",
    "* Each sub-layer adopts a residual connection and a layer normalization.\n",
    "* All the sub-layers output $\\mathbb{R}^{512}$ in the original transformer, but dimensionality can vary (e.g., BERT uses $\\mathbb{R}^{768}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecfe6b",
   "metadata": {},
   "source": [
    "## Transformer Decoder\n",
    "\n",
    "<img src=\"images/transformer-decoder.png\" width=400>\n",
    "\n",
    "The decoder is also a stack of 6 identical layers\n",
    "\n",
    "* Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feed-forward network.\n",
    "* Similar to the encoder, each sub-layer adopts a residual connection and a layer normalization.\n",
    "* The first multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don’t want to look into the future of the target sequence when predicting the current position (or do we? more on that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d0044",
   "metadata": {},
   "source": [
    "## Full Architecture\n",
    "\n",
    "<img src=\"images/transformer.png\">\n",
    "\n",
    "* Both the source and target sequences first go through embedding layers to produce representations in $\\mathbb{R}^{512}$.\n",
    "* A softmax and linear layer are added to the final decoder output.\n",
    "* To preserve the position information, a sinusoidal-wave-based positional encoding is applied and summed with the embedding output.\n",
    "\n",
    "The positional encoding represents the position of a word in the input, and therefore the distance between words in the input.  These encodings could be learned, but the AIAYN authors found that learning the encodings added more parameters to the model without being more effective (Transformers are heavy enough!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f990f",
   "metadata": {},
   "source": [
    "How does a sinusoidal wave represent integer positions?  Aren't sine waves periodic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "075a3f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000\n",
      "0001\n",
      "0010\n",
      "0011\n",
      "0100\n",
      "0101\n",
      "0110\n",
      "0111\n",
      "1000\n",
      "1001\n",
      "1010\n",
      "1011\n",
      "1100\n",
      "1101\n",
      "1110\n",
      "1111\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    print(np.binary_repr(i,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63150166",
   "metadata": {},
   "source": [
    "So are binary numbers, but storing these is a waste of space, so we can use a comparable floating point representation, a sinusoidal wave.  Therefore a combination of sinusoidal waves with different periods can represent an arbitrary position.\n",
    "\n",
    "Pick some encoding dimension $d$ that is divisible by 2.  The positional encoding $\\overrightarrow p_t^{(i)}$ will be a function $f :  \\mathbb{N}\\rightarrow \\mathbb{R}^d$ consisting of pairs of sines and cosines for each frequency $1..\\frac{d}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ab4dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(t,d=256,l=12):\n",
    "    assert t < l\n",
    "    d = d\n",
    "    l = l\n",
    "\n",
    "    p_t = np.array([[np.sin((1/(10000**(2*k/d)))*t),np.cos((1/(10000**(2*k/d)))*t)] for k in range(int(d/2))])\n",
    "    \n",
    "    return p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9bf0539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99999021,  0.0044257 ],\n",
       "       [-0.72533058, -0.68840072],\n",
       "       [-0.10065883, -0.994921  ],\n",
       "       [ 0.53162153, -0.84698203],\n",
       "       [ 0.92305248, -0.38467404],\n",
       "       [ 0.98422718,  0.17690917],\n",
       "       [ 0.75785081,  0.65242789],\n",
       "       [ 0.35608506,  0.93445354],\n",
       "       [-0.09727666,  0.99525738],\n",
       "       [-0.50285193,  0.86437257],\n",
       "       [-0.79954816,  0.60060198],\n",
       "       [-0.9631406 ,  0.26899847],\n",
       "       [-0.99728336, -0.07366067],\n",
       "       [-0.92269604, -0.38552824],\n",
       "       [-0.76775034, -0.6407491 ],\n",
       "       [-0.56169782, -0.82734247],\n",
       "       [-0.33057501, -0.94377972],\n",
       "       [-0.09526256, -0.99545218],\n",
       "       [ 0.12897081, -0.99164839],\n",
       "       [ 0.33204073, -0.94326505],\n",
       "       [ 0.50818974, -0.86124514],\n",
       "       [ 0.65500193, -0.7556272 ],\n",
       "       [ 0.77246598, -0.63505615],\n",
       "       [ 0.86217349, -0.50661313],\n",
       "       [ 0.92668158, -0.37584738],\n",
       "       [ 0.96903539, -0.24692188],\n",
       "       [ 0.99242995, -0.12281201],\n",
       "       [ 0.99998478, -0.00551747],\n",
       "       [ 0.99460489,  0.10373579],\n",
       "       [ 0.97890501,  0.20431588],\n",
       "       [ 0.95517791,  0.29603236],\n",
       "       [ 0.92539202,  0.37901136],\n",
       "       [ 0.89120736,  0.45359612],\n",
       "       [ 0.85400183,  0.52027   ],\n",
       "       [ 0.81490238,  0.57959823],\n",
       "       [ 0.77481753,  0.63218494],\n",
       "       [ 0.73446881,  0.67864244],\n",
       "       [ 0.69442007,  0.71956985],\n",
       "       [ 0.65510365,  0.75553902],\n",
       "       [ 0.61684352,  0.78708581],\n",
       "       [ 0.57987517,  0.81470534],\n",
       "       [ 0.54436256,  0.83885005],\n",
       "       [ 0.51041238,  0.85992976],\n",
       "       [ 0.47808597,  0.87831305],\n",
       "       [ 0.44740906,  0.89432943],\n",
       "       [ 0.4183799 ,  0.90827213],\n",
       "       [ 0.3909758 ,  0.92040096],\n",
       "       [ 0.36515837,  0.93094541],\n",
       "       [ 0.34087788,  0.94010759],\n",
       "       [ 0.31807656,  0.94806503],\n",
       "       [ 0.29669138,  0.95497342],\n",
       "       [ 0.2766561 ,  0.96096899],\n",
       "       [ 0.25790296,  0.96617082],\n",
       "       [ 0.24036392,  0.97068284],\n",
       "       [ 0.22397165,  0.97459566],\n",
       "       [ 0.20866022,  0.9779882 ],\n",
       "       [ 0.19436566,  0.98092915],\n",
       "       [ 0.18102629,  0.98347826],\n",
       "       [ 0.168583  ,  0.98568746],\n",
       "       [ 0.1569794 ,  0.98760188],\n",
       "       [ 0.14616187,  0.98926069],\n",
       "       [ 0.13607964,  0.9906979 ],\n",
       "       [ 0.12668469,  0.99194304],\n",
       "       [ 0.11793177,  0.9930217 ],\n",
       "       [ 0.1097783 ,  0.9939561 ],\n",
       "       [ 0.10218426,  0.99476549],\n",
       "       [ 0.09511209,  0.99546657],\n",
       "       [ 0.0885266 ,  0.99607381],\n",
       "       [ 0.08239485,  0.99659976],\n",
       "       [ 0.076686  ,  0.99705529],\n",
       "       [ 0.07137125,  0.99744982],\n",
       "       [ 0.06642366,  0.99779151],\n",
       "       [ 0.06181811,  0.99808743],\n",
       "       [ 0.05753112,  0.99834371],\n",
       "       [ 0.05354081,  0.99856566],\n",
       "       [ 0.04982678,  0.99875787],\n",
       "       [ 0.04636998,  0.99892433],\n",
       "       [ 0.04315268,  0.99906849],\n",
       "       [ 0.04015835,  0.99919333],\n",
       "       [ 0.03737159,  0.99930144],\n",
       "       [ 0.03477804,  0.99939506],\n",
       "       [ 0.03236435,  0.99947614],\n",
       "       [ 0.03011806,  0.99954635],\n",
       "       [ 0.02802759,  0.99960715],\n",
       "       [ 0.02608215,  0.9996598 ],\n",
       "       [ 0.02427169,  0.9997054 ],\n",
       "       [ 0.02258685,  0.99974488],\n",
       "       [ 0.02101893,  0.99977908],\n",
       "       [ 0.01955983,  0.99980869],\n",
       "       [ 0.01820198,  0.99983433],\n",
       "       [ 0.01693838,  0.99985654],\n",
       "       [ 0.01576249,  0.99987576],\n",
       "       [ 0.01466821,  0.99989242],\n",
       "       [ 0.01364989,  0.99990684],\n",
       "       [ 0.01270226,  0.99991932],\n",
       "       [ 0.01182041,  0.99993014],\n",
       "       [ 0.01099978,  0.9999395 ],\n",
       "       [ 0.01023611,  0.99994761],\n",
       "       [ 0.00952546,  0.99995463],\n",
       "       [ 0.00886415,  0.99996071],\n",
       "       [ 0.00824874,  0.99996598],\n",
       "       [ 0.00767606,  0.99997054],\n",
       "       [ 0.00714314,  0.99997449],\n",
       "       [ 0.00664721,  0.99997791],\n",
       "       [ 0.00618572,  0.99998087],\n",
       "       [ 0.00575626,  0.99998343],\n",
       "       [ 0.00535662,  0.99998565],\n",
       "       [ 0.00498472,  0.99998758],\n",
       "       [ 0.00463864,  0.99998924],\n",
       "       [ 0.0043166 ,  0.99999068],\n",
       "       [ 0.0040169 ,  0.99999193],\n",
       "       [ 0.00373802,  0.99999301],\n",
       "       [ 0.0034785 ,  0.99999395],\n",
       "       [ 0.00323699,  0.99999476],\n",
       "       [ 0.00301226,  0.99999546],\n",
       "       [ 0.00280312,  0.99999607],\n",
       "       [ 0.00260851,  0.9999966 ],\n",
       "       [ 0.00242741,  0.99999705],\n",
       "       [ 0.00225888,  0.99999745],\n",
       "       [ 0.00210205,  0.99999779],\n",
       "       [ 0.00195611,  0.99999809],\n",
       "       [ 0.0018203 ,  0.99999834],\n",
       "       [ 0.00169392,  0.99999857],\n",
       "       [ 0.00157631,  0.99999876],\n",
       "       [ 0.00146687,  0.99999892],\n",
       "       [ 0.00136503,  0.99999907],\n",
       "       [ 0.00127026,  0.99999919],\n",
       "       [ 0.00118207,  0.9999993 ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4aa92",
   "metadata": {},
   "source": [
    "So those are the key attention mechanisms and other components used in transformers.  As we now know, attention mechanisms are alignment weights that help an encoder and decoder match parts of sequences to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb3ffb",
   "metadata": {},
   "source": [
    "What does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3be92",
   "metadata": {},
   "source": [
    "## Contextualized Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66273238",
   "metadata": {},
   "source": [
    "We are now familiar with `word2vec` as a way to turn words into fixed-length vectors by training to predict them from their context.  Let's load up some word vectors and examine their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a665d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a,b):\n",
    "    return np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "233ef665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 12.6132 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "print(\"Data loaded in %.4f seconds\" % (time.time()-start_time,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebee85e",
   "metadata": {},
   "source": [
    "Let's look at a word we know has multiple meanings: \"bank\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "276acd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.19726562e-02,  1.34765625e-01, -5.78613281e-02,  5.56640625e-02,\n",
       "        9.91210938e-02, -1.40625000e-01, -3.03649902e-03,  1.87988281e-02,\n",
       "        2.53906250e-01, -4.88281250e-02, -1.63574219e-02, -1.33666992e-02,\n",
       "        6.25000000e-02,  6.07910156e-02, -9.22851562e-02,  3.12500000e-01,\n",
       "        1.38282776e-04, -1.34765625e-01, -4.32128906e-02,  1.16699219e-01,\n",
       "        2.22656250e-01, -9.81445312e-02,  4.51660156e-02, -2.23388672e-02,\n",
       "        5.17578125e-02, -2.41210938e-01, -1.11328125e-01,  9.71679688e-02,\n",
       "        2.28515625e-01, -1.08642578e-02, -4.02832031e-02, -1.83105469e-02,\n",
       "        3.10546875e-01,  3.88183594e-02, -2.85156250e-01, -2.06054688e-01,\n",
       "        3.69140625e-01, -5.24902344e-02,  1.30859375e-01,  1.51367188e-01,\n",
       "        1.59179688e-01, -2.36328125e-01,  7.47070312e-02, -5.54199219e-02,\n",
       "       -8.64257812e-02, -2.28515625e-01,  2.44140625e-03,  8.11767578e-03,\n",
       "       -1.62109375e-01,  1.46484375e-01,  1.40625000e-01, -3.82995605e-03,\n",
       "        1.09375000e-01,  1.14257812e-01, -1.19140625e-01, -7.12890625e-02,\n",
       "       -3.53515625e-01,  2.00195312e-01,  5.76171875e-02, -3.67187500e-01,\n",
       "       -2.42919922e-02, -3.97949219e-02, -1.48437500e-01,  4.61425781e-02,\n",
       "        5.00488281e-03, -1.23535156e-01, -7.32421875e-02,  7.47070312e-02,\n",
       "       -3.54003906e-02,  6.98242188e-02,  8.88671875e-02,  3.41796875e-02,\n",
       "        2.65625000e-01, -2.01416016e-02, -2.49023438e-01,  1.73828125e-01,\n",
       "       -3.44238281e-02,  1.74804688e-01,  2.61718750e-01, -9.03320312e-03,\n",
       "       -1.58203125e-01, -2.02148438e-01,  1.15234375e-01,  2.17773438e-01,\n",
       "       -1.08032227e-02,  1.94335938e-01, -2.87109375e-01, -3.93066406e-02,\n",
       "        5.12695312e-02, -8.88671875e-02,  5.95703125e-02,  1.46484375e-01,\n",
       "        6.93359375e-02, -3.06640625e-01, -1.74804688e-01, -3.63281250e-01,\n",
       "        7.42187500e-02, -3.34472656e-02,  1.60156250e-01,  7.50732422e-03,\n",
       "       -1.55029297e-02, -6.59179688e-02,  3.49121094e-02,  8.78906250e-02,\n",
       "        3.55468750e-01, -1.52343750e-01, -6.78710938e-02, -3.01513672e-02,\n",
       "        1.83593750e-01, -1.85546875e-01, -9.66796875e-02, -1.19140625e-01,\n",
       "       -2.34375000e-01, -1.76757812e-01, -6.29882812e-02,  1.53320312e-01,\n",
       "       -8.31604004e-04,  2.47070312e-01, -1.02233887e-03,  1.77734375e-01,\n",
       "        8.05664062e-02, -1.36718750e-01, -1.19140625e-01,  7.86132812e-02,\n",
       "        1.24511719e-01, -1.78710938e-01,  6.68945312e-02,  5.66406250e-02,\n",
       "       -6.44531250e-02,  2.50244141e-02, -4.66308594e-02, -7.76367188e-02,\n",
       "       -2.40478516e-02, -1.31835938e-01,  4.68750000e-02, -1.23046875e-01,\n",
       "       -2.32421875e-01, -1.94335938e-01, -2.12890625e-01,  4.37011719e-02,\n",
       "        8.05664062e-02, -4.41406250e-01, -1.62353516e-02,  1.71875000e-01,\n",
       "       -1.69921875e-01,  1.20117188e-01,  4.85839844e-02,  2.44140625e-01,\n",
       "        7.81250000e-02,  1.16210938e-01, -2.88085938e-02,  4.80957031e-02,\n",
       "       -1.15234375e-01,  1.03027344e-01, -3.66210938e-02, -5.82885742e-03,\n",
       "       -4.98046875e-02,  1.12792969e-01,  2.41210938e-01,  2.91015625e-01,\n",
       "       -1.33666992e-02,  1.53320312e-01,  1.76757812e-01, -8.17871094e-03,\n",
       "       -1.72851562e-01, -6.39648438e-02,  2.40234375e-01, -7.32421875e-02,\n",
       "       -4.12597656e-02,  3.95507812e-02, -3.22265625e-01,  1.36718750e-01,\n",
       "       -1.38671875e-01, -2.04101562e-01,  7.27539062e-02,  1.03515625e-01,\n",
       "        1.08032227e-02,  7.47070312e-02,  1.80664062e-01, -1.62109375e-01,\n",
       "        4.58984375e-02, -3.19824219e-02, -1.10351562e-01, -3.04687500e-01,\n",
       "       -9.71679688e-02, -3.41796875e-01,  2.18505859e-02,  3.90625000e-01,\n",
       "       -6.44531250e-02,  1.17675781e-01, -2.65625000e-01, -1.69677734e-02,\n",
       "       -2.50000000e-01,  5.46875000e-02, -5.41992188e-02,  1.00097656e-01,\n",
       "       -2.04101562e-01, -1.00097656e-01,  4.41894531e-02, -2.46093750e-01,\n",
       "       -2.83203125e-01,  1.10351562e-01,  8.20312500e-02,  1.07910156e-01,\n",
       "        4.37011719e-02,  2.21679688e-01,  1.04492188e-01,  2.05078125e-01,\n",
       "        4.56542969e-02, -2.87109375e-01,  1.01562500e-01, -9.32617188e-02,\n",
       "        2.55859375e-01,  1.68945312e-01,  6.22558594e-02, -9.17968750e-02,\n",
       "       -1.38671875e-01, -3.46679688e-02, -1.65039062e-01, -1.11328125e-01,\n",
       "        4.98046875e-02,  1.25976562e-01,  6.10351562e-02, -3.88183594e-02,\n",
       "       -1.14257812e-01,  2.67578125e-01, -2.69775391e-02,  1.71875000e-01,\n",
       "       -4.10156250e-02, -7.81250000e-03,  7.91015625e-02, -1.10351562e-01,\n",
       "       -2.03125000e-01, -6.56127930e-03, -1.08886719e-01, -2.27539062e-01,\n",
       "        1.26953125e-01,  2.53906250e-02, -9.61914062e-02,  4.76074219e-02,\n",
       "        1.67968750e-01,  1.81884766e-02, -1.04492188e-01,  7.66601562e-02,\n",
       "        9.57031250e-02, -2.79296875e-01,  2.08984375e-01, -1.52343750e-01,\n",
       "        1.49414062e-01,  2.44140625e-01, -1.57226562e-01, -1.95312500e-01,\n",
       "       -3.80859375e-02,  1.70898438e-01, -6.44531250e-02, -1.07421875e-01,\n",
       "        1.50390625e-01,  1.15722656e-01, -4.37011719e-02,  1.41601562e-01,\n",
       "        3.24707031e-02,  1.92871094e-02,  3.83300781e-02, -2.03857422e-02,\n",
       "       -1.81884766e-02,  4.39453125e-01, -1.67236328e-02, -1.20605469e-01,\n",
       "       -2.73437500e-01, -6.78710938e-02, -1.52343750e-01,  1.41601562e-01,\n",
       "       -5.61523438e-02,  4.95605469e-02, -7.37304688e-02, -3.05175781e-03,\n",
       "        1.82617188e-01, -2.95410156e-02,  4.56542969e-02, -2.19726562e-02,\n",
       "        6.46972656e-03, -3.68652344e-02, -4.68750000e-02,  1.18408203e-02,\n",
       "       -6.88476562e-02, -4.56542969e-02,  1.02996826e-03,  1.00097656e-01,\n",
       "       -1.22558594e-01,  1.68945312e-01, -1.57226562e-01,  7.17773438e-02,\n",
       "       -9.17968750e-02, -3.97949219e-02,  1.08886719e-01, -1.92871094e-02,\n",
       "       -1.45507812e-01, -5.56640625e-02, -3.83300781e-02, -3.18359375e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"bank\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1415a70",
   "metadata": {},
   "source": [
    "A bank is the shore of a river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71e2b65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.06445312e-01,  6.10351562e-02, -5.71289062e-02,  7.42187500e-02,\n",
       "        7.61718750e-02, -2.36328125e-01, -1.14746094e-01, -7.51953125e-02,\n",
       "        7.51953125e-02,  7.03125000e-02,  4.43359375e-01, -1.35742188e-01,\n",
       "        1.23046875e-01,  1.76757812e-01, -6.10351562e-02, -2.69531250e-01,\n",
       "       -1.97265625e-01, -1.98242188e-01,  8.44726562e-02,  5.10253906e-02,\n",
       "       -1.06445312e-01,  8.25195312e-02, -2.04101562e-01,  3.97949219e-02,\n",
       "        3.88671875e-01, -5.11718750e-01, -3.18359375e-01,  7.37304688e-02,\n",
       "        1.43554688e-01, -2.31445312e-01, -2.65625000e-01,  1.66992188e-01,\n",
       "       -8.25195312e-02, -1.53320312e-01, -8.49609375e-02, -2.53906250e-01,\n",
       "        2.09960938e-01,  7.86132812e-02, -5.98144531e-02, -2.08984375e-01,\n",
       "       -4.92187500e-01,  2.41699219e-02,  7.76367188e-02, -1.64062500e-01,\n",
       "       -1.65039062e-01, -2.92968750e-01,  6.16455078e-03,  1.06933594e-01,\n",
       "        2.01171875e-01,  7.08007812e-02,  3.47656250e-01, -1.02539062e-01,\n",
       "        2.50000000e-01, -9.37500000e-02,  1.29882812e-01, -2.08007812e-01,\n",
       "       -6.78710938e-02, -1.08642578e-02,  2.41210938e-01,  2.72216797e-02,\n",
       "        7.03125000e-02,  1.27929688e-01, -1.21582031e-01, -2.91748047e-02,\n",
       "        9.66796875e-02,  1.25000000e-01,  1.55639648e-02, -1.77734375e-01,\n",
       "        9.27734375e-02, -3.97949219e-02, -1.49414062e-01, -6.83593750e-02,\n",
       "        5.51757812e-02, -1.02539062e-01, -1.02539062e-01, -2.55859375e-01,\n",
       "       -2.01171875e-01, -2.53906250e-01,  3.02734375e-01, -3.12500000e-01,\n",
       "        1.85546875e-01,  1.83593750e-01,  1.84570312e-01, -2.13867188e-01,\n",
       "       -6.93359375e-02,  8.88671875e-02,  3.98437500e-01, -1.66992188e-01,\n",
       "        2.71484375e-01,  4.32128906e-02, -3.65234375e-01,  1.88476562e-01,\n",
       "        3.85742188e-02,  4.49218750e-02, -4.34570312e-02, -6.29882812e-02,\n",
       "       -9.03320312e-02, -1.51367188e-01, -6.10351562e-02, -1.76757812e-01,\n",
       "        6.71386719e-04, -7.27539062e-02,  2.96875000e-01,  2.04101562e-01,\n",
       "       -5.68847656e-02,  1.43554688e-01,  1.31835938e-01, -1.62109375e-01,\n",
       "        5.79833984e-03,  1.44531250e-01, -6.12792969e-02,  8.69140625e-02,\n",
       "        8.25195312e-02, -1.85546875e-01,  7.22656250e-02, -2.85156250e-01,\n",
       "        9.91210938e-02,  3.78417969e-02, -4.51171875e-01,  3.35937500e-01,\n",
       "       -1.26342773e-02,  2.01171875e-01, -2.75390625e-01, -1.10351562e-01,\n",
       "        2.87109375e-01, -2.15820312e-01, -2.96875000e-01, -4.24804688e-02,\n",
       "        2.48046875e-01,  1.16699219e-01, -5.54199219e-02, -7.27539062e-02,\n",
       "       -1.76757812e-01, -1.18652344e-01, -2.34375000e-01,  1.40625000e-01,\n",
       "        3.05175781e-02, -2.28515625e-01,  1.51367188e-01,  3.47656250e-01,\n",
       "       -1.17187500e-01,  4.02343750e-01, -5.88989258e-03,  1.82617188e-01,\n",
       "        1.64062500e-01,  1.45507812e-01, -2.09960938e-01,  3.73046875e-01,\n",
       "       -2.05078125e-01, -3.32031250e-01,  2.61718750e-01, -1.01318359e-02,\n",
       "       -1.40625000e-01,  8.78906250e-02, -1.79687500e-01,  5.51757812e-02,\n",
       "        5.81054688e-02, -7.27539062e-02,  9.96093750e-02,  2.37304688e-01,\n",
       "        8.98437500e-02,  4.29687500e-02, -9.70458984e-03, -6.44531250e-02,\n",
       "        1.25976562e-01,  9.33837891e-03,  1.78710938e-01, -2.55859375e-01,\n",
       "        1.94091797e-02, -1.04980469e-01, -2.36328125e-01,  2.36328125e-01,\n",
       "       -7.56835938e-02,  2.58789062e-02, -1.80664062e-01, -3.78906250e-01,\n",
       "        2.03125000e-01,  2.73437500e-01, -8.49609375e-02,  4.54101562e-02,\n",
       "       -1.95312500e-03, -3.06640625e-01,  2.59765625e-01, -2.05078125e-01,\n",
       "        1.78710938e-01, -9.57031250e-02,  2.12402344e-02,  2.08740234e-02,\n",
       "        3.65234375e-01,  3.12500000e-02,  5.66406250e-02, -1.77734375e-01,\n",
       "       -2.32421875e-01, -3.83300781e-02, -2.77099609e-02, -1.53198242e-02,\n",
       "       -2.67578125e-01,  2.05078125e-01, -2.80761719e-03, -1.34765625e-01,\n",
       "        6.93359375e-02,  2.04101562e-01, -3.63281250e-01, -1.46484375e-01,\n",
       "        4.71191406e-02,  8.25195312e-02,  2.96875000e-01,  3.12500000e-02,\n",
       "       -1.42578125e-01,  3.97949219e-02, -3.83300781e-02, -3.30078125e-01,\n",
       "       -1.97265625e-01,  1.25000000e-01, -4.22363281e-02, -1.73828125e-01,\n",
       "       -6.64062500e-02,  4.19921875e-02,  1.41601562e-01,  2.61718750e-01,\n",
       "       -7.32421875e-03,  2.53906250e-01,  6.49414062e-02, -3.95507812e-02,\n",
       "       -5.66406250e-02, -2.75390625e-01, -1.10839844e-01,  3.33984375e-01,\n",
       "        2.13867188e-01,  2.17773438e-01,  1.07910156e-01,  1.67083740e-03,\n",
       "       -8.66699219e-03,  1.54296875e-01,  1.23535156e-01, -1.25976562e-01,\n",
       "        1.77734375e-01,  1.96289062e-01,  2.67578125e-01,  1.82617188e-01,\n",
       "       -7.47070312e-02, -1.40625000e-01,  9.96093750e-02,  7.12890625e-02,\n",
       "        3.22265625e-02, -3.56445312e-02,  2.11914062e-01, -8.78906250e-02,\n",
       "       -1.45507812e-01, -3.51562500e-02, -8.00781250e-02, -1.68945312e-01,\n",
       "       -1.33789062e-01,  3.76953125e-01,  7.93457031e-03, -4.27734375e-01,\n",
       "       -5.37109375e-02,  2.14843750e-02, -3.10546875e-01,  1.03515625e-01,\n",
       "       -2.41210938e-01, -1.25000000e-01,  1.90734863e-04,  9.22851562e-02,\n",
       "        1.38671875e-01,  4.74609375e-01, -3.55468750e-01, -1.47460938e-01,\n",
       "        1.64062500e-01, -3.56445312e-02,  5.88378906e-02, -2.17773438e-01,\n",
       "        4.49218750e-02,  1.89453125e-01,  2.61718750e-01, -1.03515625e-01,\n",
       "        3.36914062e-02, -1.13281250e-01, -6.40869141e-03,  7.66601562e-02,\n",
       "        5.44433594e-02, -2.48046875e-01,  4.60937500e-01,  2.50000000e-01,\n",
       "        1.90429688e-01, -1.75781250e-01, -4.88281250e-01,  2.45117188e-01,\n",
       "        2.02636719e-02,  3.49609375e-01, -4.25781250e-01,  6.05468750e-02,\n",
       "       -2.92968750e-01, -8.88671875e-02, -3.57055664e-03, -1.25000000e-01,\n",
       "       -7.65991211e-03, -2.42187500e-01,  1.87500000e-01, -8.88671875e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"riverbank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3a1243b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12643033"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"bank\"],model[\"riverbank\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e854bc",
   "metadata": {},
   "source": [
    "??? That's not very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f79cb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2152313"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"bank\"],model[\"river\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28829325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26132068"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"bank\"],model[\"money\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f58ade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banks', 0.7440759539604187),\n",
       " ('banking', 0.6901614665985107),\n",
       " ('Bank', 0.6698698401451111),\n",
       " ('lender', 0.634228527545929),\n",
       " ('banker', 0.6092953681945801),\n",
       " ('depositors', 0.6031531691551208),\n",
       " ('mortgage_lender', 0.5797975659370422),\n",
       " ('depositor', 0.5716428160667419),\n",
       " ('BofA', 0.5714625120162964),\n",
       " ('Citibank', 0.5589520931243896)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c9c5f1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45348078"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"bank\"],model[\"Wells_Fargo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d24daef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33500585"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model[\"bank\"],model[\"finance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209202d",
   "metadata": {},
   "source": [
    "So clearly this bank is the other kind of bank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4bea2",
   "metadata": {},
   "source": [
    "**Note**: A *polysemous* word is commonly used for a word with multiple meanings.  Another word for this is a *homonym*.\n",
    "\n",
    "There's a difference.\n",
    "\n",
    "*Polyseme*: Two words with the same spelling, same origin, and different meanings.\n",
    "\n",
    "*Homonym*: Two words with the same spelling, *different* origins, and different meanings.\n",
    "\n",
    "Which one is \"bank\"?\n",
    "\n",
    "The \"riverbank\" sense comes from Germanic, originally meaning \"bench\" or \"flat area.\"  The \"money\" sense was borrowed from Italian via French, so the origins are different, making it a homonym.\n",
    "\n",
    "*Except*: the Italian word *banca* was actually borrowed from German, in the sense of \"moneychanging table\" or... \"bench\" or \"flat thing.\"  Meaning \"bank\" is actually a true polyseme.\n",
    "\n",
    "Transformers and `word2vec` don't know any of this, stupid things..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663df256",
   "metadata": {},
   "source": [
    "We can try a couple of other examples of words with multiple meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34ce351c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weapons', 0.5423978567123413),\n",
       " ('stimulates_innate_immunity', 0.5276414752006531),\n",
       " ('Copacabana_frantically', 0.5087413191795349),\n",
       " ('armaments', 0.4976296126842499),\n",
       " ('doctors_revived_Coltyn', 0.48752522468566895),\n",
       " ('arm', 0.48622867465019226),\n",
       " ('Copacabana_frantically_waving_flags', 0.47894036769866943),\n",
       " ('Arms', 0.47419044375419617),\n",
       " ('cradling_squirming_infant', 0.4572297930717468),\n",
       " ('bowled_shouldering', 0.4543934464454651)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"arms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69022951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barks', 0.6022706031799316),\n",
       " ('Pacific_yew_tree', 0.5659282803535461),\n",
       " ('barking', 0.5538278222084045),\n",
       " ('cork_oak_tree', 0.5402647256851196),\n",
       " ('beetles_burrow', 0.5370648503303528),\n",
       " ('barky', 0.5280892848968506),\n",
       " ('cambium', 0.5276376605033875),\n",
       " ('sapwood', 0.5159558057785034),\n",
       " ('frass', 0.5140876173973083),\n",
       " ('barked', 0.5108721852302551)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"bark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ea1b1",
   "metadata": {},
   "source": [
    "These word vectors were trained from Google News.  Does this distribution make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429072a",
   "metadata": {},
   "source": [
    "Now we're going to get some contextually-dependent vectors using BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29517d63",
   "metadata": {},
   "source": [
    "We will go into details about BERT, its differences from the original transformer architecture, and other transformer variants, in the next lecture, but for now what you need to know is that to compute embeddings, BERT **requires** a sentence as input.  Context is computed using the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0849bb",
   "metadata": {},
   "source": [
    "<img src=\"images/firth.png\" width=200>\n",
    "<img src=\"images/wittgenstein.jpg\" width=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3cbab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cell, run necessary BERT installs here with !pip install ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4391d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f126bc6",
   "metadata": {},
   "source": [
    "First we need to tokenize all inputs.  Let's see how BERT tokenizes a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4165b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\" # more about the [CLS] and [SEP] tokens next time\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1288d3",
   "metadata": {},
   "source": [
    "This is subword tokenization.  Why does BERT do it this way?  The BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fits our language data. Since the vocabulary limit size of our BERT tokenizer model is 30,000, the WordPiece model generated a vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on. So \"embeddings\" isn't in that top 30K, but \"em\" (e.g., \"go get 'em\") is, as is \"bed\", and I guess \"ding,\" and the single character \"s\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372139e3",
   "metadata": {},
   "source": [
    "Remember stupid machine learning tricks?  IMO this is an objectively dumb way to break up the word \"embeddings,\" but I guess it works, and that's very frustrating to me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e614dd",
   "metadata": {},
   "source": [
    "Here are some examples of the tokens contained in our vocabulary. Tokens beginning with two hashes are subwords or individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30ad2dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e402d3",
   "metadata": {},
   "source": [
    "After breaking the text into tokens, we then have to convert the words in the sentence to vocabulary indeces.\n",
    "\n",
    "From here on, we’ll use the example sentence below, which contains two instances of the word “bank” with different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a3b8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6680154",
   "metadata": {},
   "source": [
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). Since this example uses only a single sentence, every index will be marked with 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0dfcc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6463730",
   "metadata": {},
   "source": [
    "We will now convert everything to tensors and load the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "122aa5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "595c1835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df1de2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0367a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 22\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee40f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Type of hidden_states:  <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
     ]
    }
   ],
   "source": [
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "892e3021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 22, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f236f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 22, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebfbf542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 13, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183254a",
   "metadata": {},
   "source": [
    "Now we have 22 tokens, representations for each in the 13 layers, each of which consist of 768 dimensions.  Now we have a way to extract a representation in $\\mathbb{R}^{768}$ for each token, from each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48c4d269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42db97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cf5d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f74f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "006db183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70beffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    tensor([ 3.3596, -2.9805, -1.5421,  0.7065,  2.0031])\n",
      "bank robber   tensor([ 2.7359, -2.5577, -1.3094,  0.6797,  1.6633])\n",
      "river bank    tensor([ 1.5266, -0.8895, -0.5152, -0.9298,  2.8334])\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs_sum[19][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f35a3",
   "metadata": {},
   "source": [
    "Those look different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80e00142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for *similar* meanings:\t0.94\n",
      "Vector similarity for *different* meanings:\t0.70\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = cos_sim(token_vecs_sum[6], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = cos_sim(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for *similar* meanings:\\t%.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:\\t%.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1dd50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
