{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarathsreedharan/CSU_CS_445/blob/main/Spring25/Notebooks/module08-03-transformers-contd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "728f6247",
      "metadata": {
        "id": "728f6247"
      },
      "source": [
        "# Transformers: Revenge of the Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bec33c6",
      "metadata": {
        "id": "0bec33c6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53190bc",
      "metadata": {
        "id": "f53190bc"
      },
      "source": [
        "## Properties of Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b4ef78",
      "metadata": {
        "id": "22b4ef78"
      },
      "source": [
        "Multi-headed self-attention is the core modeling component of Transformers.\n",
        "\n",
        "Recall that attention can be viewed as an operation on a query $q\\in\\mathbb{R}^d$, a set of value vectors $\\{v_1,\\ldots,v_n\\}$, $v_i\\in\\mathbb{R}^d$, and a set of key vectors $\\{k_1,\\ldots,k_n\\}$, $k_i\\in\\mathbb{R}^d$, specified as follows:\n",
        "\\begin{equation}\n",
        "c=\\sum_{i=1}^n v_i\\alpha_i\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\alpha_i=\\dfrac{\\exp(k_i^\\top q)}{\\sum_{j=1}^n\\exp(k_j^\\top q)},\n",
        "\\end{equation}\n",
        "where $\\alpha_i$ are frequently called the \"attention weights\", and the output $c\\in\\mathbb{R}^d$ is a correspondingly weighted average over the value vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99093ba1",
      "metadata": {
        "id": "99093ba1"
      },
      "source": [
        "It's particularly simple for attention to \"copy\" a value vector to the output $c$.\n",
        "\n",
        "For $c$ to be approximately equal to $v_j$, $k_j^\\top q$ should be relatively large, which can happen when $k_j$ and/or $q$ are relatively large and point in roughly the same direction, while the other $k_i$ are roughly orthogonal to $q$. (Trivially, if all of the $v_i$ are approximately equal to each other, then so will $c$ be.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540c4948",
      "metadata": {
        "id": "540c4948",
        "outputId": "dcf85dfc-fbc4-4056-eee9-037de3d6bde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.21163017 0.57640222 0.21196761] [[ 0.00226544 -0.03356713  0.21980202]\n",
            " [-0.07147272 -0.31360029 -0.92967035]\n",
            " [ 0.22847857  0.6739347  -0.83983578]] [ 0.00771246 -0.04501139 -0.6673653 ] False\n"
          ]
        }
      ],
      "source": [
        "# toy example with orthogonal K vectors and equal-length query\n",
        "V = np.random.uniform(-1,1,size=(3,3))\n",
        "K = np.array([[1,0,0],\n",
        "              [0,1,0],\n",
        "              [0,0,1]])\n",
        "q = np.array([0,1,0]) + np.random.uniform(-.01,.01,size=(3,))\n",
        "\n",
        "alpha = np.exp(K.T @ q)/np.sum(np.exp(K.T @ q))\n",
        "print(alpha,V,V.T @ alpha,np.allclose(V.T @ alpha,V[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47720efc",
      "metadata": {
        "id": "47720efc",
        "outputId": "a386ba4b-5c1c-49e5-9a31-56be00b1a16b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2.23430114e-09 9.99999996e-01 2.23784453e-09] [[ 0.08852328 -0.2962662  -0.87455059]\n",
            " [-0.27916976 -0.82515604 -0.68182753]\n",
            " [ 0.73153863 -0.89563775 -0.74404042]] [-0.27916975 -0.82515604 -0.68182753] True\n"
          ]
        }
      ],
      "source": [
        "# make one key large\n",
        "V = np.random.uniform(-1,1,size=(3,3))\n",
        "K = np.array([[1,0,0],\n",
        "              [0,20,0],\n",
        "              [0,0,1]])\n",
        "q = np.array([0,1,0]) + np.random.uniform(-.01,.01,size=(3,))\n",
        "\n",
        "alpha = np.exp(K.T @ q)/np.sum(np.exp(K.T @ q))\n",
        "print(alpha,V,V.T @ alpha,np.allclose(V.T @ alpha,V[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99441dbd",
      "metadata": {
        "id": "99441dbd",
        "outputId": "5f6ee6c9-20f8-4810-d85a-0b2049f0e81b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9.99999996e-01 1.86938365e-09 1.86341842e-09] [[ 0.35185978 -0.71377887  0.90807102]\n",
            " [ 0.73349967  0.63372146 -0.05139053]\n",
            " [ 0.8285758  -0.74065813  0.9394479 ]] [ 0.35185978 -0.71377887  0.90807102] True\n"
          ]
        }
      ],
      "source": [
        "# different key\n",
        "V = np.random.uniform(-1,1,size=(3,3))\n",
        "K = np.array([[20,0,0],\n",
        "              [0,1,0],\n",
        "              [0,0,1]])\n",
        "q = np.array([1,0,0]) + np.random.uniform(-.01,.01,size=(3,))\n",
        "\n",
        "alpha = np.exp(K.T @ q)/np.sum(np.exp(K.T @ q))\n",
        "print(alpha,V,V.T @ alpha,np.allclose(V.T @ alpha,V[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce1e2ac",
      "metadata": {
        "id": "cce1e2ac"
      },
      "source": [
        "What would this mean (choose all that apply)?\n",
        "1. The query vector is ignoring certain elements of the input.\n",
        "2. The attention weights are evenly distributed across the input.\n",
        "3. One particular element of the input carries almost all the relevant information (as judged by the network)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f86ddcf",
      "metadata": {
        "id": "2f86ddcf"
      },
      "source": [
        "Consider a set of key vectors $\\{k_1,\\ldots,k_n\\}$ where all key vectors are perpendicular, that is, $k_i\\perp k_j$ for all $i\\neq j$. Let $||k_i||=1$ for all $i$. Let $\\{v_1,\\ldots,v_n\\}$ be a set of arbitrary value vectors.\n",
        "\n",
        "Let $v_a,v_b\\in\\{v_1,\\ldots,v_n\\}$ be two of the value vectors, and let the corresponding key vectors of $v_a$ and $v_b$ be $k_a$ and $k_b$.\n",
        "\n",
        "Give an expression for a query vector $q$ such that the output $c$ is approximately equal to the average of $v_a$ and $v_b$, that is, $\\frac{1}{2}(v_a+v_b)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55371bb6",
      "metadata": {
        "id": "55371bb6",
        "outputId": "9e90e287-5177-4660-c030-1e43774266b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 [0.3174877  0.44397629 0.21328666] [0.19246124 0.61697466 0.18604583] False\n",
            "2 [0.24346485 0.54640126 0.19715853] [0.19246124 0.61697466 0.18604583] False\n",
            "3 [0.2120075  0.5899286  0.19030458] [0.19246124 0.61697466 0.18604583] False\n",
            "4 [0.19976403 0.60686983 0.18763697] [0.19246124 0.61697466 0.18604583] False\n",
            "5 [0.19516329 0.61323585 0.18663456] [0.19246124 0.61697466 0.18604583] False\n",
            "6 [0.19345738 0.6155963  0.18626287] [0.19246124 0.61697466 0.18604583] False\n",
            "7 [0.19282798 0.61646719 0.18612574] [0.19246124 0.61697466 0.18604583] False\n",
            "8 [0.1925962  0.61678792 0.18607524] [0.19246124 0.61697466 0.18604583] False\n",
            "9 [0.19251089 0.61690595 0.18605665] [0.19246124 0.61697466 0.18604583] False\n",
            "10 [0.1924795  0.61694938 0.18604981] [0.19246124 0.61697466 0.18604583] False\n",
            "11 [0.19246796 0.61696536 0.1860473 ] [0.19246124 0.61697466 0.18604583] False\n",
            "12 [0.19246371 0.61697124 0.18604637] [0.19246124 0.61697466 0.18604583] False\n",
            "13 [0.19246215 0.6169734  0.18604603] [0.19246124 0.61697466 0.18604583] True\n",
            "14 [0.19246157 0.61697419 0.18604591] [0.19246124 0.61697466 0.18604583] True\n",
            "15 [0.19246136 0.61697449 0.18604586] [0.19246124 0.61697466 0.18604583] True\n",
            "16 [0.19246128 0.61697459 0.18604584] [0.19246124 0.61697466 0.18604583] True\n",
            "17 [0.19246125 0.61697463 0.18604584] [0.19246124 0.61697466 0.18604583] True\n",
            "18 [0.19246124 0.61697465 0.18604583] [0.19246124 0.61697466 0.18604583] True\n",
            "19 [0.19246124 0.61697465 0.18604583] [0.19246124 0.61697466 0.18604583] True\n",
            "20 [0.19246124 0.61697466 0.18604583] [0.19246124 0.61697466 0.18604583] True\n"
          ]
        }
      ],
      "source": [
        "V = np.random.uniform(-1,1,size=(3,3))\n",
        "K = np.array([[1,0,0],\n",
        "              [0,1,0],\n",
        "              [0,0,1]])\n",
        "\n",
        "for beta in range(1,21):\n",
        "    q = beta * (K[0]+K[1])\n",
        "    alpha = np.exp(K.T @ q)/np.sum(np.exp(K.T @ q))\n",
        "    print(beta, V.T @ alpha, .5 * (V[0]+V[1]), np.allclose(V.T @ alpha, .5 * (V[0]+V[1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5042cc39",
      "metadata": {
        "id": "5042cc39"
      },
      "source": [
        "* Any expression $q=\\beta(k_a+k_b)$, for large $\\beta$, will work here.\n",
        "* While the softmax function will never *exactly* average the two vectors, you can get close by using a large scalar multiple in the expression."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b245ff",
      "metadata": {
        "id": "90b245ff"
      },
      "source": [
        "It is *possible* for a single-headed attention mechanism to focus equally on two values. The same concept could easily be extended to any subset of values.\n",
        "\n",
        "Consider a set of key vectors $\\{k_1,\\ldots,k_n\\}$ that are now randomly sampled, $k_i\\sim\\mathcal{N}(\\mu_i,\\Sigma_i)$, where the means $\\mu_i$ are known to you, but the covariances $\\Sigma_i$ are unknown. Further, assume that the means $\\mu_i$ are all perpendicular; $\\mu_i^\\top\\mu_j=0$ if $i\\neq j$, and unit norm, $||\\mu_i||=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5aa952",
      "metadata": {
        "id": "1d5aa952"
      },
      "source": [
        "Assume that the covariance matrices are $\\Sigma_i=\\alpha I$, for vanishingly small $\\alpha$. ($I$ is the identity matrix.) Let us design a query $q$ in terms of the $\\mu_i$ such that as before, $c\\approx\\frac{1}{2}(v_a+v_b)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5686bed",
      "metadata": {
        "id": "a5686bed",
        "outputId": "9762dcc4-2446-43cb-87f7-18a2d9aadaf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.00000216 1.00000296 0.99999714]\n",
            " [0.99999743 0.99999771 0.99999975]\n",
            " [0.99999855 0.99999994 0.99999676]]\n",
            "[[0.99999938 0.         0.        ]\n",
            " [0.         1.00000021 0.        ]\n",
            " [0.         0.         0.99999788]]\n",
            "1 [ 0.32500814  0.20712639 -0.11239126] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "2 [ 0.32479616  0.25045662 -0.2157991 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "3 [ 0.32470632  0.26887076 -0.25974383] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "4 [ 0.32467162  0.27603787 -0.27684717] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "5 [ 0.32465883  0.27873123 -0.28327377] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "6 [ 0.32465435  0.27973006 -0.28565635] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "7 [ 0.32465295  0.28009874 -0.28653507] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "8 [ 0.3246527   0.28023469 -0.28685833] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "9 [ 0.32465286  0.28028488 -0.28697696] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "10 [ 0.32465318  0.28030351 -0.28702027] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "11 [ 0.32465356  0.28031054 -0.28703587] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "12 [ 0.32465395  0.28031328 -0.28704126] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "13 [ 0.32465436  0.28031446 -0.28704291] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "14 [ 0.32465476  0.28031506 -0.28704318] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "15 [ 0.32465517  0.28031544 -0.28704294] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "16 [ 0.32465558  0.28031575 -0.28704251] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "17 [ 0.32465599  0.28031602 -0.28704201] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "18 [ 0.3246564   0.28031629 -0.28704149] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "19 [ 0.32465681  0.28031655 -0.28704096] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "20 [ 0.32465721  0.28031682 -0.28704042] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "21 [ 0.32465762  0.28031708 -0.28703989] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "22 [ 0.32465803  0.28031734 -0.28703935] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "23 [ 0.32465844  0.2803176  -0.28703881] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "24 [ 0.32465885  0.28031786 -0.28703828] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "25 [ 0.32465926  0.28031812 -0.28703774] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "26 [ 0.32465967  0.28031838 -0.2870372 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "27 [ 0.32466008  0.28031864 -0.28703666] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "28 [ 0.32466048  0.2803189  -0.28703613] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "29 [ 0.32466089  0.28031917 -0.28703559] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "30 [ 0.3246613   0.28031943 -0.28703505] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "31 [ 0.32466171  0.28031969 -0.28703452] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "32 [ 0.32466212  0.28031995 -0.28703398] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "33 [ 0.32466253  0.28032021 -0.28703344] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "34 [ 0.32466294  0.28032047 -0.28703291] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "35 [ 0.32466335  0.28032073 -0.28703237] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "36 [ 0.32466375  0.28032099 -0.28703183] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "37 [ 0.32466416  0.28032125 -0.28703129] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "38 [ 0.32466457  0.28032151 -0.28703076] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "39 [ 0.32466498  0.28032178 -0.28703022] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "40 [ 0.32466539  0.28032204 -0.28702968] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "41 [ 0.3246658   0.2803223  -0.28702915] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "42 [ 0.32466621  0.28032256 -0.28702861] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "43 [ 0.32466661  0.28032282 -0.28702807] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "44 [ 0.32466702  0.28032308 -0.28702754] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "45 [ 0.32466743  0.28032334 -0.287027  ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "46 [ 0.32466784  0.2803236  -0.28702646] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "47 [ 0.32466825  0.28032386 -0.28702592] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "48 [ 0.32466866  0.28032412 -0.28702539] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "49 [ 0.32466907  0.28032438 -0.28702485] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "50 [ 0.32466948  0.28032465 -0.28702431] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "51 [ 0.32466988  0.28032491 -0.28702378] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "52 [ 0.32467029  0.28032517 -0.28702324] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "53 [ 0.3246707   0.28032543 -0.2870227 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "54 [ 0.32467111  0.28032569 -0.28702217] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "55 [ 0.32467152  0.28032595 -0.28702163] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "56 [ 0.32467193  0.28032621 -0.28702109] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "57 [ 0.32467234  0.28032647 -0.28702056] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "58 [ 0.32467274  0.28032673 -0.28702002] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "59 [ 0.32467315  0.28032699 -0.28701948] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "60 [ 0.32467356  0.28032725 -0.28701894] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "61 [ 0.32467397  0.28032752 -0.28701841] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "62 [ 0.32467438  0.28032778 -0.28701787] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "63 [ 0.32467479  0.28032804 -0.28701733] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "64 [ 0.3246752  0.2803283 -0.2870168] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "65 [ 0.32467561  0.28032856 -0.28701626] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "66 [ 0.32467601  0.28032882 -0.28701572] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "67 [ 0.32467642  0.28032908 -0.28701519] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "68 [ 0.32467683  0.28032934 -0.28701465] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "69 [ 0.32467724  0.2803296  -0.28701411] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "70 [ 0.32467765  0.28032986 -0.28701357] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "71 [ 0.32467806  0.28033013 -0.28701304] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "72 [ 0.32467847  0.28033039 -0.2870125 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "73 [ 0.32467888  0.28033065 -0.28701196] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "74 [ 0.32467928  0.28033091 -0.28701143] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "75 [ 0.32467969  0.28033117 -0.28701089] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "76 [ 0.3246801   0.28033143 -0.28701035] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "77 [ 0.32468051  0.28033169 -0.28700982] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "78 [ 0.32468092  0.28033195 -0.28700928] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "79 [ 0.32468133  0.28033221 -0.28700874] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "80 [ 0.32468174  0.28033247 -0.28700821] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "81 [ 0.32468214  0.28033273 -0.28700767] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "82 [ 0.32468255  0.280333   -0.28700713] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "83 [ 0.32468296  0.28033326 -0.28700659] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "84 [ 0.32468337  0.28033352 -0.28700606] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "85 [ 0.32468378  0.28033378 -0.28700552] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "86 [ 0.32468419  0.28033404 -0.28700498] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "87 [ 0.3246846   0.2803343  -0.28700445] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "88 [ 0.32468501  0.28033456 -0.28700391] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "89 [ 0.32468541  0.28033482 -0.28700337] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "90 [ 0.32468582  0.28033508 -0.28700284] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "91 [ 0.32468623  0.28033534 -0.2870023 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "92 [ 0.32468664  0.2803356  -0.28700176] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "93 [ 0.32468705  0.28033587 -0.28700122] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "94 [ 0.32468746  0.28033613 -0.28700069] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "95 [ 0.32468787  0.28033639 -0.28700015] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "96 [ 0.32468827  0.28033665 -0.28699961] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "97 [ 0.32468868  0.28033691 -0.28699908] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "98 [ 0.32468909  0.28033717 -0.28699854] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "99 [ 0.3246895   0.28033743 -0.286998  ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "100 [ 0.32468991  0.28033769 -0.28699747] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "101 [ 0.32469032  0.28033795 -0.28699693] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "102 [ 0.32469073  0.28033821 -0.28699639] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "103 [ 0.32469114  0.28033848 -0.28699586] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "104 [ 0.32469154  0.28033874 -0.28699532] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "105 [ 0.32469195  0.280339   -0.28699478] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "106 [ 0.32469236  0.28033926 -0.28699424] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "107 [ 0.32469277  0.28033952 -0.28699371] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "108 [ 0.32469318  0.28033978 -0.28699317] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "109 [ 0.32469359  0.28034004 -0.28699263] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "110 [ 0.324694   0.2803403 -0.2869921] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "111 [ 0.32469441  0.28034056 -0.28699156] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "112 [ 0.32469481  0.28034082 -0.28699102] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "113 [ 0.32469522  0.28034108 -0.28699049] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "114 [ 0.32469563  0.28034135 -0.28698995] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "115 [ 0.32469604  0.28034161 -0.28698941] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "116 [ 0.32469645  0.28034187 -0.28698887] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "117 [ 0.32469686  0.28034213 -0.28698834] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "118 [ 0.32469727  0.28034239 -0.2869878 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "119 [ 0.32469767  0.28034265 -0.28698726] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "120 [ 0.32469808  0.28034291 -0.28698673] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "121 [ 0.32469849  0.28034317 -0.28698619] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "122 [ 0.3246989   0.28034343 -0.28698565] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "123 [ 0.32469931  0.28034369 -0.28698512] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "124 [ 0.32469972  0.28034395 -0.28698458] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "125 [ 0.32470013  0.28034422 -0.28698404] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "126 [ 0.32470054  0.28034448 -0.2869835 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "127 [ 0.32470094  0.28034474 -0.28698297] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "128 [ 0.32470135  0.280345   -0.28698243] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "129 [ 0.32470176  0.28034526 -0.28698189] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "130 [ 0.32470217  0.28034552 -0.28698136] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "131 [ 0.32470258  0.28034578 -0.28698082] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "132 [ 0.32470299  0.28034604 -0.28698028] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "133 [ 0.3247034   0.2803463  -0.28697975] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "134 [ 0.3247038   0.28034656 -0.28697921] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "135 [ 0.32470421  0.28034683 -0.28697867] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "136 [ 0.32470462  0.28034709 -0.28697814] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "137 [ 0.32470503  0.28034735 -0.2869776 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "138 [ 0.32470544  0.28034761 -0.28697706] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "139 [ 0.32470585  0.28034787 -0.28697652] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "140 [ 0.32470626  0.28034813 -0.28697599] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "141 [ 0.32470667  0.28034839 -0.28697545] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "142 [ 0.32470707  0.28034865 -0.28697491] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "143 [ 0.32470748  0.28034891 -0.28697438] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "144 [ 0.32470789  0.28034917 -0.28697384] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "145 [ 0.3247083   0.28034943 -0.2869733 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "146 [ 0.32470871  0.2803497  -0.28697277] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "147 [ 0.32470912  0.28034996 -0.28697223] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "148 [ 0.32470953  0.28035022 -0.28697169] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "149 [ 0.32470994  0.28035048 -0.28697115] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "150 [ 0.32471034  0.28035074 -0.28697062] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "151 [ 0.32471075  0.280351   -0.28697008] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "152 [ 0.32471116  0.28035126 -0.28696954] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "153 [ 0.32471157  0.28035152 -0.28696901] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "154 [ 0.32471198  0.28035178 -0.28696847] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "155 [ 0.32471239  0.28035204 -0.28696793] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "156 [ 0.3247128  0.2803523 -0.2869674] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "157 [ 0.3247132   0.28035257 -0.28696686] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "158 [ 0.32471361  0.28035283 -0.28696632] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "159 [ 0.32471402  0.28035309 -0.28696579] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "160 [ 0.32471443  0.28035335 -0.28696525] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "161 [ 0.32471484  0.28035361 -0.28696471] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "162 [ 0.32471525  0.28035387 -0.28696417] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "163 [ 0.32471566  0.28035413 -0.28696364] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "164 [ 0.32471607  0.28035439 -0.2869631 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "165 [ 0.32471647  0.28035465 -0.28696256] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "166 [ 0.32471688  0.28035491 -0.28696203] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "167 [ 0.32471729  0.28035518 -0.28696149] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "168 [ 0.3247177   0.28035544 -0.28696095] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "169 [ 0.32471811  0.2803557  -0.28696042] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "170 [ 0.32471852  0.28035596 -0.28695988] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "171 [ 0.32471893  0.28035622 -0.28695934] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "172 [ 0.32471934  0.28035648 -0.2869588 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "173 [ 0.32471974  0.28035674 -0.28695827] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "174 [ 0.32472015  0.280357   -0.28695773] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "175 [ 0.32472056  0.28035726 -0.28695719] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "176 [ 0.32472097  0.28035752 -0.28695666] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "177 [ 0.32472138  0.28035778 -0.28695612] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "178 [ 0.32472179  0.28035805 -0.28695558] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "179 [ 0.3247222   0.28035831 -0.28695505] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "180 [ 0.3247226   0.28035857 -0.28695451] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "181 [ 0.32472301  0.28035883 -0.28695397] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "182 [ 0.32472342  0.28035909 -0.28695344] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "183 [ 0.32472383  0.28035935 -0.2869529 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "184 [ 0.32472424  0.28035961 -0.28695236] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "185 [ 0.32472465  0.28035987 -0.28695182] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "186 [ 0.32472506  0.28036013 -0.28695129] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "187 [ 0.32472547  0.28036039 -0.28695075] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "188 [ 0.32472587  0.28036065 -0.28695021] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "189 [ 0.32472628  0.28036092 -0.28694968] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "190 [ 0.32472669  0.28036118 -0.28694914] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "191 [ 0.3247271   0.28036144 -0.2869486 ] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "192 [ 0.32472751  0.2803617  -0.28694807] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "193 [ 0.32472792  0.28036196 -0.28694753] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "194 [ 0.32472833  0.28036222 -0.28694699] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "195 [ 0.32472873  0.28036248 -0.28694645] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "196 [ 0.32472914  0.28036274 -0.28694592] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "197 [ 0.32472955  0.280363   -0.28694538] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "198 [ 0.32472996  0.28036326 -0.28694484] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "199 [ 0.32473037  0.28036353 -0.28694431] [ 0.32464904  0.2803116  -0.28705116] False\n",
            "200 [ 0.32473078  0.28036379 -0.28694377] [ 0.32464904  0.2803116  -0.28705116] False\n"
          ]
        }
      ],
      "source": [
        "V = np.random.uniform(-1,1,size=(3,3))\n",
        "mean = (1,1,1)\n",
        "#sigma = np.eye(3,3)*np.random.uniform(.00001,.0001,size=(3,))\n",
        "K = np.random.multivariate_normal(mean,np.eye(3,3)*np.random.uniform(.000000000001,.00000000001),size=(3,))\n",
        "\n",
        "mu = np.eye(3,3)*np.mean(K, axis=0)\n",
        "print(K)\n",
        "print(mu)\n",
        "\n",
        "for beta in range(1,201):\n",
        "    q = beta * (mu[0]+mu[1])\n",
        "    alpha = np.exp(mu.T @ q)/np.sum(np.exp(mu.T @ q))\n",
        "    print(beta, V.T @ alpha, .5 * (V[0]+V[1]), np.allclose(V.T @ alpha, .5 * (V[0]+V[1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c5cbd3",
      "metadata": {
        "id": "14c5cbd3"
      },
      "source": [
        "* Any expression $q=\\beta(\\mu_a+\\mu_b)$, for large $\\beta$, will work here. If $\\alpha$ is vanishingly small, then for all $i$, $k_i\\approx\\mu_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90789be4",
      "metadata": {
        "id": "90789be4"
      },
      "source": [
        "Though single-headed attention is resistant to small perturbations in the keys, some types of larger perturbations may pose a bigger issue.\n",
        "\n",
        "Specifically, in some cases, one key vector $k_a$ may have a larger or smaller norm than the others, while still pointing in the same direction as $\\mu_a$.\n",
        "\n",
        "As an example, let us consider a covariance for item $a$ as $\\Sigma_a=\\alpha I+\\frac{1}{2}(\\mu_a\\mu_a^\\top)$ for vanishingly small $\\alpha$. Further, let $\\Sigma_i=\\alpha I$ for all $i\\neq a$.\n",
        "\n",
        "<img src=\"https://github.com/sarathsreedharan/CSU_CS_445/blob/main/Spring25/Notebooks/images/plausible_ka.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e030a749",
      "metadata": {
        "id": "e030a749"
      },
      "source": [
        "When you sample $\\{k_1,\\ldots,k_n\\}$ multiple times, and use the $q$ vector defined above ($q = \\beta(\\mu_a + \\mu_b)$), what (qualitatively) do you expect the vector $c$ will look like for different samples?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9a102f",
      "metadata": {
        "id": "5d9a102f"
      },
      "source": [
        "* The greater $||k_a||$ is, the more $c$ will look like $v_a$, while the smaller $||k_a||$ is, the more $c$ will look like $v_b$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acaed75b",
      "metadata": {
        "id": "acaed75b"
      },
      "source": [
        "Therefore this is not a *practical* solution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb69b66",
      "metadata": {
        "id": "8fb69b66"
      },
      "source": [
        "Now we'll see some of the power of multi-headed attention.\n",
        "\n",
        "Consider a simple version of multi-headed attention which is identical to single-headed self-attention as we've presented it, except two query vectors ($q_1$ and $q_2$) are defined, which leads to a pair of vectors ($c_1$ and $c_2$), each the output of single-headed attention given its respective query vector.\n",
        "\n",
        "The final output of the multi-headed attention is their average, $\\frac{1}{2}(c_1+c_2)$.\n",
        "\n",
        "Consider a set of key vectors $\\{k_1,\\ldots,k_n\\}$ that are randomly sampled, $k_i\\sim\\mathcal{N}(\\mu_i,\\Sigma_i)$, where the means $\\mu_i$ are known to you, but the covariances $\\Sigma_i$ are unknown. Also as before, assume that the means $\\mu_i$ are mutually orthogonal; $\\mu_i^\\top\\mu_j=0$ if $i\\neq j$, and unit norm, $||\\mu_i||=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810ffad6",
      "metadata": {
        "id": "810ffad6"
      },
      "source": [
        "Assume that the covariance matrices are $\\Sigma_i=\\alpha I$, for vanishingly small $\\alpha$. Design $q_1$ and $q_2$ such that $c$ is approximately equal to $\\frac{1}{2}(v_a+v_b)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d9a07eb",
      "metadata": {
        "id": "7d9a07eb"
      },
      "source": [
        "* Any expressions $q_1=\\beta_1\\mu_a$ and $q_2=\\beta_2\\mu_b$, for large $\\beta_1$ and $\\beta_2$, will work here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c488606c",
      "metadata": {
        "id": "c488606c"
      },
      "source": [
        "Assume that the covariance matrices are $\\Sigma_a=\\alpha I+\\frac{1}{2}(\\mu_a\\mu_a^\\top)$ for vanishingly small $\\alpha$, and $\\Sigma_i=\\alpha I$ for all $i\\neq a$. Take the query vectors $q_1$ and $q_2$ above.\n",
        "\n",
        "What, qualitatively, would you expect the output $c$ to look like across different samples of the key vectors?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed77d27",
      "metadata": {
        "id": "9ed77d27"
      },
      "source": [
        "* We expect $c$ to look like $\\frac{1}{2}(v_a+v_b)$, no matter what $||k_a||$ is.\n",
        "* We expect $c_1$ to look like $v_a$, since $k_a$ points in roughly the same direction as $q_1$, while the other $k_i$ are roughly orthogonal to $q_1$.\n",
        "* Likewise, we expect $c_2$ to look like $v_b$. So $c=\\frac{1}{2}(c_1+c_2)\\approx\\frac{1}{2}(v_a+v_b)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4111fe13",
      "metadata": {
        "id": "4111fe13"
      },
      "source": [
        "So far, we've discussed attention as a function on a set of key vectors, a set of value vectors, and a query vector. In Transformers, we perform *self-attention*, which roughly means that we draw the keys, values, and queries from the same data.\n",
        "\n",
        "More precisely, let $\\{x_1,\\ldots,x_n\\}$ be a sequence of vectors in $\\mathbb{R}^d$. Think of each $x_i$ as representing word $i$ in a sentence. One form of self-attention defines keys, queries, and values as follows. Let $V,K,Q\\in\\mathbb{R}^{d\\times d}$ be parameter matrices. Then\n",
        "\\begin{equation}\n",
        "v_i=Vx_i,i\\in\\{1,\\ldots,n\\}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "k_i=Kx_i,i\\in\\{1,\\ldots,n\\}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "q_i=Qx_i,i\\in\\{1,\\ldots,n\\}.\n",
        "\\end{equation}\n",
        "Then we get a context vector for each input $i$; we have $c_i=\\Sigma_{j=1}^n\\alpha_{ij}v_j$, where $\\alpha_{ij}$ is defined as $\\alpha_{ij}=\\frac{\\exp(k_j^\\top q_i)}{\\sum_{\\ell=1}^n\\exp(k_\\ell^\\top q_i)}$. Note that this is single-headed self-attention."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb28042",
      "metadata": {
        "id": "aeb28042"
      },
      "source": [
        "We'll show how key-value-query attention like this allows the network to use different aspects of the input vectors $x_i$ in how it defines keys, queries, and values. Intuitively, this allows networks to choose different aspects of $x_i$ to be the \"content\" (value vector) versus what it uses to determine \"where to look\" for content (keys and queries)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1d8d7c",
      "metadata": {
        "id": "5e1d8d7c"
      },
      "source": [
        "First, consider if we didn't have key-query-value attention. For keys, queries, and values we'll just use $x_i$; that is, $v_i=q_i=k_i=x_i$. We'll consider a specific set of $x_i$. In particular, let $u_a,u_b,u_c,u_d$ be mutually orthogonal vectors in $\\mathbb{R}^d$, each with equal norm $||u_a||=||u_b||=||u_c||=||u_d||=\\beta$, where $\\beta$ is very large. Now, let our $x_i$ be:\n",
        "\\begin{equation}\n",
        "x_1=u_d+u_b\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "x_2=u_a\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "x_3=u_c+u_b.\n",
        "\\end{equation}\n",
        "If we perform self-attention with these vectors, what vector does $c_2$ approximate? Would it be possible for $c_2$ to approximate $u_b$ by adding either $u_d$ or $u_c$ to $x_2$?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a9a8003",
      "metadata": {
        "id": "3a9a8003"
      },
      "source": [
        "* We expect $c_2$ to look like $u_a$.\n",
        "* It is not possible for $c_2$ to approximate $u_b$ by adding either $u_d$ or $u_c$ to $x_2$.\n",
        "* Since $q_2=k_2$, and they are very large and point in the same direction (because they are the same), while the other $k_i$ point in different directions, $k_2^\\top q_2$ will be much larger than the other $k_i^\\top q_2$, and so $c_2$ will be approximately equal to $v_2=x_2\\neq u_b$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbc3ab5a",
      "metadata": {
        "id": "bbc3ab5a"
      },
      "source": [
        "Now consider using key-query-value attention as we've defined it originally. Using the same definitions of $x_1$, $x_2$, and $x_3$ as above, we want to specify matrices $K,Q,V$ such that $c_2\\approx u_b$, and $c_1\\approx u_b-u_c$.\n",
        "\n",
        "Some outer product properties may be helpful:\n",
        "\n",
        "For orthogonal vectors $u,v,w\\in\\mathbb{R}^d$, the outer product $uv^\\top$ is a matrix in $\\mathbb{R}^{d\\times d}$, and $(uv^\\top)v=u(v^\\top v)=u||v||_2^2$, and $(uv^\\top)w=u(v^\\top w)=u\\ast 0$ (The last equality is because $v$ and $w$ are orthogonal)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4720c7b",
      "metadata": {
        "id": "d4720c7b"
      },
      "source": [
        "There are many solutions to this problem.  One such is:\n",
        "\n",
        "$$V=\\frac{1}{\\beta^2}(u_b u_b^\\top-u_c u_c^\\top)$$\n",
        "\n",
        "$$Q=u_d u_a^\\top+u_c u_d^\\top$$\n",
        "\n",
        "$$K=I$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90acadaf",
      "metadata": {
        "id": "90acadaf"
      },
      "source": [
        "The idea is that when multiplying a matrix $uv^\\top$ and a vector $w$, the result will be either a scalar multiple of $u$ if $v$ and $w$ are not orthogonal, or $\\mathbf{0}$ if they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae3306e",
      "metadata": {
        "id": "9ae3306e",
        "outputId": "7f1bcde3-e1eb-4b78-93ca-3e1c2324d6f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.28504581 -1.52601644 -1.47858088]]\n",
            "[[2. 2. 2.]]\n",
            "[[0. 0. 0.]]\n",
            "[[-0. -0. -0.]]\n"
          ]
        }
      ],
      "source": [
        "u = np.random.uniform(-1,1,size=(1,3))\n",
        "v = np.array([[0,0,2]])\n",
        "w = np.array([[0,0,1]])\n",
        "\n",
        "print(w @ (v.T @ u))\n",
        "print(w @ (v.T @ u)/u)\n",
        "\n",
        "v = np.array([[0,2,0]])\n",
        "w = np.array([[0,0,1]])\n",
        "\n",
        "print(w @ (v.T @ u))\n",
        "print(w @ (v.T @ u)/u)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f5ca5c8",
      "metadata": {
        "id": "8f5ca5c8"
      },
      "source": [
        "Applying this idea to $V$, we want $u_b$ to appear in both $v_1$ and $v_3$, so we can let $u=u_b$. Then $v$ should be orthogonal to neither $x_1$ nor $x_3$; $v=u_b$ works. Therefore, we know that $u_bu_b^\\top$ should appear somewhere in $V$. Then we want $-u_c$ to appear in $v_3$ but not $v_1$, so we let $u=-u_c$, and $v=u_c$ is orthogonal to $x_1$ but not $x_3$, giving us another term $-u_cu_c^\\top$ to include in $V$. Finally, we add $u_bu_b^\\top$ and $-u_cu_c^\\top$ and factor out a scalar multiple of $\\beta^2$ to get $V=\\frac{1}{\\beta^2}(u_b u_b^\\top-u_c u_c^\\top)$.\n",
        "\n",
        "Then for $Q$ and $K$, we basically want $q_2$ to match $k_1$ and $q_1$ to match $k_3$. For simplicity, we can let $K=I$. One possible way to do the matching is to let $q_2$ be (a scalar multiple of) $u_d$ and $q_1$ be (a scalar multiple of) $u_c$; one can verify that $k_1^\\top q_2$ and $k_3^\\top q_1$ will be very large, while the other combinations of $k_j^\\top q_i$ will be 0. Then we can apply the same idea as before: $u=u_d$ and $v=u_a$ handle $q_2$, while $u=u_c$ and $v=u_d$ handle $q_1$; add them to get $Q=u_d u_a^\\top+u_c u_d^\\top$.\n",
        "\n",
        "We can finally verify that:\n",
        "\n",
        "$v_1=u_b\\hspace{20mm}$\n",
        "$v_2=\\mathbf{0}\\hspace{20mm}$\n",
        "$v_3=u_b-u_c$\n",
        "\n",
        "$q_1=\\beta^2u_cv\\hspace{14mm}$\n",
        "$q_2=\\beta^2u_d\\hspace{14mm}$\n",
        "$q_3=\\mathbf{0}$\n",
        "\n",
        "$k_1=u_d+u_b\\hspace{11mm}$\n",
        "$k_2=u_a\\hspace{18mm}$\n",
        "$k_3=u_c+u_b$\n",
        "\n",
        "$c_1\\approx u_b-u_c\\hspace{12mm}$\n",
        "$c_2\\approx u_b\\hspace{19mm}$\n",
        "$c_3=\\frac{2}{3}u_b-\\frac{1}{3}u_c$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f84fcb1",
      "metadata": {
        "id": "5f84fcb1"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "535abca1",
      "metadata": {
        "id": "535abca1"
      },
      "source": [
        "**BERT**: *B*idirectional *E*ncoder *R*epresentations from *T*ransformers.\n",
        "\n",
        "The name is a lie.  BERT actually consumes all the input at once, making it *non*directional.  But that would be NERT, which sounds stupid."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e47be5e",
      "metadata": {
        "id": "8e47be5e"
      },
      "source": [
        "Responsible for the profusion of muppet-named models in NLP.\n",
        "\n",
        "<img src=\"https://github.com/sarathsreedharan/CSU_CS_445/blob/main/Spring25/Notebooks/images/bert_models_layout.jpeg?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d17846",
      "metadata": {
        "id": "c6d17846"
      },
      "source": [
        "You can't go to a \\*ACL conference without being assaulted by muppets.  I've got PTSD."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b780e8d7",
      "metadata": {
        "id": "b780e8d7"
      },
      "source": [
        "Published in 2018 on arXiv (Devlin et al.).  Published at NAACL 2019 as some kind of goof or weird flex, I guess.  Won best paper."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abd3137",
      "metadata": {
        "id": "3abd3137"
      },
      "source": [
        "BERT is trained on 2 tasks: *Masked Language Modeling* (MLM) and *Next Sentence Prediction* (NSP)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ee1955e",
      "metadata": {
        "id": "6ee1955e"
      },
      "source": [
        "<img src=\"https://github.com/sarathsreedharan/CSU_CS_445/blob/main/Spring25/Notebooks/images/mlm.png?raw=1\" width=500>\n",
        "\n",
        "**In the MLM task** 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.\n",
        "\n",
        "Why 15%?\n",
        "\n",
        "# 🤷‍♂️\n",
        "\n",
        "It seemed to work best.\n",
        "\n",
        "Within that 15%:\n",
        "\n",
        "* 80% of the tokens are actually replaced with the token [MASK].\n",
        "* 10% of the time tokens are replaced with a random token.\n",
        "* 10% of the time tokens are left unchanged.\n",
        "\n",
        "This is because the model needs to be trained to predict a word regardless of whether or not the [MASK] token is there in the input; it should predict all the time.\n",
        "\n",
        "While training the BERT loss function considers only the prediction of the masked tokens and ignores the prediction of the non-masked ones. This is just one reason why BERT is slow to train."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c185a4",
      "metadata": {
        "id": "62c185a4"
      },
      "source": [
        "<img src=\"https://github.com/sarathsreedharan/CSU_CS_445/blob/main/Spring25/Notebooks/images/nsp.png?raw=1\" width=500>\n",
        "\n",
        "**In the NSP task**, there are two sentences structured like so:\n",
        "\n",
        "    [CLS] <sentence 1> [SEP] <sentence 2> [SEP]\n",
        "\n",
        "The [CLS] token is trained to contain an embedding that, when softmaxed, will result in one of two labels: `IsNext` or `NotNext`, depending on if `<sentence 2>` is determined to follow `<sentence 1>` or not.\n",
        "\n",
        "During training the model is fed with two input sentences at a time such that:\n",
        "\n",
        "* 50% of the time the second sentence comes after the first one.\n",
        "* 50% of the time it is a random sentence from the full corpus.\n",
        "\n",
        "The two sentences also have a token-level mask associated with them, of all 0s for tokens belonging to the first sentence, and all 1s for tokens belonging to the second sentence.\n",
        "\n",
        "Therefore, when performing BERT tasks with only one sentence (as we will do), you mask all tokens with 1s."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564489ae",
      "metadata": {
        "id": "564489ae"
      },
      "source": [
        "## Attention Mechanism In Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51356f51",
      "metadata": {
        "id": "51356f51"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d00bf0",
      "metadata": {
        "id": "c9d00bf0"
      },
      "source": [
        "### Inputs to the scoring function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fde239cb",
      "metadata": {
        "id": "fde239cb"
      },
      "source": [
        "Let's start by looking at the inputs we'll give to the scoring function. We will assume we're in the first step in the decoding phase. The first input to the scoring function is $s_t$, the hidden state of decoder (assume only three hidden nodes -- not usable in real life, but easier to illustrate):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5536d80b",
      "metadata": {
        "id": "5536d80b",
        "outputId": "e887ece5-91e6-4612-fd30-1ec80c89bfe7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 108x324 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "dec_hidden_state = [5,1,20]\n",
        "plt.figure(figsize=(1.5, 4.5))\n",
        "sns.heatmap(np.transpose(np.matrix(dec_hidden_state)), annot=True, \\\n",
        "            cmap=sns.light_palette(\"purple\", as_cmap=True), linewidths=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71352c82",
      "metadata": {
        "id": "71352c82"
      },
      "source": [
        "Our first scoring function will score a single encoder hidden state $h_i$, which looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f84417",
      "metadata": {
        "id": "d6f84417"
      },
      "outputs": [],
      "source": [
        "enc_hidden_state = [3,12,45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19dd7aa2",
      "metadata": {
        "id": "19dd7aa2",
        "outputId": "dfc58edc-1e07-4348-8c49-a8c2fe1aff7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 108x324 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Let's visualize our encoder hidden state\n",
        "plt.figure(figsize=(1.5, 4.5))\n",
        "sns.heatmap(np.transpose(np.matrix(enc_hidden_state)), annot=True,\\\n",
        "            cmap=sns.light_palette(\"orange\", as_cmap=True), linewidths=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e1ede8",
      "metadata": {
        "id": "07e1ede8",
        "outputId": "8dc9feea-df1b-446e-a02a-c30fb2dec7de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "927"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Score using dot product\n",
        "np.dot(dec_hidden_state, enc_hidden_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79305f8b",
      "metadata": {
        "id": "79305f8b"
      },
      "source": [
        "This is single-head dot-product attention: $s_th_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b636b04c",
      "metadata": {
        "id": "b636b04c"
      },
      "source": [
        "### Scoring multiple inputs at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be995bc8",
      "metadata": {
        "id": "be995bc8"
      },
      "outputs": [],
      "source": [
        "annotations = np.transpose([[3,12,45], [59,2,5], [1,43,5], [4,3,45.3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e99e514a",
      "metadata": {
        "id": "e99e514a"
      },
      "source": [
        "Now every column is a hidden state (i.e., embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd7eec0",
      "metadata": {
        "id": "0cd7eec0",
        "outputId": "8ed96a7b-6ae8-47d6-9702-5c9e971b9a30"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "ax = sns.heatmap(annotations, annot=True, cmap=sns.light_palette(\"orange\", as_cmap=True),\\\n",
        "                 linewidths=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7213f8e0",
      "metadata": {
        "id": "7213f8e0"
      },
      "source": [
        "### Scoring All Annotations at Once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ee45db",
      "metadata": {
        "id": "20ee45db",
        "outputId": "fd7ba4e2-269d-4fe4-b251-1dc0ee3f1d2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([927., 397., 148., 929.])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention_weights_raw = np.array(dec_hidden_state).T @ annotations\n",
        "attention_weights_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "597f60d1",
      "metadata": {
        "id": "597f60d1"
      },
      "source": [
        "Which of these inputs will get the most attention from this decoder?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f5196d",
      "metadata": {
        "id": "c6f5196d"
      },
      "source": [
        "Once more using scaled dot-product attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9639c0d1",
      "metadata": {
        "id": "9639c0d1",
        "outputId": "227f1a24-89b5-4e17-ccca-8ec0bc82a741"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([535.20369954, 229.20805687,  85.44783984, 536.35840008])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention_weights_scaled = (np.array(dec_hidden_state).T @ annotations)/np.sqrt(3)\n",
        "attention_weights_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b55a4d",
      "metadata": {
        "id": "98b55a4d"
      },
      "source": [
        "The formula for attention is: $$\\text{Attention}(s,h) = \\text{softmax}(\\frac{s_t^Th_i}{\\sqrt n})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba0568a",
      "metadata": {
        "id": "3ba0568a"
      },
      "source": [
        "We've done $\\frac{s_t^Th_i}{\\sqrt n}$, so let's apply softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0509d2",
      "metadata": {
        "id": "1a0509d2"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    x = np.array(x, dtype=float)\n",
        "    e_x = np.exp(x)\n",
        "    return e_x / e_x.sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85da5f22",
      "metadata": {
        "id": "85da5f22",
        "outputId": "a07dc7a3-103d-4200-c2c3-c4567fb412ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2.39631558e-001, 3.07131891e-134, 1.12994145e-196, 7.60368442e-001])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention_weights = softmax(attention_weights_scaled)\n",
        "attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79dcd982",
      "metadata": {
        "id": "79dcd982",
        "outputId": "3dac0853-6b8b-4860-d7c2-4ac3bdcb9f7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((535.2036995387831, 536.3584000771624),\n",
              " (0.2396315581419701, 0.7603684418580298))"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(attention_weights_scaled[0],attention_weights_scaled[3]),\\\n",
        "(attention_weights[0],attention_weights[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f87a51e",
      "metadata": {
        "id": "7f87a51e"
      },
      "source": [
        "Notice how now, even though the raw and scaled raw attention weights for indices 0 and 3 were really close, after softmax, the difference is stark."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a360f3c2",
      "metadata": {
        "id": "a360f3c2"
      },
      "source": [
        "### Apply the attention weights back onto the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99bf293",
      "metadata": {
        "id": "a99bf293",
        "outputId": "05b61ec8-4fda-4589-8b44-9eb6fde9de31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[7.18894674e-001, 1.81207816e-132, 1.12994145e-196,\n",
              "        3.04147377e+000],\n",
              "       [2.87557870e+000, 6.14263782e-134, 4.85874824e-195,\n",
              "        2.28110533e+000],\n",
              "       [1.07834201e+001, 1.53565945e-133, 5.64970726e-196,\n",
              "        3.44446904e+001]])"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# just a simple elementwise multiplication\n",
        "applied_attention = attention_weights * annotations\n",
        "applied_attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d44e1a9",
      "metadata": {
        "id": "7d44e1a9",
        "outputId": "3b722935-2b98-4377-faab-eb72411cb6b5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Let's visualize our annotations after applying attention to them\n",
        "ax = sns.heatmap(applied_attention, annot=True,\\\n",
        "                 cmap=sns.light_palette(\"orange\", as_cmap=True), linewidths=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d70072b6",
      "metadata": {
        "id": "d70072b6"
      },
      "source": [
        "Contrast this with the raw inputs visualized earlier in the notebook, and we can see that the second and third inputs (columns) have been nearly wiped out. The first annotation maintains some of its value, and the fourth annotation is the most pronounced."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa68835",
      "metadata": {
        "id": "2fa68835"
      },
      "source": [
        "### Calculating the attention context vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14638b64",
      "metadata": {
        "id": "14638b64",
        "outputId": "c6274a9f-1afb-446c-b31f-41c67bdd9176"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 108x324 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# sum across the rows of the attention applied to inputs\n",
        "attention_vector = np.sum(applied_attention, axis=1)\n",
        "plt.figure(figsize=(1.5, 4.5))\n",
        "sns.heatmap(np.transpose(np.matrix(attention_vector)), annot=True,\\\n",
        "            cmap=sns.light_palette(\"Blue\", as_cmap=True), linewidths=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d97e572",
      "metadata": {
        "id": "1d97e572"
      },
      "source": [
        "## Playing with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b00545a",
      "metadata": {
        "id": "2b00545a"
      },
      "source": [
        "In this section, we're going to compare the behavior of BERT, the original, and RoBERTa, a **R**obustly **O**ptimized **BERT** Pretraining **A**pproach, on the Masked Language Model task, one of the tasks BERT was originally trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4391d71",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1a2f9f892037443d80996734158a3c0a",
            "39325edb67a042f4aaa4b1fce9904103",
            "5defa8ca37174d40bc76bbb2880faa1a",
            "06e9dc9b76d3413ebe28c9f2851e9005"
          ]
        },
        "id": "a4391d71",
        "outputId": "299b94b7-2e13-4055-affa-8c9ccc6165a9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a2f9f892037443d80996734158a3c0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39325edb67a042f4aaa4b1fce9904103",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5defa8ca37174d40bc76bbb2880faa1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06e9dc9b76d3413ebe28c9f2851e9005",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from itertools import product\n",
        "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "bert_base_cased_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "bert_base_cased_model = BertModel.from_pretrained('bert-base-cased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states\n",
        "                                  )\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "roberta_base_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "roberta_base_model = RobertaModel.from_pretrained('roberta-base',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e244705",
      "metadata": {
        "id": "4e244705"
      },
      "outputs": [],
      "source": [
        "# define a cosine similarity function\n",
        "def cos_sim(a,b):\n",
        "    return np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a052ffd",
      "metadata": {
        "id": "1a052ffd"
      },
      "source": [
        "Get sentence and token embeddings (same as Lecture 25)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c28255a",
      "metadata": {
        "id": "9c28255a"
      },
      "outputs": [],
      "source": [
        "'''Get the embedding for a single sentence'''\n",
        "def get_sentence_embedding(text, tokenizer, model):\n",
        "\n",
        "    # Add the special tokens\n",
        "    marked_text = text\n",
        "    if isinstance(model,BertModel):\n",
        "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "    # Split the sentence into tokens\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "    # Map the token strings to their vocabulary indices\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    # Display the words with their indices\n",
        "    for i, tup in enumerate(zip(tokenized_text, indexed_tokens)):\n",
        "        print('{:} {:<12} {:>6,}'.format(i, tup[0], tup[1]))\n",
        "\n",
        "    # Mark each token as belonging to sentence \"1\"\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Run the text through BERT, and collect all of the hidden states produced\n",
        "    # from all 12 layers.\n",
        "    with torch.no_grad():\n",
        "\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "        # Evaluating the model will return a different number of objects based on\n",
        "        # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
        "        # becase we set `output_hidden_states = True`, the third item will be the\n",
        "        # hidden states from all layers. See the documentation for more details:\n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "    print(\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "    layer_i = 0\n",
        "\n",
        "    print(\"Number of batches:\", len(hidden_states[layer_i]))\n",
        "    batch_i = 0\n",
        "\n",
        "    print(\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
        "    token_i = 0\n",
        "\n",
        "    print(\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
        "\n",
        "    return tokenized_text, outputs[0], outputs[1], hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c76e2a2",
      "metadata": {
        "id": "7c76e2a2"
      },
      "outputs": [],
      "source": [
        "'''Returns embeddings for tokens and indices in \"indices\", concatenated and summed over the last n layers'''\n",
        "def get_token_embeddings(token_embeddings, last_n_layers, indices):\n",
        "\n",
        "    # Stores the token vectors\n",
        "    token_vecs_cat = []\n",
        "\n",
        "    # `token_embeddings` is a [N x 12 x 768] tensor.\n",
        "\n",
        "    # For each token in the sentence...\n",
        "    for token in token_embeddings:\n",
        "\n",
        "        # `token` is a [12 x 768] tensor\n",
        "\n",
        "        # Concatenate the vectors (that is, append them together) from the last\n",
        "        # four layers.\n",
        "        # Each layer vector is 768 values, so `cat_vec` is length last_n_layers*768\n",
        "        layer_reps = []\n",
        "        for n in range(1,last_n_layers+1):\n",
        "            layer_reps.append(token[-n])\n",
        "            cat_vec = torch.cat(tuple(layer_reps), dim=0)\n",
        "\n",
        "        # Use `cat_vec` to represent `token`\n",
        "        token_vecs_cat.append(cat_vec)\n",
        "\n",
        "    print ('Concatenated embeddings shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
        "\n",
        "    # Stores the token vectors\n",
        "    token_vecs_sum = []\n",
        "\n",
        "    # For each token in the sentence...\n",
        "    for token in token_embeddings:\n",
        "\n",
        "        # `token` is a [12 x 768] tensor\n",
        "\n",
        "        # Sum the vectors from the last four layers.\n",
        "        sum_vec = torch.sum(token[-n:], dim=0)\n",
        "\n",
        "        # Use `sum_vec` to represent `token`.\n",
        "        token_vecs_sum.append(sum_vec)\n",
        "\n",
        "    print ('Summed embeddings shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
        "\n",
        "    return token_vecs_cat, token_vecs_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e50048",
      "metadata": {
        "id": "06e50048"
      },
      "outputs": [],
      "source": [
        "def calc_similarities(tokenized_text, cat_embeddings, sum_embeddings, indices):\n",
        "    i2i = dict(zip(indices,[i for i in range(len(indices))]))\n",
        "\n",
        "    for i1,i2 in list(product(indices, repeat=2)):\n",
        "        if i1 != i2:\n",
        "            print(\"Cosine similarity between %s (%s) and %s (%s) (concatenated embeddings): %.4f\" % \\\n",
        "                  (tokenized_text[i1], i1, tokenized_text[i2], i2, \\\n",
        "                   cos_sim(cat_embeddings[i2i[i1]],cat_embeddings[i2i[i2]])))\n",
        "            print(\"Cosine similarity between %s (%s) and %s (%s) (summed embeddings): %.4f\\n\" % \\\n",
        "                  (tokenized_text[i1], i1, tokenized_text[i2], i2, \\\n",
        "                   cos_sim(sum_embeddings[i2i[i1]],sum_embeddings[i2i[i2]])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39302bd0",
      "metadata": {
        "id": "39302bd0"
      },
      "source": [
        "This test sentence below serves the same purpose as multiple uses of the word \"bank\" (from Lecture 25), except here we're looking at \"bed\", which occurs as a whole word, and also part of the word \"embedding.\"  Because of BERT's fixed vocabulary, \"embedded\" appears as its own word, and doesn't need the subword, like \"embedding(s)\" does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32420d7b",
      "metadata": {
        "id": "32420d7b",
        "outputId": "17a843f6-33ac-4504-c3e1-a49ce4e5a459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 [CLS]           101\n",
            "1 After         1,258\n",
            "2 calculating  23,172\n",
            "3 some          1,199\n",
            "4 em            9,712\n",
            "5 ##bed         4,774\n",
            "6 ##ding        3,408\n",
            "7 ##s           1,116\n",
            "8 for           1,111\n",
            "9 an            1,126\n",
            "10 embedded     11,783\n",
            "11 system        1,449\n",
            "12 and           1,105\n",
            "13 turning       3,219\n",
            "14 over          1,166\n",
            "15 the           1,103\n",
            "16 flower        7,366\n",
            "17 beds          9,884\n",
            "18 ,               117\n",
            "19 the           1,103\n",
            "20 professor     3,083\n",
            "21 went          1,355\n",
            "22 to            1,106\n",
            "23 bed           1,908\n",
            "24 .               119\n",
            "25 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 26\n",
            "Number of hidden units: 768\n"
          ]
        }
      ],
      "source": [
        "text = \"After calculating some embeddings for an embedded system and turning over the flower beds, the professor went to bed.\"\n",
        "tokenized_text, seq_embedding, pooled_embedding, hidden_states = \\\n",
        "    get_sentence_embedding(text, bert_base_cased_tokenizer, bert_base_cased_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8750ab6c",
      "metadata": {
        "id": "8750ab6c"
      },
      "source": [
        "Let's look at the similarities between different occurrences of \"bed\", as a whole word, and a subword, and associated terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6731bcae",
      "metadata": {
        "id": "6731bcae",
        "outputId": "3c11bad7-a52d-48c6-9a0a-1fd72a699149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated embeddings shape is: 26 x 3072\n",
            "Summed embeddings shape is: 26 x 768\n",
            "Cosine similarity between em (4) and ##bed (5) (concatenated embeddings): 0.6543\n",
            "Cosine similarity between em (4) and ##bed (5) (summed embeddings): 0.6699\n",
            "\n",
            "Cosine similarity between em (4) and embedded (10) (concatenated embeddings): 0.6703\n",
            "Cosine similarity between em (4) and embedded (10) (summed embeddings): 0.6889\n",
            "\n",
            "Cosine similarity between em (4) and beds (17) (concatenated embeddings): 0.6401\n",
            "Cosine similarity between em (4) and beds (17) (summed embeddings): 0.6595\n",
            "\n",
            "Cosine similarity between em (4) and bed (23) (concatenated embeddings): 0.5851\n",
            "Cosine similarity between em (4) and bed (23) (summed embeddings): 0.6089\n",
            "\n",
            "Cosine similarity between ##bed (5) and em (4) (concatenated embeddings): 0.6543\n",
            "Cosine similarity between ##bed (5) and em (4) (summed embeddings): 0.6699\n",
            "\n",
            "Cosine similarity between ##bed (5) and embedded (10) (concatenated embeddings): 0.6593\n",
            "Cosine similarity between ##bed (5) and embedded (10) (summed embeddings): 0.6905\n",
            "\n",
            "Cosine similarity between ##bed (5) and beds (17) (concatenated embeddings): 0.6596\n",
            "Cosine similarity between ##bed (5) and beds (17) (summed embeddings): 0.6881\n",
            "\n",
            "Cosine similarity between ##bed (5) and bed (23) (concatenated embeddings): 0.5288\n",
            "Cosine similarity between ##bed (5) and bed (23) (summed embeddings): 0.5650\n",
            "\n",
            "Cosine similarity between embedded (10) and em (4) (concatenated embeddings): 0.6703\n",
            "Cosine similarity between embedded (10) and em (4) (summed embeddings): 0.6889\n",
            "\n",
            "Cosine similarity between embedded (10) and ##bed (5) (concatenated embeddings): 0.6593\n",
            "Cosine similarity between embedded (10) and ##bed (5) (summed embeddings): 0.6905\n",
            "\n",
            "Cosine similarity between embedded (10) and beds (17) (concatenated embeddings): 0.8159\n",
            "Cosine similarity between embedded (10) and beds (17) (summed embeddings): 0.8246\n",
            "\n",
            "Cosine similarity between embedded (10) and bed (23) (concatenated embeddings): 0.7447\n",
            "Cosine similarity between embedded (10) and bed (23) (summed embeddings): 0.7520\n",
            "\n",
            "Cosine similarity between beds (17) and em (4) (concatenated embeddings): 0.6401\n",
            "Cosine similarity between beds (17) and em (4) (summed embeddings): 0.6595\n",
            "\n",
            "Cosine similarity between beds (17) and ##bed (5) (concatenated embeddings): 0.6596\n",
            "Cosine similarity between beds (17) and ##bed (5) (summed embeddings): 0.6881\n",
            "\n",
            "Cosine similarity between beds (17) and embedded (10) (concatenated embeddings): 0.8159\n",
            "Cosine similarity between beds (17) and embedded (10) (summed embeddings): 0.8246\n",
            "\n",
            "Cosine similarity between beds (17) and bed (23) (concatenated embeddings): 0.7326\n",
            "Cosine similarity between beds (17) and bed (23) (summed embeddings): 0.7435\n",
            "\n",
            "Cosine similarity between bed (23) and em (4) (concatenated embeddings): 0.5851\n",
            "Cosine similarity between bed (23) and em (4) (summed embeddings): 0.6089\n",
            "\n",
            "Cosine similarity between bed (23) and ##bed (5) (concatenated embeddings): 0.5288\n",
            "Cosine similarity between bed (23) and ##bed (5) (summed embeddings): 0.5650\n",
            "\n",
            "Cosine similarity between bed (23) and embedded (10) (concatenated embeddings): 0.7447\n",
            "Cosine similarity between bed (23) and embedded (10) (summed embeddings): 0.7520\n",
            "\n",
            "Cosine similarity between bed (23) and beds (17) (concatenated embeddings): 0.7326\n",
            "Cosine similarity between bed (23) and beds (17) (summed embeddings): 0.7435\n",
            "\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = torch.squeeze(torch.stack(hidden_states, dim=0), dim=1).permute(1,0,2)\n",
        "indices = [4,5,10,17,23]\n",
        "cat_embeddings, sum_embeddings = get_token_embeddings(token_embeddings, 4, indices)\n",
        "calc_similarities(tokenized_text, cat_embeddings, sum_embeddings, indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec23af4",
      "metadata": {
        "id": "cec23af4"
      },
      "source": [
        "I'm also going to look at the similarities between occurrences of \"bed\" and the pooled embedding, or the embedding of the [CLS] token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202fca7d",
      "metadata": {
        "id": "202fca7d",
        "outputId": "28b97f24-160a-4e8c-ebeb-702ad88cb2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.043538228\n",
            "0.029575096\n",
            "0.07148381\n",
            "0.048761006\n",
            "0.053450435\n",
            "0.047518592\n",
            "0.038523562\n",
            "0.033171542\n",
            "0.058744162\n",
            "0.060452648\n",
            "0.06925282\n",
            "0.05077903\n",
            "0.0812717\n",
            "0.0075849914\n",
            "0.043029685\n",
            "0.031228842\n",
            "0.01775464\n",
            "0.008402588\n",
            "0.017173048\n",
            "0.028169237\n",
            "0.039336994\n",
            "0.016609754\n",
            "-0.0045498535\n",
            "0.0037792975\n",
            "0.03786347\n",
            "0.019259058\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(sum_embeddings)):\n",
        "    print(cos_sim(sum_embeddings[i],pooled_embedding[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36d42987",
      "metadata": {
        "id": "36d42987"
      },
      "source": [
        "No close similarity between any mention of \"bed\" and the meaning of the whole sentence.  The pooled embedding would also be different from the average of all the contextualized token embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de425452",
      "metadata": {
        "id": "de425452"
      },
      "source": [
        "Now, I want to look at the similarity between things I *know* should be very different: named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0103f5",
      "metadata": {
        "id": "ce0103f5"
      },
      "outputs": [],
      "source": [
        "sentence_ids = {}\n",
        "cat_embeddings = {}\n",
        "sum_embeddings = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34317a89",
      "metadata": {
        "id": "34317a89",
        "outputId": "e924b5fb-77dc-4744-e0fd-4cf045c08f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 [CLS]           101\n",
            "1 President     1,697\n",
            "2 Joseph        2,419\n",
            "3 R               155\n",
            "4 .               119\n",
            "5 B               139\n",
            "6 ##iden       26,859\n",
            "7 ,               117\n",
            "8 Jr            3,108\n",
            "9 .               119\n",
            "10 met           1,899\n",
            "11 virtually     9,024\n",
            "12 on            1,113\n",
            "13 November      1,379\n",
            "14 15            1,405\n",
            "15 with          1,114\n",
            "16 President     1,697\n",
            "17 Xi           20,802\n",
            "18 Jin          10,922\n",
            "19 ##ping        2,624\n",
            "20 of            1,104\n",
            "21 the           1,103\n",
            "22 People        2,563\n",
            "23 ’               787\n",
            "24 s               188\n",
            "25 Republic      2,250\n",
            "26 of            1,104\n",
            "27 China         1,975\n",
            "28 (               113\n",
            "29 PR           11,629\n",
            "30 ##C           1,658\n",
            "31 )               114\n",
            "32 .               119\n",
            "33 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 34\n",
            "Number of hidden units: 768\n"
          ]
        }
      ],
      "source": [
        "text = \"President Joseph R. Biden, Jr. met virtually on November 15 with President Xi Jinping of the People’s Republic of China (PRC).\"\n",
        "tokenized_text, seq_embedding, pooled_embedding, hidden_states = \\\n",
        "    get_sentence_embedding(text, bert_base_cased_tokenizer, bert_base_cased_model)\n",
        "sentence_ids[len(sentence_ids)] = tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a685a896",
      "metadata": {
        "id": "a685a896",
        "outputId": "7bd450b6-13e3-4108-e8e0-701c74f6b462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated embeddings shape is: 34 x 3072\n",
            "Summed embeddings shape is: 34 x 768\n",
            "Cosine similarity between Joseph (2) and B (5) (concatenated embeddings): 0.6462\n",
            "Cosine similarity between Joseph (2) and B (5) (summed embeddings): 0.6642\n",
            "\n",
            "Cosine similarity between Joseph (2) and ##iden (6) (concatenated embeddings): 0.5280\n",
            "Cosine similarity between Joseph (2) and ##iden (6) (summed embeddings): 0.5392\n",
            "\n",
            "Cosine similarity between Joseph (2) and Xi (17) (concatenated embeddings): 0.3453\n",
            "Cosine similarity between Joseph (2) and Xi (17) (summed embeddings): 0.3573\n",
            "\n",
            "Cosine similarity between Joseph (2) and Jin (18) (concatenated embeddings): 0.3352\n",
            "Cosine similarity between Joseph (2) and Jin (18) (summed embeddings): 0.3524\n",
            "\n",
            "Cosine similarity between Joseph (2) and ##ping (19) (concatenated embeddings): 0.4799\n",
            "Cosine similarity between Joseph (2) and ##ping (19) (summed embeddings): 0.4972\n",
            "\n",
            "Cosine similarity between B (5) and Joseph (2) (concatenated embeddings): 0.6462\n",
            "Cosine similarity between B (5) and Joseph (2) (summed embeddings): 0.6642\n",
            "\n",
            "Cosine similarity between B (5) and ##iden (6) (concatenated embeddings): 0.6324\n",
            "Cosine similarity between B (5) and ##iden (6) (summed embeddings): 0.6594\n",
            "\n",
            "Cosine similarity between B (5) and Xi (17) (concatenated embeddings): 0.4445\n",
            "Cosine similarity between B (5) and Xi (17) (summed embeddings): 0.4746\n",
            "\n",
            "Cosine similarity between B (5) and Jin (18) (concatenated embeddings): 0.4340\n",
            "Cosine similarity between B (5) and Jin (18) (summed embeddings): 0.4737\n",
            "\n",
            "Cosine similarity between B (5) and ##ping (19) (concatenated embeddings): 0.5305\n",
            "Cosine similarity between B (5) and ##ping (19) (summed embeddings): 0.5627\n",
            "\n",
            "Cosine similarity between ##iden (6) and Joseph (2) (concatenated embeddings): 0.5280\n",
            "Cosine similarity between ##iden (6) and Joseph (2) (summed embeddings): 0.5392\n",
            "\n",
            "Cosine similarity between ##iden (6) and B (5) (concatenated embeddings): 0.6324\n",
            "Cosine similarity between ##iden (6) and B (5) (summed embeddings): 0.6594\n",
            "\n",
            "Cosine similarity between ##iden (6) and Xi (17) (concatenated embeddings): 0.6207\n",
            "Cosine similarity between ##iden (6) and Xi (17) (summed embeddings): 0.6284\n",
            "\n",
            "Cosine similarity between ##iden (6) and Jin (18) (concatenated embeddings): 0.6630\n",
            "Cosine similarity between ##iden (6) and Jin (18) (summed embeddings): 0.6759\n",
            "\n",
            "Cosine similarity between ##iden (6) and ##ping (19) (concatenated embeddings): 0.6879\n",
            "Cosine similarity between ##iden (6) and ##ping (19) (summed embeddings): 0.6951\n",
            "\n",
            "Cosine similarity between Xi (17) and Joseph (2) (concatenated embeddings): 0.3453\n",
            "Cosine similarity between Xi (17) and Joseph (2) (summed embeddings): 0.3573\n",
            "\n",
            "Cosine similarity between Xi (17) and B (5) (concatenated embeddings): 0.4445\n",
            "Cosine similarity between Xi (17) and B (5) (summed embeddings): 0.4746\n",
            "\n",
            "Cosine similarity between Xi (17) and ##iden (6) (concatenated embeddings): 0.6207\n",
            "Cosine similarity between Xi (17) and ##iden (6) (summed embeddings): 0.6284\n",
            "\n",
            "Cosine similarity between Xi (17) and Jin (18) (concatenated embeddings): 0.6888\n",
            "Cosine similarity between Xi (17) and Jin (18) (summed embeddings): 0.6996\n",
            "\n",
            "Cosine similarity between Xi (17) and ##ping (19) (concatenated embeddings): 0.6823\n",
            "Cosine similarity between Xi (17) and ##ping (19) (summed embeddings): 0.6910\n",
            "\n",
            "Cosine similarity between Jin (18) and Joseph (2) (concatenated embeddings): 0.3352\n",
            "Cosine similarity between Jin (18) and Joseph (2) (summed embeddings): 0.3524\n",
            "\n",
            "Cosine similarity between Jin (18) and B (5) (concatenated embeddings): 0.4340\n",
            "Cosine similarity between Jin (18) and B (5) (summed embeddings): 0.4737\n",
            "\n",
            "Cosine similarity between Jin (18) and ##iden (6) (concatenated embeddings): 0.6630\n",
            "Cosine similarity between Jin (18) and ##iden (6) (summed embeddings): 0.6759\n",
            "\n",
            "Cosine similarity between Jin (18) and Xi (17) (concatenated embeddings): 0.6888\n",
            "Cosine similarity between Jin (18) and Xi (17) (summed embeddings): 0.6996\n",
            "\n",
            "Cosine similarity between Jin (18) and ##ping (19) (concatenated embeddings): 0.6826\n",
            "Cosine similarity between Jin (18) and ##ping (19) (summed embeddings): 0.6908\n",
            "\n",
            "Cosine similarity between ##ping (19) and Joseph (2) (concatenated embeddings): 0.4799\n",
            "Cosine similarity between ##ping (19) and Joseph (2) (summed embeddings): 0.4972\n",
            "\n",
            "Cosine similarity between ##ping (19) and B (5) (concatenated embeddings): 0.5305\n",
            "Cosine similarity between ##ping (19) and B (5) (summed embeddings): 0.5627\n",
            "\n",
            "Cosine similarity between ##ping (19) and ##iden (6) (concatenated embeddings): 0.6879\n",
            "Cosine similarity between ##ping (19) and ##iden (6) (summed embeddings): 0.6951\n",
            "\n",
            "Cosine similarity between ##ping (19) and Xi (17) (concatenated embeddings): 0.6823\n",
            "Cosine similarity between ##ping (19) and Xi (17) (summed embeddings): 0.6910\n",
            "\n",
            "Cosine similarity between ##ping (19) and Jin (18) (concatenated embeddings): 0.6826\n",
            "Cosine similarity between ##ping (19) and Jin (18) (summed embeddings): 0.6908\n",
            "\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = torch.squeeze(torch.stack(hidden_states, dim=0), dim=1).permute(1,0,2)\n",
        "indices = [2,5,6,17,18,19]\n",
        "c_emb, s_emb = get_token_embeddings(token_embeddings, 4, indices)\n",
        "cat_embeddings[len(cat_embeddings)] = c_emb\n",
        "sum_embeddings[len(sum_embeddings)] = s_emb\n",
        "calc_similarities(tokenized_text, cat_embeddings[0], sum_embeddings[0], indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47990a4d",
      "metadata": {
        "id": "47990a4d",
        "outputId": "29eaa69d-0f86-4a9e-c2aa-8b5eab0e401c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated embeddings shape is: 34 x 9216\n",
            "Summed embeddings shape is: 34 x 768\n",
            "Cosine similarity between of (20) and of (26) (concatenated embeddings): 0.6462\n",
            "Cosine similarity between of (20) and of (26) (summed embeddings): 0.6642\n",
            "\n",
            "Cosine similarity between of (26) and of (20) (concatenated embeddings): 0.6462\n",
            "Cosine similarity between of (26) and of (20) (summed embeddings): 0.6642\n",
            "\n"
          ]
        }
      ],
      "source": [
        "c_emb, s_emb = get_token_embeddings(token_embeddings, 12, [20,26])\n",
        "calc_similarities(tokenized_text, cat_embeddings[0], sum_embeddings[0], [20,26])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c5725e",
      "metadata": {
        "id": "25c5725e",
        "outputId": "2f5d28ec-6e98-46e9-c703-135aab42b8d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 [CLS]           101\n",
            "1 President     1,697\n",
            "2 B               139\n",
            "3 ##iden       26,859\n",
            "4 welcomed     10,977\n",
            "5 the           1,103\n",
            "6 opportunity   3,767\n",
            "7 to            1,106\n",
            "8 speak         2,936\n",
            "9 can           1,169\n",
            "10 ##di          3,309\n",
            "11 ##dly        18,733\n",
            "12 and           1,105\n",
            "13 straightforward 21,546\n",
            "14 ##ly          1,193\n",
            "15 to            1,106\n",
            "16 President     1,697\n",
            "17 Xi           20,802\n",
            "18 about         1,164\n",
            "19 our           1,412\n",
            "20 intentions   11,489\n",
            "21 and           1,105\n",
            "22 priorities   21,249\n",
            "23 across        1,506\n",
            "24 a               170\n",
            "25 range         2,079\n",
            "26 of            1,104\n",
            "27 issues        2,492\n",
            "28 .               119\n",
            "29 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 30\n",
            "Number of hidden units: 768\n"
          ]
        }
      ],
      "source": [
        "text = \"President Biden welcomed the opportunity to speak candidly and straightforwardly to President Xi about our intentions and priorities across a range of issues.\"\n",
        "tokenized_text, seq_embedding, pooled_embedding, hidden_states = \\\n",
        "    get_sentence_embedding(text, bert_base_cased_tokenizer, bert_base_cased_model)\n",
        "sentence_ids[len(sentence_ids)] = tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d485dbc",
      "metadata": {
        "id": "0d485dbc",
        "outputId": "e8acabf8-ff59-49e6-d152-381d47f0d90a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated embeddings shape is: 30 x 3072\n",
            "Summed embeddings shape is: 30 x 768\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = torch.squeeze(torch.stack(hidden_states, dim=0), dim=1).permute(1,0,2)\n",
        "indices = [1,2,3,15,16]\n",
        "c_emb, s_emb = get_token_embeddings(token_embeddings, 4, indices)\n",
        "cat_embeddings[len(cat_embeddings)] = c_emb\n",
        "sum_embeddings[len(sum_embeddings)] = s_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9cb2da0",
      "metadata": {
        "id": "b9cb2da0",
        "outputId": "96ee0103-1606-46d9-896b-3ed3f1e6a258"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8793123"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ##iden from S1 compared to ##iden from S2\n",
        "cos_sim(cat_embeddings[0][5],cat_embeddings[1][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6727564d",
      "metadata": {
        "id": "6727564d",
        "outputId": "2197bc75-5315-47ea-b976-cf6fed03edbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.89297724"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ##iden from S1 compared to ##iden from S2\n",
        "cos_sim(sum_embeddings[0][5],sum_embeddings[1][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034ea20b",
      "metadata": {
        "id": "034ea20b",
        "outputId": "a1fa393b-9479-44c9-8e45-d9f0ce203ca2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7731297"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Xi from S1 compared to Xi from S2\n",
        "cos_sim(cat_embeddings[0][17],cat_embeddings[1][17])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de1e934",
      "metadata": {
        "id": "3de1e934",
        "outputId": "87ee996e-2cce-403d-a853-282c75cf1b24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.49985635"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ##iden from S1 compared to Xi from S2\n",
        "cos_sim(cat_embeddings[0][5],cat_embeddings[1][17])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3813ae",
      "metadata": {
        "id": "fd3813ae",
        "outputId": "5bbbf8de-6bca-4771-d5cd-c3e99942aeeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.51215386"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ##iden from S1 compared to Xi from S2\n",
        "cos_sim(sum_embeddings[0][5],sum_embeddings[1][17])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c71d39",
      "metadata": {
        "id": "24c71d39",
        "outputId": "30721b84-443a-4c7b-9cfc-1af7311ce81f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5663463"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Xi from S1 compared to ##iden from S2\n",
        "cos_sim(cat_embeddings[0][17],cat_embeddings[1][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b95d7b0",
      "metadata": {
        "id": "7b95d7b0",
        "outputId": "65cf3946-363c-4324-bbdf-8d31657f9552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.93082124"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# President (Joseph) from S1 compared to President (B) from S2\n",
        "cos_sim(cat_embeddings[0][1],cat_embeddings[1][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdbd42f",
      "metadata": {
        "id": "8bdbd42f",
        "outputId": "8244beab-1a14-4ca2-ec2d-3eadb40484be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.74788404"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# President (Joseph) from S1 compared to President (Xi) from S2\n",
        "cos_sim(cat_embeddings[0][1],cat_embeddings[1][16])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9902c85b",
      "metadata": {
        "id": "9902c85b"
      },
      "source": [
        "So, even naively with cosine similarity, BERT seems to be able to tell me that Joe Biden is different from Xi Jinping.\n",
        "\n",
        "(More accurately, I should that Joe Biden mentions are more similar to each other than Xi Jinping mentions, and vice versa)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c6e8fdf",
      "metadata": {
        "id": "5c6e8fdf"
      },
      "source": [
        "Data below: 1 WH press release about the Biden/Xi virtual meeting + 1 CNN article about the same meeting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caeb58da",
      "metadata": {
        "scrolled": true,
        "id": "caeb58da",
        "outputId": "271e3f77-094d-4919-fd2c-f22509996113"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['President Joseph R. Biden, Jr. met virtually on November 15 with President Xi Jinping of the People’s Republic of China (PRC).',\n",
              " 'The two leaders discussed the complex nature of relations between our two countries and the importance of managing competition responsibly.',\n",
              " 'As in previous discussions, the two leaders covered areas where our interests align, and areas where our interests, values, and perspectives diverge.',\n",
              " 'President Biden welcomed the opportunity to speak candidly and straightforwardly to President Xi about our intentions and priorities across a range of issues.',\n",
              " 'President Biden underscored that the United States will continue to stand up for its interests and values and, together with our allies and partners, ensure the rules of the road for the 21st century advance an international system that is free, open, and fair.',\n",
              " 'He emphasized the priority he places on far-reaching investments at home while we align with allies and partners abroad to take on the challenges of our time.',\n",
              " 'President Biden raised concerns about the PRC’s practices in Xinjiang, Tibet, and Hong Kong, as well as human rights more broadly.',\n",
              " 'He was clear about the need to protect American workers and industries from the PRC’s unfair trade and economic practices.',\n",
              " 'He also discussed the importance of a free and open Indo-Pacific, and communicated the continued determination of the United States to uphold our commitments in the region.',\n",
              " 'President Biden reiterated the importance of freedom of navigation and safe overflight to the region’s prosperity.',\n",
              " 'On Taiwan, President Biden underscored that the United States remains committed to the “one China” policy, guided by the Taiwan Relations Act, the three Joint Communiques, and the Six Assurances, and that the United States strongly opposes unilateral efforts to change the status quo or undermine peace and stability across the Taiwan Strait.',\n",
              " 'President Biden also underscored the importance of managing strategic risks.',\n",
              " 'He noted the need for common-sense guardrails to ensure that competition does not veer into conflict and to keep lines of communication open.',\n",
              " 'He raised specific transnational challenges where our interests intersect, such as health security.',\n",
              " 'In particular, the two leaders discussed the existential nature of the climate crisis to the world and the important role that the United States and the PRC play.',\n",
              " 'They also discussed the importance of taking measures to address global energy supplies.',\n",
              " 'The two leaders also exchanged views on key regional challenges, including DPRK, Afghanistan, and Iran.',\n",
              " 'Finally, they discussed ways for the two sides to continue discussions on a number of areas, with President Biden underscoring the importance of substantive and concrete conversations.',\n",
              " 'When Chinese President Xi Jinping beamed into the White House on Monday evening for a virtual summit with President Joe Biden, the two men needed no introduction.',\n",
              " '\"We\\'ve spent an awful lot of time talking to one another, and I hope we can have a candid conversation tonight as well,\" Biden said as the talks got underway, sitting at the head of the Roosevelt Room table as Xi\\'s visage was broadcast on a pair of television screens.',\n",
              " 'From his seat in a cavernous room inside the Great Hall of the People in Beijing, Xi was just as friendly.',\n",
              " '\"Although it\\'s not as good as a face-to-face meeting,\" Xi said as the summit got underway, \"I\\'m very happy to see my old friend.\"',\n",
              " \"It was an auspicious start to some of the most critical talks of Biden's presidency, given the deteriorating ties between Washington and Beijing and the reality, acknowledged by administration officials, that managing the US relationship with China will amount to Biden's most critical international objective.\",\n",
              " 'The affable greetings eventually turned more serious as Biden raised concerns about human rights, Chinese aggression toward Taiwan and trade issues.',\n",
              " 'Throughout, the leaders engaged in a \"healthy debate,\" according to a senior administration official present for the discussions.',\n",
              " 'Officials said the three-and-a-half hour summit, which stretched longer than planned, allowed the two men ample opportunity to diverge from their prepared talking points.',\n",
              " 'The tone remained \"respectful and straightforward,\" the officials said.',\n",
              " 'But the highly anticipated summit yielded no major breakthroughs -- none were expected ahead of time -- and officials dismissed the notion the summit was intended to ease what has become an increasingly tense relationship.',\n",
              " '\"I don\\'t think the purpose was particularly to ease tensions, or that that was the result. We want to make sure the competition is responsibly managed, that we have ways to do that.',\n",
              " 'The President\\'s been quite clear he\\'s going to engage in that stiff competition,\" the senior administration official said afterward.',\n",
              " \"Biden is fond of citing the dozens of hours and thousands of miles he clocked with Xi when both were serving as their country's vice presidents.\",\n",
              " \"He's claimed to have spent more time with the Chinese president than any other world leader.\",\n",
              " 'But things have changed since Biden, as he likes to recall, was dining with Xi on the Tibetan Plateau and describing the United States in one word: \"possibilities.\"',\n",
              " \"Now, the world's two largest economies are engaged in fierce tensions on trade, military aggression and human rights.\",\n",
              " \"And Biden, who initiated Monday evening's virtual summit, finds himself in a high-wire act with China's most powerful leader in decades.\",\n",
              " \"During the summit, each man recounted stories from their time traveling with each other, sometimes quoting each other's words from that era, the senior administration official said.\",\n",
              " 'As the talks were getting underway, Biden said he was expecting to discuss a wide-ranging and substantive agenda.',\n",
              " '\"As I\\'ve said before, it seems to me our responsibility as leaders of China and the United States is to ensure the competition between our two countries does not veer into conflict, either intended or unintended.\"',\n",
              " '\"Just simply straightforward competition,\" he said, speaking to Xi through a translator.',\n",
              " '\"It seems to me we have to establish some common sense guardrails, to be clear and honest where we disagree and work together where our interests intersect,\" Biden went on, asking to communicate \"honestly and directly\" over the range of topics up for discussion.',\n",
              " '\"We never walk away wondering what the other man is thinking,\" he said.',\n",
              " 'Afterward, the White House said Biden raised concerns about human rights abuses against the Uyghur minority in the western Xinjiang Provence and in Tibet.',\n",
              " 'Taiwan, which has been a source of increased tension in recent months, was a topic of extensive discussion during the summit.',\n",
              " 'Biden stressed the importance of the \"One China\" policy, and was direct in his concerns about Chinese behavior that threatens stability in the Taiwan strait.',\n",
              " 'But he did not set any of the new \"guardrails\" referenced at the start of the talks.',\n",
              " \"On Covid-19, Biden reaffirmed the importance of transparency in preventing future outbreaks of disease, a nod to China's unwillingness to allow an international investigation into the origins of the current pandemic.\",\n",
              " 'And he raised areas where the US and China can cooperate, including on climate change.',\n",
              " 'The two countries recently surprised observers at the COP26 climate talks in Scotland with a joint pledge to cut emissions.',\n",
              " 'After speculation Xi might use the meeting to invite Biden to the upcoming Beijing Winter Olympics, officials said the topic did not arise.',\n",
              " \"White House officials had hoped a large South Lawn signing ceremony for a massive new public works package, which occurred a few hours before Biden's virtual summit, would help signal progress on the main underpinning of his foreign policy: proving democracies can deliver more effectively than autocracies like China.\",\n",
              " 'He planned to detail the new infrastructure package to Xi.',\n",
              " \"The fact the bill was passed with help from some Republicans -- fulfilling Biden's promise to work across party lines -- helps sustain his pledge to prove democracies can work, according to the officials.\",\n",
              " 'Yet he still entered the talks at a politically weakened moment.',\n",
              " 'His party fared poorly in off-year elections this month in Virginia, and polls continue to show his approval rating at some of the lowest levels of his presidency.',\n",
              " 'That is in sharp contrast to Xi, whose consolidation of power in China was cemented last week when the Chinese Communist Party adopted a landmark resolution elevating him in stature to his two most powerful predecessors -- Mao Zedong and Deng Xiaoping.',\n",
              " \"Officials said the upgrading of Xi's status only enhanced the imperative of a face-to-face with Biden.\",\n",
              " 'Nearly every issue Biden is focused on, domestically and internationally, has a nexus to China.',\n",
              " 'Supply chain issues that are driving inflation at home can be traced in part to shortages in Chinese plants.',\n",
              " 'Combatting climate change requires buy-in from Xi, who has shown some willingness to partner with Biden on the issue.',\n",
              " 'Managing global trouble-spots like North Korea and Iran each involves coordination with Beijing.',\n",
              " 'Biden is a fan of in-person meetings and complained early in his presidency that virtual summits -- where foreign leaders are patched in on video screens -- could not replicate the chemistry of sitting face-to-face.American officials say leader-to-leader meetings are even more important with Xi, whose inner circle has become smaller and smaller and who now wields a historic level of power.',\n",
              " \"Over the summer, aides were hopeful of setting up a meeting between the two men on the sidelines of this year's Group of 20 summit in Rome.\",\n",
              " 'But Xi has not left China in nearly two years, partly over Covid-19 concerns.',\n",
              " 'So Biden settled on a virtual summit instead as a way to advance his two previous phone conversations with Xi.',\n",
              " '\"There is something different about actually seeing someone physically, about the depth of the conversation you can have, versus just on a regular phone line,\" the senior administration official said earlier, describing different ways of preparing for a video conference compared to just a phone conversation.']"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_data = [\"President Joseph R. Biden, Jr. met virtually on November 15 with President Xi Jinping of the People’s Republic of China (PRC).\",\n",
        "            \"The two leaders discussed the complex nature of relations between our two countries and the importance of managing competition responsibly.\",\n",
        "            \"As in previous discussions, the two leaders covered areas where our interests align, and areas where our interests, values, and perspectives diverge.\",\n",
        "            \"President Biden welcomed the opportunity to speak candidly and straightforwardly to President Xi about our intentions and priorities across a range of issues.\",\n",
        "            \"President Biden underscored that the United States will continue to stand up for its interests and values and, together with our allies and partners, ensure the rules of the road for the 21st century advance an international system that is free, open, and fair.\",\n",
        "            \"He emphasized the priority he places on far-reaching investments at home while we align with allies and partners abroad to take on the challenges of our time.\",\n",
        "            \"President Biden raised concerns about the PRC’s practices in Xinjiang, Tibet, and Hong Kong, as well as human rights more broadly.\",\n",
        "            \"He was clear about the need to protect American workers and industries from the PRC’s unfair trade and economic practices.\",\n",
        "            \"He also discussed the importance of a free and open Indo-Pacific, and communicated the continued determination of the United States to uphold our commitments in the region.\",\n",
        "            \"President Biden reiterated the importance of freedom of navigation and safe overflight to the region’s prosperity.\",\n",
        "            \"On Taiwan, President Biden underscored that the United States remains committed to the “one China” policy, guided by the Taiwan Relations Act, the three Joint Communiques, and the Six Assurances, and that the United States strongly opposes unilateral efforts to change the status quo or undermine peace and stability across the Taiwan Strait.\",\n",
        "            \"President Biden also underscored the importance of managing strategic risks.\",\n",
        "            \"He noted the need for common-sense guardrails to ensure that competition does not veer into conflict and to keep lines of communication open.\",\n",
        "            \"He raised specific transnational challenges where our interests intersect, such as health security.\",\n",
        "            \"In particular, the two leaders discussed the existential nature of the climate crisis to the world and the important role that the United States and the PRC play.\",\n",
        "            \"They also discussed the importance of taking measures to address global energy supplies.\",\n",
        "            \"The two leaders also exchanged views on key regional challenges, including DPRK, Afghanistan, and Iran.\",\n",
        "            \"Finally, they discussed ways for the two sides to continue discussions on a number of areas, with President Biden underscoring the importance of substantive and concrete conversations.\",\n",
        "            \"When Chinese President Xi Jinping beamed into the White House on Monday evening for a virtual summit with President Joe Biden, the two men needed no introduction.\",\n",
        "            \"\\\"We've spent an awful lot of time talking to one another, and I hope we can have a candid conversation tonight as well,\\\" Biden said as the talks got underway, sitting at the head of the Roosevelt Room table as Xi's visage was broadcast on a pair of television screens.\",\n",
        "            \"From his seat in a cavernous room inside the Great Hall of the People in Beijing, Xi was just as friendly.\",\n",
        "            \"\\\"Although it's not as good as a face-to-face meeting,\\\" Xi said as the summit got underway, \\\"I'm very happy to see my old friend.\\\"\",\n",
        "            \"It was an auspicious start to some of the most critical talks of Biden's presidency, given the deteriorating ties between Washington and Beijing and the reality, acknowledged by administration officials, that managing the US relationship with China will amount to Biden's most critical international objective.\",\n",
        "            \"The affable greetings eventually turned more serious as Biden raised concerns about human rights, Chinese aggression toward Taiwan and trade issues.\",\n",
        "            \"Throughout, the leaders engaged in a \\\"healthy debate,\\\" according to a senior administration official present for the discussions.\",\n",
        "            \"Officials said the three-and-a-half hour summit, which stretched longer than planned, allowed the two men ample opportunity to diverge from their prepared talking points.\",\n",
        "            \"The tone remained \\\"respectful and straightforward,\\\" the officials said.\",\n",
        "            \"But the highly anticipated summit yielded no major breakthroughs -- none were expected ahead of time -- and officials dismissed the notion the summit was intended to ease what has become an increasingly tense relationship.\",\n",
        "            \"\\\"I don't think the purpose was particularly to ease tensions, or that that was the result. We want to make sure the competition is responsibly managed, that we have ways to do that.\",\n",
        "            \"The President's been quite clear he's going to engage in that stiff competition,\\\" the senior administration official said afterward.\",\n",
        "            \"Biden is fond of citing the dozens of hours and thousands of miles he clocked with Xi when both were serving as their country's vice presidents.\",\n",
        "            \"He's claimed to have spent more time with the Chinese president than any other world leader.\",\n",
        "            \"But things have changed since Biden, as he likes to recall, was dining with Xi on the Tibetan Plateau and describing the United States in one word: \\\"possibilities.\\\"\",\n",
        "            \"Now, the world's two largest economies are engaged in fierce tensions on trade, military aggression and human rights.\",\n",
        "            \"And Biden, who initiated Monday evening's virtual summit, finds himself in a high-wire act with China's most powerful leader in decades.\",\n",
        "            \"During the summit, each man recounted stories from their time traveling with each other, sometimes quoting each other's words from that era, the senior administration official said.\",\n",
        "            \"As the talks were getting underway, Biden said he was expecting to discuss a wide-ranging and substantive agenda.\",\n",
        "            \"\\\"As I've said before, it seems to me our responsibility as leaders of China and the United States is to ensure the competition between our two countries does not veer into conflict, either intended or unintended.\\\"\",\n",
        "            \"\\\"Just simply straightforward competition,\\\" he said, speaking to Xi through a translator.\",\n",
        "            \"\\\"It seems to me we have to establish some common sense guardrails, to be clear and honest where we disagree and work together where our interests intersect,\\\" Biden went on, asking to communicate \\\"honestly and directly\\\" over the range of topics up for discussion.\",\n",
        "            \"\\\"We never walk away wondering what the other man is thinking,\\\" he said.\",\n",
        "            \"Afterward, the White House said Biden raised concerns about human rights abuses against the Uyghur minority in the western Xinjiang Provence and in Tibet.\",\n",
        "            \"Taiwan, which has been a source of increased tension in recent months, was a topic of extensive discussion during the summit.\",\n",
        "            \"Biden stressed the importance of the \\\"One China\\\" policy, and was direct in his concerns about Chinese behavior that threatens stability in the Taiwan strait.\",\n",
        "            \"But he did not set any of the new \\\"guardrails\\\" referenced at the start of the talks.\",\n",
        "            \"On Covid-19, Biden reaffirmed the importance of transparency in preventing future outbreaks of disease, a nod to China's unwillingness to allow an international investigation into the origins of the current pandemic.\",\n",
        "            \"And he raised areas where the US and China can cooperate, including on climate change.\",\n",
        "            \"The two countries recently surprised observers at the COP26 climate talks in Scotland with a joint pledge to cut emissions.\",\n",
        "            \"After speculation Xi might use the meeting to invite Biden to the upcoming Beijing Winter Olympics, officials said the topic did not arise.\",\n",
        "            \"White House officials had hoped a large South Lawn signing ceremony for a massive new public works package, which occurred a few hours before Biden's virtual summit, would help signal progress on the main underpinning of his foreign policy: proving democracies can deliver more effectively than autocracies like China.\",\n",
        "            \"He planned to detail the new infrastructure package to Xi.\",\n",
        "            \"The fact the bill was passed with help from some Republicans -- fulfilling Biden's promise to work across party lines -- helps sustain his pledge to prove democracies can work, according to the officials.\",\n",
        "            \"Yet he still entered the talks at a politically weakened moment.\",\n",
        "            \"His party fared poorly in off-year elections this month in Virginia, and polls continue to show his approval rating at some of the lowest levels of his presidency.\",\n",
        "            \"That is in sharp contrast to Xi, whose consolidation of power in China was cemented last week when the Chinese Communist Party adopted a landmark resolution elevating him in stature to his two most powerful predecessors -- Mao Zedong and Deng Xiaoping.\",\n",
        "            \"Officials said the upgrading of Xi's status only enhanced the imperative of a face-to-face with Biden.\",\n",
        "            \"Nearly every issue Biden is focused on, domestically and internationally, has a nexus to China.\",\n",
        "            \"Supply chain issues that are driving inflation at home can be traced in part to shortages in Chinese plants.\",\n",
        "            \"Combatting climate change requires buy-in from Xi, who has shown some willingness to partner with Biden on the issue.\",\n",
        "            \"Managing global trouble-spots like North Korea and Iran each involves coordination with Beijing.\",\n",
        "            \"Biden is a fan of in-person meetings and complained early in his presidency that virtual summits -- where foreign leaders are patched in on video screens -- could not replicate the chemistry of sitting face-to-face.\"\n",
        "            \"American officials say leader-to-leader meetings are even more important with Xi, whose inner circle has become smaller and smaller and who now wields a historic level of power.\",\n",
        "            \"Over the summer, aides were hopeful of setting up a meeting between the two men on the sidelines of this year's Group of 20 summit in Rome.\",\n",
        "            \"But Xi has not left China in nearly two years, partly over Covid-19 concerns.\",\n",
        "            \"So Biden settled on a virtual summit instead as a way to advance his two previous phone conversations with Xi.\",\n",
        "            \"\\\"There is something different about actually seeing someone physically, about the depth of the conversation you can have, versus just on a regular phone line,\\\" the senior administration official said earlier, describing different ways of preparing for a video conference compared to just a phone conversation.\",\n",
        "           ]\n",
        "training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5458410c",
      "metadata": {
        "id": "5458410c"
      },
      "outputs": [],
      "source": [
        "# define a function that will tell me where a token occurs\n",
        "def where_is(token, sentence_dict):\n",
        "    found = {}\n",
        "    for sid in sentence_dict:\n",
        "        found[sid] = [i for i, word in enumerate(sentence_dict[sid]) if word == token]\n",
        "    return found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e589be1c",
      "metadata": {
        "id": "e589be1c",
        "outputId": "62ddd0ae-3813-490e-8787-585c348314c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 [CLS]           101\n",
            "1 President     1,697\n",
            "2 Joseph        2,419\n",
            "3 R               155\n",
            "4 .               119\n",
            "5 B               139\n",
            "6 ##iden       26,859\n",
            "7 ,               117\n",
            "8 Jr            3,108\n",
            "9 .               119\n",
            "10 met           1,899\n",
            "11 virtually     9,024\n",
            "12 on            1,113\n",
            "13 November      1,379\n",
            "14 15            1,405\n",
            "15 with          1,114\n",
            "16 President     1,697\n",
            "17 Xi           20,802\n",
            "18 Jin          10,922\n",
            "19 ##ping        2,624\n",
            "20 of            1,104\n",
            "21 the           1,103\n",
            "22 People        2,563\n",
            "23 ’               787\n",
            "24 s               188\n",
            "25 Republic      2,250\n",
            "26 of            1,104\n",
            "27 China         1,975\n",
            "28 (               113\n",
            "29 PR           11,629\n",
            "30 ##C           1,658\n",
            "31 )               114\n",
            "32 .               119\n",
            "33 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 34\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 34 x 3072\n",
            "Summed embeddings shape is: 34 x 768\n",
            "0 President     1,697\n",
            "1 Joseph        2,419\n",
            "2 R               155\n",
            "3 .               119\n",
            "4 B               139\n",
            "5 ##iden       26,859\n",
            "6 ,               117\n",
            "7 Jr            3,108\n",
            "8 .               119\n",
            "9 met           1,899\n",
            "10 virtually     9,024\n",
            "11 on            1,113\n",
            "12 November      1,379\n",
            "13 15            1,405\n",
            "14 with          1,114\n",
            "15 President     1,697\n",
            "16 Xi           20,802\n",
            "17 Jin          10,922\n",
            "18 ##ping        2,624\n",
            "19 of            1,104\n",
            "20 the           1,103\n",
            "21 People        2,563\n",
            "22 ’               787\n",
            "23 s               188\n",
            "24 Republic      2,250\n",
            "25 of            1,104\n",
            "26 China         1,975\n",
            "27 (               113\n",
            "28 PR           11,629\n",
            "29 ##C           1,658\n",
            "30 )               114\n",
            "31 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 32\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 32 x 3072\n",
            "Summed embeddings shape is: 32 x 768\n",
            "0 [CLS]           101\n",
            "1 The           1,109\n",
            "2 two           1,160\n",
            "3 leaders       3,478\n",
            "4 discussed     6,352\n",
            "5 the           1,103\n",
            "6 complex       2,703\n",
            "7 nature        2,731\n",
            "8 of            1,104\n",
            "9 relations     4,125\n",
            "10 between       1,206\n",
            "11 our           1,412\n",
            "12 two           1,160\n",
            "13 countries     2,182\n",
            "14 and           1,105\n",
            "15 the           1,103\n",
            "16 importance    4,495\n",
            "17 of            1,104\n",
            "18 managing      7,204\n",
            "19 competition   2,208\n",
            "20 re            1,231\n",
            "21 ##sp         20,080\n",
            "22 ##ons         4,199\n",
            "23 ##ibly       15,298\n",
            "24 .               119\n",
            "25 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 26\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 26 x 3072\n",
            "Summed embeddings shape is: 26 x 768\n",
            "0 The           1,109\n",
            "1 two           1,160\n",
            "2 leaders       3,478\n",
            "3 discussed     6,352\n",
            "4 the           1,103\n",
            "5 complex       2,703\n",
            "6 nature        2,731\n",
            "7 of            1,104\n",
            "8 relations     4,125\n",
            "9 between       1,206\n",
            "10 our           1,412\n",
            "11 two           1,160\n",
            "12 countries     2,182\n",
            "13 and           1,105\n",
            "14 the           1,103\n",
            "15 importance    4,495\n",
            "16 of            1,104\n",
            "17 managing      7,204\n",
            "18 competition   2,208\n",
            "19 re            1,231\n",
            "20 ##sp         20,080\n",
            "21 ##ons         4,199\n",
            "22 ##ibly       15,298\n",
            "23 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 24 x 3072\n",
            "Summed embeddings shape is: 24 x 768\n",
            "0 [CLS]           101\n",
            "1 As            1,249\n",
            "2 in            1,107\n",
            "3 previous      2,166\n",
            "4 discussions  10,508\n",
            "5 ,               117\n",
            "6 the           1,103\n",
            "7 two           1,160\n",
            "8 leaders       3,478\n",
            "9 covered       2,262\n",
            "10 areas         1,877\n",
            "11 where         1,187\n",
            "12 our           1,412\n",
            "13 interests     4,740\n",
            "14 al            2,393\n",
            "15 ##ign        11,368\n",
            "16 ,               117\n",
            "17 and           1,105\n",
            "18 areas         1,877\n",
            "19 where         1,187\n",
            "20 our           1,412\n",
            "21 interests     4,740\n",
            "22 ,               117\n",
            "23 values        4,718\n",
            "24 ,               117\n",
            "25 and           1,105\n",
            "26 perspectives 22,168\n",
            "27 diver        23,448\n",
            "28 ##ge          2,176\n",
            "29 .               119\n",
            "30 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 31\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 31 x 3072\n",
            "Summed embeddings shape is: 31 x 768\n",
            "0 As            1,249\n",
            "1 in            1,107\n",
            "2 previous      2,166\n",
            "3 discussions  10,508\n",
            "4 ,               117\n",
            "5 the           1,103\n",
            "6 two           1,160\n",
            "7 leaders       3,478\n",
            "8 covered       2,262\n",
            "9 areas         1,877\n",
            "10 where         1,187\n",
            "11 our           1,412\n",
            "12 interests     4,740\n",
            "13 al            2,393\n",
            "14 ##ign        11,368\n",
            "15 ,               117\n",
            "16 and           1,105\n",
            "17 areas         1,877\n",
            "18 where         1,187\n",
            "19 our           1,412\n",
            "20 interests     4,740\n",
            "21 ,               117\n",
            "22 values        4,718\n",
            "23 ,               117\n",
            "24 and           1,105\n",
            "25 perspectives 22,168\n",
            "26 diver        23,448\n",
            "27 ##ge          2,176\n",
            "28 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 29\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 29 x 3072\n",
            "Summed embeddings shape is: 29 x 768\n",
            "0 [CLS]           101\n",
            "1 President     1,697\n",
            "2 B               139\n",
            "3 ##iden       26,859\n",
            "4 welcomed     10,977\n",
            "5 the           1,103\n",
            "6 opportunity   3,767\n",
            "7 to            1,106\n",
            "8 speak         2,936\n",
            "9 can           1,169\n",
            "10 ##di          3,309\n",
            "11 ##dly        18,733\n",
            "12 and           1,105\n",
            "13 straightforward 21,546\n",
            "14 ##ly          1,193\n",
            "15 to            1,106\n",
            "16 President     1,697\n",
            "17 Xi           20,802\n",
            "18 about         1,164\n",
            "19 our           1,412\n",
            "20 intentions   11,489\n",
            "21 and           1,105\n",
            "22 priorities   21,249\n",
            "23 across        1,506\n",
            "24 a               170\n",
            "25 range         2,079\n",
            "26 of            1,104\n",
            "27 issues        2,492\n",
            "28 .               119\n",
            "29 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 30\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 30 x 3072\n",
            "Summed embeddings shape is: 30 x 768\n",
            "0 President     1,697\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 welcomed     10,977\n",
            "4 the           1,103\n",
            "5 opportunity   3,767\n",
            "6 to            1,106\n",
            "7 speak         2,936\n",
            "8 can           1,169\n",
            "9 ##di          3,309\n",
            "10 ##dly        18,733\n",
            "11 and           1,105\n",
            "12 straightforward 21,546\n",
            "13 ##ly          1,193\n",
            "14 to            1,106\n",
            "15 President     1,697\n",
            "16 Xi           20,802\n",
            "17 about         1,164\n",
            "18 our           1,412\n",
            "19 intentions   11,489\n",
            "20 and           1,105\n",
            "21 priorities   21,249\n",
            "22 across        1,506\n",
            "23 a               170\n",
            "24 range         2,079\n",
            "25 of            1,104\n",
            "26 issues        2,492\n",
            "27 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 28\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 28 x 3072\n",
            "Summed embeddings shape is: 28 x 768\n",
            "0 [CLS]           101\n",
            "1 President     1,697\n",
            "2 B               139\n",
            "3 ##iden       26,859\n",
            "4 under         1,223\n",
            "5 ##sco        11,428\n",
            "6 ##red         4,359\n",
            "7 that          1,115\n",
            "8 the           1,103\n",
            "9 United        1,244\n",
            "10 States        1,311\n",
            "11 will          1,209\n",
            "12 continue      2,760\n",
            "13 to            1,106\n",
            "14 stand         2,484\n",
            "15 up            1,146\n",
            "16 for           1,111\n",
            "17 its           1,157\n",
            "18 interests     4,740\n",
            "19 and           1,105\n",
            "20 values        4,718\n",
            "21 and           1,105\n",
            "22 ,               117\n",
            "23 together      1,487\n",
            "24 with          1,114\n",
            "25 our           1,412\n",
            "26 allies        8,224\n",
            "27 and           1,105\n",
            "28 partners      6,449\n",
            "29 ,               117\n",
            "30 ensure        4,989\n",
            "31 the           1,103\n",
            "32 rules         2,995\n",
            "33 of            1,104\n",
            "34 the           1,103\n",
            "35 road          1,812\n",
            "36 for           1,111\n",
            "37 the           1,103\n",
            "38 21st          6,880\n",
            "39 century       1,432\n",
            "40 advance       4,657\n",
            "41 an            1,126\n",
            "42 international  1,835\n",
            "43 system        1,449\n",
            "44 that          1,115\n",
            "45 is            1,110\n",
            "46 free          1,714\n",
            "47 ,               117\n",
            "48 open          1,501\n",
            "49 ,               117\n",
            "50 and           1,105\n",
            "51 fair          4,652\n",
            "52 .               119\n",
            "53 [SEP]           102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 54\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 54 x 3072\n",
            "Summed embeddings shape is: 54 x 768\n",
            "0 President     1,697\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 under         1,223\n",
            "4 ##sco        11,428\n",
            "5 ##red         4,359\n",
            "6 that          1,115\n",
            "7 the           1,103\n",
            "8 United        1,244\n",
            "9 States        1,311\n",
            "10 will          1,209\n",
            "11 continue      2,760\n",
            "12 to            1,106\n",
            "13 stand         2,484\n",
            "14 up            1,146\n",
            "15 for           1,111\n",
            "16 its           1,157\n",
            "17 interests     4,740\n",
            "18 and           1,105\n",
            "19 values        4,718\n",
            "20 and           1,105\n",
            "21 ,               117\n",
            "22 together      1,487\n",
            "23 with          1,114\n",
            "24 our           1,412\n",
            "25 allies        8,224\n",
            "26 and           1,105\n",
            "27 partners      6,449\n",
            "28 ,               117\n",
            "29 ensure        4,989\n",
            "30 the           1,103\n",
            "31 rules         2,995\n",
            "32 of            1,104\n",
            "33 the           1,103\n",
            "34 road          1,812\n",
            "35 for           1,111\n",
            "36 the           1,103\n",
            "37 21st          6,880\n",
            "38 century       1,432\n",
            "39 advance       4,657\n",
            "40 an            1,126\n",
            "41 international  1,835\n",
            "42 system        1,449\n",
            "43 that          1,115\n",
            "44 is            1,110\n",
            "45 free          1,714\n",
            "46 ,               117\n",
            "47 open          1,501\n",
            "48 ,               117\n",
            "49 and           1,105\n",
            "50 fair          4,652\n",
            "51 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 52\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 52 x 3072\n",
            "Summed embeddings shape is: 52 x 768\n",
            "0 [CLS]           101\n",
            "1 He            1,124\n",
            "2 emphasized   13,463\n",
            "3 the           1,103\n",
            "4 priority      9,830\n",
            "5 he            1,119\n",
            "6 places        2,844\n",
            "7 on            1,113\n",
            "8 far           1,677\n",
            "9 -               118\n",
            "10 reaching      3,634\n",
            "11 investments  12,372\n",
            "12 at            1,120\n",
            "13 home          1,313\n",
            "14 while         1,229\n",
            "15 we            1,195\n",
            "16 al            2,393\n",
            "17 ##ign        11,368\n",
            "18 with          1,114\n",
            "19 allies        8,224\n",
            "20 and           1,105\n",
            "21 partners      6,449\n",
            "22 abroad        6,629\n",
            "23 to            1,106\n",
            "24 take          1,321\n",
            "25 on            1,113\n",
            "26 the           1,103\n",
            "27 challenges    7,806\n",
            "28 of            1,104\n",
            "29 our           1,412\n",
            "30 time          1,159\n",
            "31 .               119\n",
            "32 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 33\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 33 x 3072\n",
            "Summed embeddings shape is: 33 x 768\n",
            "0 He            1,124\n",
            "1 emphasized   13,463\n",
            "2 the           1,103\n",
            "3 priority      9,830\n",
            "4 he            1,119\n",
            "5 places        2,844\n",
            "6 on            1,113\n",
            "7 far           1,677\n",
            "8 -               118\n",
            "9 reaching      3,634\n",
            "10 investments  12,372\n",
            "11 at            1,120\n",
            "12 home          1,313\n",
            "13 while         1,229\n",
            "14 we            1,195\n",
            "15 al            2,393\n",
            "16 ##ign        11,368\n",
            "17 with          1,114\n",
            "18 allies        8,224\n",
            "19 and           1,105\n",
            "20 partners      6,449\n",
            "21 abroad        6,629\n",
            "22 to            1,106\n",
            "23 take          1,321\n",
            "24 on            1,113\n",
            "25 the           1,103\n",
            "26 challenges    7,806\n",
            "27 of            1,104\n",
            "28 our           1,412\n",
            "29 time          1,159\n",
            "30 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 31\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 31 x 3072\n",
            "Summed embeddings shape is: 31 x 768\n",
            "0 [CLS]           101\n",
            "1 President     1,697\n",
            "2 B               139\n",
            "3 ##iden       26,859\n",
            "4 raised        2,120\n",
            "5 concerns      5,365\n",
            "6 about         1,164\n",
            "7 the           1,103\n",
            "8 PR           11,629\n",
            "9 ##C           1,658\n",
            "10 ’               787\n",
            "11 s               188\n",
            "12 practices     5,660\n",
            "13 in            1,107\n",
            "14 Xi           20,802\n",
            "15 ##nji        21,440\n",
            "16 ##ang         4,993\n",
            "17 ,               117\n",
            "18 Tibet        13,659\n",
            "19 ,               117\n",
            "20 and           1,105\n",
            "21 Hong          3,475\n",
            "22 Kong          3,462\n",
            "23 ,               117\n",
            "24 as            1,112\n",
            "25 well          1,218\n",
            "26 as            1,112\n",
            "27 human         1,769\n",
            "28 rights        2,266\n",
            "29 more          1,167\n",
            "30 broadly      14,548\n",
            "31 .               119\n",
            "32 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 33\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 33 x 3072\n",
            "Summed embeddings shape is: 33 x 768\n",
            "0 President     1,697\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 raised        2,120\n",
            "4 concerns      5,365\n",
            "5 about         1,164\n",
            "6 the           1,103\n",
            "7 PR           11,629\n",
            "8 ##C           1,658\n",
            "9 ’               787\n",
            "10 s               188\n",
            "11 practices     5,660\n",
            "12 in            1,107\n",
            "13 Xi           20,802\n",
            "14 ##nji        21,440\n",
            "15 ##ang         4,993\n",
            "16 ,               117\n",
            "17 Tibet        13,659\n",
            "18 ,               117\n",
            "19 and           1,105\n",
            "20 Hong          3,475\n",
            "21 Kong          3,462\n",
            "22 ,               117\n",
            "23 as            1,112\n",
            "24 well          1,218\n",
            "25 as            1,112\n",
            "26 human         1,769\n",
            "27 rights        2,266\n",
            "28 more          1,167\n",
            "29 broadly      14,548\n",
            "30 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 31\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 31 x 3072\n",
            "Summed embeddings shape is: 31 x 768\n",
            "0 [CLS]           101\n",
            "1 He            1,124\n",
            "2 was           1,108\n",
            "3 clear         2,330\n",
            "4 about         1,164\n",
            "5 the           1,103\n",
            "6 need          1,444\n",
            "7 to            1,106\n",
            "8 protect       3,244\n",
            "9 American      1,237\n",
            "10 workers       3,239\n",
            "11 and           1,105\n",
            "12 industries    7,519\n",
            "13 from          1,121\n",
            "14 the           1,103\n",
            "15 PR           11,629\n",
            "16 ##C           1,658\n",
            "17 ’               787\n",
            "18 s               188\n",
            "19 unfair       17,111\n",
            "20 trade         2,597\n",
            "21 and           1,105\n",
            "22 economic      2,670\n",
            "23 practices     5,660\n",
            "24 .               119\n",
            "25 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 26\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 26 x 3072\n",
            "Summed embeddings shape is: 26 x 768\n",
            "0 He            1,124\n",
            "1 was           1,108\n",
            "2 clear         2,330\n",
            "3 about         1,164\n",
            "4 the           1,103\n",
            "5 need          1,444\n",
            "6 to            1,106\n",
            "7 protect       3,244\n",
            "8 American      1,237\n",
            "9 workers       3,239\n",
            "10 and           1,105\n",
            "11 industries    7,519\n",
            "12 from          1,121\n",
            "13 the           1,103\n",
            "14 PR           11,629\n",
            "15 ##C           1,658\n",
            "16 ’               787\n",
            "17 s               188\n",
            "18 unfair       17,111\n",
            "19 trade         2,597\n",
            "20 and           1,105\n",
            "21 economic      2,670\n",
            "22 practices     5,660\n",
            "23 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 24 x 3072\n",
            "Summed embeddings shape is: 24 x 768\n",
            "0 [CLS]           101\n",
            "1 He            1,124\n",
            "2 also          1,145\n",
            "3 discussed     6,352\n",
            "4 the           1,103\n",
            "5 importance    4,495\n",
            "6 of            1,104\n",
            "7 a               170\n",
            "8 free          1,714\n",
            "9 and           1,105\n",
            "10 open          1,501\n",
            "11 Indo         11,501\n",
            "12 -               118\n",
            "13 Pacific       2,662\n",
            "14 ,               117\n",
            "15 and           1,105\n",
            "16 communicated 27,263\n",
            "17 the           1,103\n",
            "18 continued     1,598\n",
            "19 determination  9,220\n",
            "20 of            1,104\n",
            "21 the           1,103\n",
            "22 United        1,244\n",
            "23 States        1,311\n",
            "24 to            1,106\n",
            "25 up            1,146\n",
            "26 ##hold        8,678\n",
            "27 our           1,412\n",
            "28 commitments  19,716\n",
            "29 in            1,107\n",
            "30 the           1,103\n",
            "31 region        1,805\n",
            "32 .               119\n",
            "33 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 34\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 34 x 3072\n",
            "Summed embeddings shape is: 34 x 768\n",
            "0 He            1,124\n",
            "1 also          1,145\n",
            "2 discussed     6,352\n",
            "3 the           1,103\n",
            "4 importance    4,495\n",
            "5 of            1,104\n",
            "6 a               170\n",
            "7 free          1,714\n",
            "8 and           1,105\n",
            "9 open          1,501\n",
            "10 Indo         11,501\n",
            "11 -               118\n",
            "12 Pacific       2,662\n",
            "13 ,               117\n",
            "14 and           1,105\n",
            "15 communicated 27,263\n",
            "16 the           1,103\n",
            "17 continued     1,598\n",
            "18 determination  9,220\n",
            "19 of            1,104\n",
            "20 the           1,103\n",
            "21 United        1,244\n",
            "22 States        1,311\n",
            "23 to            1,106\n",
            "24 up            1,146\n",
            "25 ##hold        8,678\n",
            "26 our           1,412\n",
            "27 commitments  19,716\n",
            "28 in            1,107\n",
            "29 the           1,103\n",
            "30 region        1,805\n",
            "31 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 32\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 32 x 3072\n",
            "Summed embeddings shape is: 32 x 768\n",
            "0 [CLS]           101\n",
            "1 President     1,697\n",
            "2 B               139\n",
            "3 ##iden       26,859\n",
            "4 re            1,231\n",
            "5 ##iter       19,385\n",
            "6 ##ated        2,913\n",
            "7 the           1,103\n",
            "8 importance    4,495\n",
            "9 of            1,104\n",
            "10 freedom       4,438\n",
            "11 of            1,104\n",
            "12 navigation   11,167\n",
            "13 and           1,105\n",
            "14 safe          2,914\n",
            "15 over          1,166\n",
            "16 ##f           2,087\n",
            "17 ##light       4,568\n",
            "18 to            1,106\n",
            "19 the           1,103\n",
            "20 region        1,805\n",
            "21 ’               787\n",
            "22 s               188\n",
            "23 prosperity   16,286\n",
            "24 .               119\n",
            "25 [SEP]           102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 26\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 26 x 3072\n",
            "Summed embeddings shape is: 26 x 768\n",
            "0 President     1,697\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 re            1,231\n",
            "4 ##iter       19,385\n",
            "5 ##ated        2,913\n",
            "6 the           1,103\n",
            "7 importance    4,495\n",
            "8 of            1,104\n",
            "9 freedom       4,438\n",
            "10 of            1,104\n",
            "11 navigation   11,167\n",
            "12 and           1,105\n",
            "13 safe          2,914\n",
            "14 over          1,166\n",
            "15 ##f           2,087\n",
            "16 ##light       4,568\n",
            "17 to            1,106\n",
            "18 the           1,103\n",
            "19 region        1,805\n",
            "20 ’               787\n",
            "21 s               188\n",
            "22 prosperity   16,286\n",
            "23 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 24 x 3072\n",
            "Summed embeddings shape is: 24 x 768\n",
            "0 [CLS]           101\n",
            "1 On            1,212\n",
            "2 Taiwan        6,036\n",
            "3 ,               117\n",
            "4 President     1,697\n",
            "5 B               139\n",
            "6 ##iden       26,859\n",
            "7 under         1,223\n",
            "8 ##sco        11,428\n",
            "9 ##red         4,359\n",
            "10 that          1,115\n",
            "11 the           1,103\n",
            "12 United        1,244\n",
            "13 States        1,311\n",
            "14 remains       2,606\n",
            "15 committed     4,762\n",
            "16 to            1,106\n",
            "17 the           1,103\n",
            "18 “               789\n",
            "19 one           1,141\n",
            "20 China         1,975\n",
            "21 ”               790\n",
            "22 policy        2,818\n",
            "23 ,               117\n",
            "24 guided        8,610\n",
            "25 by            1,118\n",
            "26 the           1,103\n",
            "27 Taiwan        6,036\n",
            "28 Relations     9,269\n",
            "29 Act           2,173\n",
            "30 ,               117\n",
            "31 the           1,103\n",
            "32 three         1,210\n",
            "33 Joint         7,885\n",
            "34 Co            3,291\n",
            "35 ##mm          6,262\n",
            "36 ##uni        19,782\n",
            "37 ##ques       11,962\n",
            "38 ,               117\n",
            "39 and           1,105\n",
            "40 the           1,103\n",
            "41 Six           4,995\n",
            "42 As            1,249\n",
            "43 ##su          6,385\n",
            "44 ##rance      10,555\n",
            "45 ##s           1,116\n",
            "46 ,               117\n",
            "47 and           1,105\n",
            "48 that          1,115\n",
            "49 the           1,103\n",
            "50 United        1,244\n",
            "51 States        1,311\n",
            "52 strongly      5,473\n",
            "53 oppose       16,315\n",
            "54 ##s           1,116\n",
            "55 un            8,362\n",
            "56 ##ila         8,009\n",
            "57 ##teral      16,719\n",
            "58 efforts       3,268\n",
            "59 to            1,106\n",
            "60 change        1,849\n",
            "61 the           1,103\n",
            "62 status        2,781\n",
            "63 q               186\n",
            "64 ##uo         11,848\n",
            "65 or            1,137\n",
            "66 under         1,223\n",
            "67 ##mine        9,685\n",
            "68 peace         3,519\n",
            "69 and           1,105\n",
            "70 stability     9,397\n",
            "71 across        1,506\n",
            "72 the           1,103\n",
            "73 Taiwan        6,036\n",
            "74 Strait       12,925\n",
            "75 .               119\n",
            "76 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 77\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 77 x 3072\n",
            "Summed embeddings shape is: 77 x 768\n",
            "0 On            1,212\n",
            "1 Taiwan        6,036\n",
            "2 ,               117\n",
            "3 President     1,697\n",
            "4 B               139\n",
            "5 ##iden       26,859\n",
            "6 under         1,223\n",
            "7 ##sco        11,428\n",
            "8 ##red         4,359\n",
            "9 that          1,115\n",
            "10 the           1,103\n",
            "11 United        1,244\n",
            "12 States        1,311\n",
            "13 remains       2,606\n",
            "14 committed     4,762\n",
            "15 to            1,106\n",
            "16 the           1,103\n",
            "17 “               789\n",
            "18 one           1,141\n",
            "19 China         1,975\n",
            "20 ”               790\n",
            "21 policy        2,818\n",
            "22 ,               117\n",
            "23 guided        8,610\n",
            "24 by            1,118\n",
            "25 the           1,103\n",
            "26 Taiwan        6,036\n",
            "27 Relations     9,269\n",
            "28 Act           2,173\n",
            "29 ,               117\n",
            "30 the           1,103\n",
            "31 three         1,210\n",
            "32 Joint         7,885\n",
            "33 Co            3,291\n",
            "34 ##mm          6,262\n",
            "35 ##uni        19,782\n",
            "36 ##ques       11,962\n",
            "37 ,               117\n",
            "38 and           1,105\n",
            "39 the           1,103\n",
            "40 Six           4,995\n",
            "41 As            1,249\n",
            "42 ##su          6,385\n",
            "43 ##rance      10,555\n",
            "44 ##s           1,116\n",
            "45 ,               117\n",
            "46 and           1,105\n",
            "47 that          1,115\n",
            "48 the           1,103\n",
            "49 United        1,244\n",
            "50 States        1,311\n",
            "51 strongly      5,473\n",
            "52 oppose       16,315\n",
            "53 ##s           1,116\n",
            "54 un            8,362\n",
            "55 ##ila         8,009\n",
            "56 ##teral      16,719\n",
            "57 efforts       3,268\n",
            "58 to            1,106\n",
            "59 change        1,849\n",
            "60 the           1,103\n",
            "61 status        2,781\n",
            "62 q               186\n",
            "63 ##uo         11,848\n",
            "64 or            1,137\n",
            "65 under         1,223\n",
            "66 ##mine        9,685\n",
            "67 peace         3,519\n",
            "68 and           1,105\n",
            "69 stability     9,397\n",
            "70 across        1,506\n",
            "71 the           1,103\n",
            "72 Taiwan        6,036\n",
            "73 Strait       12,925\n",
            "74 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 75\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 75 x 3072\n",
            "Summed embeddings shape is: 75 x 768\n",
            "0 [CLS]           101\n",
            "1 President     1,697\n",
            "2 B               139\n",
            "3 ##iden       26,859\n",
            "4 also          1,145\n",
            "5 under         1,223\n",
            "6 ##sco        11,428\n",
            "7 ##red         4,359\n",
            "8 the           1,103\n",
            "9 importance    4,495\n",
            "10 of            1,104\n",
            "11 managing      7,204\n",
            "12 strategic     7,061\n",
            "13 risks        11,040\n",
            "14 .               119\n",
            "15 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 16\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 16 x 3072\n",
            "Summed embeddings shape is: 16 x 768\n",
            "0 President     1,697\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 also          1,145\n",
            "4 under         1,223\n",
            "5 ##sco        11,428\n",
            "6 ##red         4,359\n",
            "7 the           1,103\n",
            "8 importance    4,495\n",
            "9 of            1,104\n",
            "10 managing      7,204\n",
            "11 strategic     7,061\n",
            "12 risks        11,040\n",
            "13 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 14\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 14 x 3072\n",
            "Summed embeddings shape is: 14 x 768\n",
            "0 [CLS]           101\n",
            "1 He            1,124\n",
            "2 noted         2,382\n",
            "3 the           1,103\n",
            "4 need          1,444\n",
            "5 for           1,111\n",
            "6 common        1,887\n",
            "7 -               118\n",
            "8 sense         2,305\n",
            "9 guard         3,542\n",
            "10 ##rail       12,797\n",
            "11 ##s           1,116\n",
            "12 to            1,106\n",
            "13 ensure        4,989\n",
            "14 that          1,115\n",
            "15 competition   2,208\n",
            "16 does          1,674\n",
            "17 not           1,136\n",
            "18 ve            1,396\n",
            "19 ##er          1,200\n",
            "20 into          1,154\n",
            "21 conflict      4,139\n",
            "22 and           1,105\n",
            "23 to            1,106\n",
            "24 keep          1,712\n",
            "25 lines         2,442\n",
            "26 of            1,104\n",
            "27 communication  4,909\n",
            "28 open          1,501\n",
            "29 .               119\n",
            "30 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 31\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 31 x 3072\n",
            "Summed embeddings shape is: 31 x 768\n",
            "0 He            1,124\n",
            "1 noted         2,382\n",
            "2 the           1,103\n",
            "3 need          1,444\n",
            "4 for           1,111\n",
            "5 common        1,887\n",
            "6 -               118\n",
            "7 sense         2,305\n",
            "8 guard         3,542\n",
            "9 ##rail       12,797\n",
            "10 ##s           1,116\n",
            "11 to            1,106\n",
            "12 ensure        4,989\n",
            "13 that          1,115\n",
            "14 competition   2,208\n",
            "15 does          1,674\n",
            "16 not           1,136\n",
            "17 ve            1,396\n",
            "18 ##er          1,200\n",
            "19 into          1,154\n",
            "20 conflict      4,139\n",
            "21 and           1,105\n",
            "22 to            1,106\n",
            "23 keep          1,712\n",
            "24 lines         2,442\n",
            "25 of            1,104\n",
            "26 communication  4,909\n",
            "27 open          1,501\n",
            "28 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 29\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 29 x 3072\n",
            "Summed embeddings shape is: 29 x 768\n",
            "0 [CLS]           101\n",
            "1 He            1,124\n",
            "2 raised        2,120\n",
            "3 specific      2,747\n",
            "4 trans        14,715\n",
            "5 ##national   23,322\n",
            "6 challenges    7,806\n",
            "7 where         1,187\n",
            "8 our           1,412\n",
            "9 interests     4,740\n",
            "10 inter         9,455\n",
            "11 ##sect       26,338\n",
            "12 ,               117\n",
            "13 such          1,216\n",
            "14 as            1,112\n",
            "15 health        2,332\n",
            "16 security      2,699\n",
            "17 .               119\n",
            "18 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 19\n",
            "Number of hidden units: 768\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated embeddings shape is: 19 x 3072\n",
            "Summed embeddings shape is: 19 x 768\n",
            "0 He            1,124\n",
            "1 raised        2,120\n",
            "2 specific      2,747\n",
            "3 trans        14,715\n",
            "4 ##national   23,322\n",
            "5 challenges    7,806\n",
            "6 where         1,187\n",
            "7 our           1,412\n",
            "8 interests     4,740\n",
            "9 inter         9,455\n",
            "10 ##sect       26,338\n",
            "11 ,               117\n",
            "12 such          1,216\n",
            "13 as            1,112\n",
            "14 health        2,332\n",
            "15 security      2,699\n",
            "16 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 17\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 17 x 3072\n",
            "Summed embeddings shape is: 17 x 768\n",
            "0 [CLS]           101\n",
            "1 In            1,130\n",
            "2 particular    2,440\n",
            "3 ,               117\n",
            "4 the           1,103\n",
            "5 two           1,160\n",
            "6 leaders       3,478\n",
            "7 discussed     6,352\n",
            "8 the           1,103\n",
            "9 exist         4,056\n",
            "10 ##ential     15,544\n",
            "11 nature        2,731\n",
            "12 of            1,104\n",
            "13 the           1,103\n",
            "14 climate       4,530\n",
            "15 crisis        5,532\n",
            "16 to            1,106\n",
            "17 the           1,103\n",
            "18 world         1,362\n",
            "19 and           1,105\n",
            "20 the           1,103\n",
            "21 important     1,696\n",
            "22 role          1,648\n",
            "23 that          1,115\n",
            "24 the           1,103\n",
            "25 United        1,244\n",
            "26 States        1,311\n",
            "27 and           1,105\n",
            "28 the           1,103\n",
            "29 PR           11,629\n",
            "30 ##C           1,658\n",
            "31 play          1,505\n",
            "32 .               119\n",
            "33 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 34\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 34 x 3072\n",
            "Summed embeddings shape is: 34 x 768\n",
            "0 In            1,130\n",
            "1 particular    2,440\n",
            "2 ,               117\n",
            "3 the           1,103\n",
            "4 two           1,160\n",
            "5 leaders       3,478\n",
            "6 discussed     6,352\n",
            "7 the           1,103\n",
            "8 exist         4,056\n",
            "9 ##ential     15,544\n",
            "10 nature        2,731\n",
            "11 of            1,104\n",
            "12 the           1,103\n",
            "13 climate       4,530\n",
            "14 crisis        5,532\n",
            "15 to            1,106\n",
            "16 the           1,103\n",
            "17 world         1,362\n",
            "18 and           1,105\n",
            "19 the           1,103\n",
            "20 important     1,696\n",
            "21 role          1,648\n",
            "22 that          1,115\n",
            "23 the           1,103\n",
            "24 United        1,244\n",
            "25 States        1,311\n",
            "26 and           1,105\n",
            "27 the           1,103\n",
            "28 PR           11,629\n",
            "29 ##C           1,658\n",
            "30 play          1,505\n",
            "31 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 32\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 32 x 3072\n",
            "Summed embeddings shape is: 32 x 768\n",
            "0 [CLS]           101\n",
            "1 They          1,220\n",
            "2 also          1,145\n",
            "3 discussed     6,352\n",
            "4 the           1,103\n",
            "5 importance    4,495\n",
            "6 of            1,104\n",
            "7 taking        1,781\n",
            "8 measures      5,252\n",
            "9 to            1,106\n",
            "10 address       4,134\n",
            "11 global        4,265\n",
            "12 energy        2,308\n",
            "13 supplies      5,508\n",
            "14 .               119\n",
            "15 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 16\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 16 x 3072\n",
            "Summed embeddings shape is: 16 x 768\n",
            "0 They          1,220\n",
            "1 also          1,145\n",
            "2 discussed     6,352\n",
            "3 the           1,103\n",
            "4 importance    4,495\n",
            "5 of            1,104\n",
            "6 taking        1,781\n",
            "7 measures      5,252\n",
            "8 to            1,106\n",
            "9 address       4,134\n",
            "10 global        4,265\n",
            "11 energy        2,308\n",
            "12 supplies      5,508\n",
            "13 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 14\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 14 x 3072\n",
            "Summed embeddings shape is: 14 x 768\n",
            "0 [CLS]           101\n",
            "1 The           1,109\n",
            "2 two           1,160\n",
            "3 leaders       3,478\n",
            "4 also          1,145\n",
            "5 exchanged    10,476\n",
            "6 views         4,696\n",
            "7 on            1,113\n",
            "8 key           2,501\n",
            "9 regional      2,918\n",
            "10 challenges    7,806\n",
            "11 ,               117\n",
            "12 including     1,259\n",
            "13 D               141\n",
            "14 ##PR         22,861\n",
            "15 ##K           2,428\n",
            "16 ,               117\n",
            "17 Afghanistan   6,469\n",
            "18 ,               117\n",
            "19 and           1,105\n",
            "20 Iran          3,398\n",
            "21 .               119\n",
            "22 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 23\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 23 x 3072\n",
            "Summed embeddings shape is: 23 x 768\n",
            "0 The           1,109\n",
            "1 two           1,160\n",
            "2 leaders       3,478\n",
            "3 also          1,145\n",
            "4 exchanged    10,476\n",
            "5 views         4,696\n",
            "6 on            1,113\n",
            "7 key           2,501\n",
            "8 regional      2,918\n",
            "9 challenges    7,806\n",
            "10 ,               117\n",
            "11 including     1,259\n",
            "12 D               141\n",
            "13 ##PR         22,861\n",
            "14 ##K           2,428\n",
            "15 ,               117\n",
            "16 Afghanistan   6,469\n",
            "17 ,               117\n",
            "18 and           1,105\n",
            "19 Iran          3,398\n",
            "20 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 21\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 21 x 3072\n",
            "Summed embeddings shape is: 21 x 768\n",
            "0 [CLS]           101\n",
            "1 Finally       4,428\n",
            "2 ,               117\n",
            "3 they          1,152\n",
            "4 discussed     6,352\n",
            "5 ways          3,242\n",
            "6 for           1,111\n",
            "7 the           1,103\n",
            "8 two           1,160\n",
            "9 sides         3,091\n",
            "10 to            1,106\n",
            "11 continue      2,760\n",
            "12 discussions  10,508\n",
            "13 on            1,113\n",
            "14 a               170\n",
            "15 number        1,295\n",
            "16 of            1,104\n",
            "17 areas         1,877\n",
            "18 ,               117\n",
            "19 with          1,114\n",
            "20 President     1,697\n",
            "21 B               139\n",
            "22 ##iden       26,859\n",
            "23 under         1,223\n",
            "24 ##sco        11,428\n",
            "25 ##ring        3,384\n",
            "26 the           1,103\n",
            "27 importance    4,495\n",
            "28 of            1,104\n",
            "29 sub           4,841\n",
            "30 ##stant      16,566\n",
            "31 ##ive         2,109\n",
            "32 and           1,105\n",
            "33 concrete      5,019\n",
            "34 conversations 12,705\n",
            "35 .               119\n",
            "36 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 37\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 37 x 3072\n",
            "Summed embeddings shape is: 37 x 768\n",
            "0 Finally       4,428\n",
            "1 ,               117\n",
            "2 they          1,152\n",
            "3 discussed     6,352\n",
            "4 ways          3,242\n",
            "5 for           1,111\n",
            "6 the           1,103\n",
            "7 two           1,160\n",
            "8 sides         3,091\n",
            "9 to            1,106\n",
            "10 continue      2,760\n",
            "11 discussions  10,508\n",
            "12 on            1,113\n",
            "13 a               170\n",
            "14 number        1,295\n",
            "15 of            1,104\n",
            "16 areas         1,877\n",
            "17 ,               117\n",
            "18 with          1,114\n",
            "19 President     1,697\n",
            "20 B               139\n",
            "21 ##iden       26,859\n",
            "22 under         1,223\n",
            "23 ##sco        11,428\n",
            "24 ##ring        3,384\n",
            "25 the           1,103\n",
            "26 importance    4,495\n",
            "27 of            1,104\n",
            "28 sub           4,841\n",
            "29 ##stant      16,566\n",
            "30 ##ive         2,109\n",
            "31 and           1,105\n",
            "32 concrete      5,019\n",
            "33 conversations 12,705\n",
            "34 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 35\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 35 x 3072\n",
            "Summed embeddings shape is: 35 x 768\n",
            "0 [CLS]           101\n",
            "1 When          1,332\n",
            "2 Chinese       1,922\n",
            "3 President     1,697\n",
            "4 Xi           20,802\n",
            "5 Jin          10,922\n",
            "6 ##ping        2,624\n",
            "7 beamed       25,176\n",
            "8 into          1,154\n",
            "9 the           1,103\n",
            "10 White         2,061\n",
            "11 House         1,585\n",
            "12 on            1,113\n",
            "13 Monday        6,356\n",
            "14 evening       3,440\n",
            "15 for           1,111\n",
            "16 a               170\n",
            "17 virtual       8,496\n",
            "18 summit        7,966\n",
            "19 with          1,114\n",
            "20 President     1,697\n",
            "21 Joe           2,658\n",
            "22 B               139\n",
            "23 ##iden       26,859\n",
            "24 ,               117\n",
            "25 the           1,103\n",
            "26 two           1,160\n",
            "27 men           1,441\n",
            "28 needed        1,834\n",
            "29 no            1,185\n",
            "30 introduction  4,784\n",
            "31 .               119\n",
            "32 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 33\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 33 x 3072\n",
            "Summed embeddings shape is: 33 x 768\n",
            "0 When          1,332\n",
            "1 Chinese       1,922\n",
            "2 President     1,697\n",
            "3 Xi           20,802\n",
            "4 Jin          10,922\n",
            "5 ##ping        2,624\n",
            "6 beamed       25,176\n",
            "7 into          1,154\n",
            "8 the           1,103\n",
            "9 White         2,061\n",
            "10 House         1,585\n",
            "11 on            1,113\n",
            "12 Monday        6,356\n",
            "13 evening       3,440\n",
            "14 for           1,111\n",
            "15 a               170\n",
            "16 virtual       8,496\n",
            "17 summit        7,966\n",
            "18 with          1,114\n",
            "19 President     1,697\n",
            "20 Joe           2,658\n",
            "21 B               139\n",
            "22 ##iden       26,859\n",
            "23 ,               117\n",
            "24 the           1,103\n",
            "25 two           1,160\n",
            "26 men           1,441\n",
            "27 needed        1,834\n",
            "28 no            1,185\n",
            "29 introduction  4,784\n",
            "30 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 31\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 31 x 3072\n",
            "Summed embeddings shape is: 31 x 768\n",
            "0 [CLS]           101\n",
            "1 \"               107\n",
            "2 We            1,284\n",
            "3 '               112\n",
            "4 ve            1,396\n",
            "5 spent         2,097\n",
            "6 an            1,126\n",
            "7 awful         9,684\n",
            "8 lot           1,974\n",
            "9 of            1,104\n",
            "10 time          1,159\n",
            "11 talking       2,520\n",
            "12 to            1,106\n",
            "13 one           1,141\n",
            "14 another       1,330\n",
            "15 ,               117\n",
            "16 and           1,105\n",
            "17 I               146\n",
            "18 hope          2,810\n",
            "19 we            1,195\n",
            "20 can           1,169\n",
            "21 have          1,138\n",
            "22 a               170\n",
            "23 can           1,169\n",
            "24 ##di          3,309\n",
            "25 ##d           1,181\n",
            "26 conversation  3,771\n",
            "27 tonight       3,568\n",
            "28 as            1,112\n",
            "29 well          1,218\n",
            "30 ,               117\n",
            "31 \"               107\n",
            "32 B               139\n",
            "33 ##iden       26,859\n",
            "34 said          1,163\n",
            "35 as            1,112\n",
            "36 the           1,103\n",
            "37 talks         7,430\n",
            "38 got           1,400\n",
            "39 underway     14,910\n",
            "40 ,               117\n",
            "41 sitting       2,807\n",
            "42 at            1,120\n",
            "43 the           1,103\n",
            "44 head          1,246\n",
            "45 of            1,104\n",
            "46 the           1,103\n",
            "47 Roosevelt     8,189\n",
            "48 Room          7,043\n",
            "49 table         1,952\n",
            "50 as            1,112\n",
            "51 Xi           20,802\n",
            "52 '               112\n",
            "53 s               188\n",
            "54 visa         12,083\n",
            "55 ##ge          2,176\n",
            "56 was           1,108\n",
            "57 broadcast     3,012\n",
            "58 on            1,113\n",
            "59 a               170\n",
            "60 pair          3,111\n",
            "61 of            1,104\n",
            "62 television    1,778\n",
            "63 screens      12,472\n",
            "64 .               119\n",
            "65 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 66\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 66 x 3072\n",
            "Summed embeddings shape is: 66 x 768\n",
            "0 \"               107\n",
            "1 We            1,284\n",
            "2 '               112\n",
            "3 ve            1,396\n",
            "4 spent         2,097\n",
            "5 an            1,126\n",
            "6 awful         9,684\n",
            "7 lot           1,974\n",
            "8 of            1,104\n",
            "9 time          1,159\n",
            "10 talking       2,520\n",
            "11 to            1,106\n",
            "12 one           1,141\n",
            "13 another       1,330\n",
            "14 ,               117\n",
            "15 and           1,105\n",
            "16 I               146\n",
            "17 hope          2,810\n",
            "18 we            1,195\n",
            "19 can           1,169\n",
            "20 have          1,138\n",
            "21 a               170\n",
            "22 can           1,169\n",
            "23 ##di          3,309\n",
            "24 ##d           1,181\n",
            "25 conversation  3,771\n",
            "26 tonight       3,568\n",
            "27 as            1,112\n",
            "28 well          1,218\n",
            "29 ,               117\n",
            "30 \"               107\n",
            "31 B               139\n",
            "32 ##iden       26,859\n",
            "33 said          1,163\n",
            "34 as            1,112\n",
            "35 the           1,103\n",
            "36 talks         7,430\n",
            "37 got           1,400\n",
            "38 underway     14,910\n",
            "39 ,               117\n",
            "40 sitting       2,807\n",
            "41 at            1,120\n",
            "42 the           1,103\n",
            "43 head          1,246\n",
            "44 of            1,104\n",
            "45 the           1,103\n",
            "46 Roosevelt     8,189\n",
            "47 Room          7,043\n",
            "48 table         1,952\n",
            "49 as            1,112\n",
            "50 Xi           20,802\n",
            "51 '               112\n",
            "52 s               188\n",
            "53 visa         12,083\n",
            "54 ##ge          2,176\n",
            "55 was           1,108\n",
            "56 broadcast     3,012\n",
            "57 on            1,113\n",
            "58 a               170\n",
            "59 pair          3,111\n",
            "60 of            1,104\n",
            "61 television    1,778\n",
            "62 screens      12,472\n",
            "63 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 64 x 3072\n",
            "Summed embeddings shape is: 64 x 768\n",
            "0 [CLS]           101\n",
            "1 From          1,622\n",
            "2 his           1,117\n",
            "3 seat          1,946\n",
            "4 in            1,107\n",
            "5 a               170\n",
            "6 cavern       19,373\n",
            "7 ##ous         2,285\n",
            "8 room          1,395\n",
            "9 inside        1,656\n",
            "10 the           1,103\n",
            "11 Great         2,038\n",
            "12 Hall          1,944\n",
            "13 of            1,104\n",
            "14 the           1,103\n",
            "15 People        2,563\n",
            "16 in            1,107\n",
            "17 Beijing       6,671\n",
            "18 ,               117\n",
            "19 Xi           20,802\n",
            "20 was           1,108\n",
            "21 just          1,198\n",
            "22 as            1,112\n",
            "23 friendly      4,931\n",
            "24 .               119\n",
            "25 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 26\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 26 x 3072\n",
            "Summed embeddings shape is: 26 x 768\n",
            "0 From          1,622\n",
            "1 his           1,117\n",
            "2 seat          1,946\n",
            "3 in            1,107\n",
            "4 a               170\n",
            "5 cavern       19,373\n",
            "6 ##ous         2,285\n",
            "7 room          1,395\n",
            "8 inside        1,656\n",
            "9 the           1,103\n",
            "10 Great         2,038\n",
            "11 Hall          1,944\n",
            "12 of            1,104\n",
            "13 the           1,103\n",
            "14 People        2,563\n",
            "15 in            1,107\n",
            "16 Beijing       6,671\n",
            "17 ,               117\n",
            "18 Xi           20,802\n",
            "19 was           1,108\n",
            "20 just          1,198\n",
            "21 as            1,112\n",
            "22 friendly      4,931\n",
            "23 .               119\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 24 x 3072\n",
            "Summed embeddings shape is: 24 x 768\n",
            "0 [CLS]           101\n",
            "1 \"               107\n",
            "2 Although      1,966\n",
            "3 it            1,122\n",
            "4 '               112\n",
            "5 s               188\n",
            "6 not           1,136\n",
            "7 as            1,112\n",
            "8 good          1,363\n",
            "9 as            1,112\n",
            "10 a               170\n",
            "11 face          1,339\n",
            "12 -               118\n",
            "13 to            1,106\n",
            "14 -               118\n",
            "15 face          1,339\n",
            "16 meeting       2,309\n",
            "17 ,               117\n",
            "18 \"               107\n",
            "19 Xi           20,802\n",
            "20 said          1,163\n",
            "21 as            1,112\n",
            "22 the           1,103\n",
            "23 summit        7,966\n",
            "24 got           1,400\n",
            "25 underway     14,910\n",
            "26 ,               117\n",
            "27 \"               107\n",
            "28 I               146\n",
            "29 '               112\n",
            "30 m               182\n",
            "31 very          1,304\n",
            "32 happy         2,816\n",
            "33 to            1,106\n",
            "34 see           1,267\n",
            "35 my            1,139\n",
            "36 old           1,385\n",
            "37 friend        1,910\n",
            "38 .               119\n",
            "39 \"               107\n",
            "40 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 41\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 41 x 3072\n",
            "Summed embeddings shape is: 41 x 768\n",
            "0 \"               107\n",
            "1 Although      1,966\n",
            "2 it            1,122\n",
            "3 '               112\n",
            "4 s               188\n",
            "5 not           1,136\n",
            "6 as            1,112\n",
            "7 good          1,363\n",
            "8 as            1,112\n",
            "9 a               170\n",
            "10 face          1,339\n",
            "11 -               118\n",
            "12 to            1,106\n",
            "13 -               118\n",
            "14 face          1,339\n",
            "15 meeting       2,309\n",
            "16 ,               117\n",
            "17 \"               107\n",
            "18 Xi           20,802\n",
            "19 said          1,163\n",
            "20 as            1,112\n",
            "21 the           1,103\n",
            "22 summit        7,966\n",
            "23 got           1,400\n",
            "24 underway     14,910\n",
            "25 ,               117\n",
            "26 \"               107\n",
            "27 I               146\n",
            "28 '               112\n",
            "29 m               182\n",
            "30 very          1,304\n",
            "31 happy         2,816\n",
            "32 to            1,106\n",
            "33 see           1,267\n",
            "34 my            1,139\n",
            "35 old           1,385\n",
            "36 friend        1,910\n",
            "37 .               119\n",
            "38 \"               107\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 39\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 39 x 3072\n",
            "Summed embeddings shape is: 39 x 768\n",
            "0 [CLS]           101\n",
            "1 It            1,135\n",
            "2 was           1,108\n",
            "3 an            1,126\n",
            "4 au           12,686\n",
            "5 ##sp         20,080\n",
            "6 ##icious     25,430\n",
            "7 start         1,838\n",
            "8 to            1,106\n",
            "9 some          1,199\n",
            "10 of            1,104\n",
            "11 the           1,103\n",
            "12 most          1,211\n",
            "13 critical      3,607\n",
            "14 talks         7,430\n",
            "15 of            1,104\n",
            "16 B               139\n",
            "17 ##iden       26,859\n",
            "18 '               112\n",
            "19 s               188\n",
            "20 presidency   11,223\n",
            "21 ,               117\n",
            "22 given         1,549\n",
            "23 the           1,103\n",
            "24 de            1,260\n",
            "25 ##ter         2,083\n",
            "26 ##ior        18,472\n",
            "27 ##ating       3,798\n",
            "28 ties          7,057\n",
            "29 between       1,206\n",
            "30 Washington    1,994\n",
            "31 and           1,105\n",
            "32 Beijing       6,671\n",
            "33 and           1,105\n",
            "34 the           1,103\n",
            "35 reality       3,958\n",
            "36 ,               117\n",
            "37 acknowledged  8,646\n",
            "38 by            1,118\n",
            "39 administration  3,469\n",
            "40 officials     3,878\n",
            "41 ,               117\n",
            "42 that          1,115\n",
            "43 managing      7,204\n",
            "44 the           1,103\n",
            "45 US            1,646\n",
            "46 relationship  2,398\n",
            "47 with          1,114\n",
            "48 China         1,975\n",
            "49 will          1,209\n",
            "50 amount        2,971\n",
            "51 to            1,106\n",
            "52 B               139\n",
            "53 ##iden       26,859\n",
            "54 '               112\n",
            "55 s               188\n",
            "56 most          1,211\n",
            "57 critical      3,607\n",
            "58 international  1,835\n",
            "59 objective     7,649\n",
            "60 .               119\n",
            "61 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 62\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 62 x 3072\n",
            "Summed embeddings shape is: 62 x 768\n",
            "0 It            1,135\n",
            "1 was           1,108\n",
            "2 an            1,126\n",
            "3 au           12,686\n",
            "4 ##sp         20,080\n",
            "5 ##icious     25,430\n",
            "6 start         1,838\n",
            "7 to            1,106\n",
            "8 some          1,199\n",
            "9 of            1,104\n",
            "10 the           1,103\n",
            "11 most          1,211\n",
            "12 critical      3,607\n",
            "13 talks         7,430\n",
            "14 of            1,104\n",
            "15 B               139\n",
            "16 ##iden       26,859\n",
            "17 '               112\n",
            "18 s               188\n",
            "19 presidency   11,223\n",
            "20 ,               117\n",
            "21 given         1,549\n",
            "22 the           1,103\n",
            "23 de            1,260\n",
            "24 ##ter         2,083\n",
            "25 ##ior        18,472\n",
            "26 ##ating       3,798\n",
            "27 ties          7,057\n",
            "28 between       1,206\n",
            "29 Washington    1,994\n",
            "30 and           1,105\n",
            "31 Beijing       6,671\n",
            "32 and           1,105\n",
            "33 the           1,103\n",
            "34 reality       3,958\n",
            "35 ,               117\n",
            "36 acknowledged  8,646\n",
            "37 by            1,118\n",
            "38 administration  3,469\n",
            "39 officials     3,878\n",
            "40 ,               117\n",
            "41 that          1,115\n",
            "42 managing      7,204\n",
            "43 the           1,103\n",
            "44 US            1,646\n",
            "45 relationship  2,398\n",
            "46 with          1,114\n",
            "47 China         1,975\n",
            "48 will          1,209\n",
            "49 amount        2,971\n",
            "50 to            1,106\n",
            "51 B               139\n",
            "52 ##iden       26,859\n",
            "53 '               112\n",
            "54 s               188\n",
            "55 most          1,211\n",
            "56 critical      3,607\n",
            "57 international  1,835\n",
            "58 objective     7,649\n",
            "59 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 60\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 60 x 3072\n",
            "Summed embeddings shape is: 60 x 768\n",
            "0 [CLS]           101\n",
            "1 The           1,109\n",
            "2 a               170\n",
            "3 ##ff          3,101\n",
            "4 ##able        1,895\n",
            "5 greeting     15,908\n",
            "6 ##s           1,116\n",
            "7 eventually    2,028\n",
            "8 turned        1,454\n",
            "9 more          1,167\n",
            "10 serious       3,021\n",
            "11 as            1,112\n",
            "12 B               139\n",
            "13 ##iden       26,859\n",
            "14 raised        2,120\n",
            "15 concerns      5,365\n",
            "16 about         1,164\n",
            "17 human         1,769\n",
            "18 rights        2,266\n",
            "19 ,               117\n",
            "20 Chinese       1,922\n",
            "21 aggression   16,843\n",
            "22 toward        1,755\n",
            "23 Taiwan        6,036\n",
            "24 and           1,105\n",
            "25 trade         2,597\n",
            "26 issues        2,492\n",
            "27 .               119\n",
            "28 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 29\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 29 x 3072\n",
            "Summed embeddings shape is: 29 x 768\n",
            "0 The           1,109\n",
            "1 a               170\n",
            "2 ##ff          3,101\n",
            "3 ##able        1,895\n",
            "4 greeting     15,908\n",
            "5 ##s           1,116\n",
            "6 eventually    2,028\n",
            "7 turned        1,454\n",
            "8 more          1,167\n",
            "9 serious       3,021\n",
            "10 as            1,112\n",
            "11 B               139\n",
            "12 ##iden       26,859\n",
            "13 raised        2,120\n",
            "14 concerns      5,365\n",
            "15 about         1,164\n",
            "16 human         1,769\n",
            "17 rights        2,266\n",
            "18 ,               117\n",
            "19 Chinese       1,922\n",
            "20 aggression   16,843\n",
            "21 toward        1,755\n",
            "22 Taiwan        6,036\n",
            "23 and           1,105\n",
            "24 trade         2,597\n",
            "25 issues        2,492\n",
            "26 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 27\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 27 x 3072\n",
            "Summed embeddings shape is: 27 x 768\n",
            "0 [CLS]           101\n",
            "1 Throughout    7,092\n",
            "2 ,               117\n",
            "3 the           1,103\n",
            "4 leaders       3,478\n",
            "5 engaged       4,349\n",
            "6 in            1,107\n",
            "7 a               170\n",
            "8 \"               107\n",
            "9 healthy       8,071\n",
            "10 debate        5,655\n",
            "11 ,               117\n",
            "12 \"               107\n",
            "13 according     2,452\n",
            "14 to            1,106\n",
            "15 a               170\n",
            "16 senior        2,682\n",
            "17 administration  3,469\n",
            "18 official      2,078\n",
            "19 present       1,675\n",
            "20 for           1,111\n",
            "21 the           1,103\n",
            "22 discussions  10,508\n",
            "23 .               119\n",
            "24 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 25\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 25 x 3072\n",
            "Summed embeddings shape is: 25 x 768\n",
            "0 Throughout    7,092\n",
            "1 ,               117\n",
            "2 the           1,103\n",
            "3 leaders       3,478\n",
            "4 engaged       4,349\n",
            "5 in            1,107\n",
            "6 a               170\n",
            "7 \"               107\n",
            "8 healthy       8,071\n",
            "9 debate        5,655\n",
            "10 ,               117\n",
            "11 \"               107\n",
            "12 according     2,452\n",
            "13 to            1,106\n",
            "14 a               170\n",
            "15 senior        2,682\n",
            "16 administration  3,469\n",
            "17 official      2,078\n",
            "18 present       1,675\n",
            "19 for           1,111\n",
            "20 the           1,103\n",
            "21 discussions  10,508\n",
            "22 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 23\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 23 x 3072\n",
            "Summed embeddings shape is: 23 x 768\n",
            "0 [CLS]           101\n",
            "1 Official      9,018\n",
            "2 ##s           1,116\n",
            "3 said          1,163\n",
            "4 the           1,103\n",
            "5 three         1,210\n",
            "6 -               118\n",
            "7 and           1,105\n",
            "8 -               118\n",
            "9 a               170\n",
            "10 -               118\n",
            "11 half          1,544\n",
            "12 hour          2,396\n",
            "13 summit        7,966\n",
            "14 ,               117\n",
            "15 which         1,134\n",
            "16 stretched     6,572\n",
            "17 longer        2,039\n",
            "18 than          1,190\n",
            "19 planned       2,919\n",
            "20 ,               117\n",
            "21 allowed       2,148\n",
            "22 the           1,103\n",
            "23 two           1,160\n",
            "24 men           1,441\n",
            "25 ample        23,336\n",
            "26 opportunity   3,767\n",
            "27 to            1,106\n",
            "28 diver        23,448\n",
            "29 ##ge          2,176\n",
            "30 from          1,121\n",
            "31 their         1,147\n",
            "32 prepared      4,029\n",
            "33 talking       2,520\n",
            "34 points        1,827\n",
            "35 .               119\n",
            "36 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 37\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 37 x 3072\n",
            "Summed embeddings shape is: 37 x 768\n",
            "0 Official      9,018\n",
            "1 ##s           1,116\n",
            "2 said          1,163\n",
            "3 the           1,103\n",
            "4 three         1,210\n",
            "5 -               118\n",
            "6 and           1,105\n",
            "7 -               118\n",
            "8 a               170\n",
            "9 -               118\n",
            "10 half          1,544\n",
            "11 hour          2,396\n",
            "12 summit        7,966\n",
            "13 ,               117\n",
            "14 which         1,134\n",
            "15 stretched     6,572\n",
            "16 longer        2,039\n",
            "17 than          1,190\n",
            "18 planned       2,919\n",
            "19 ,               117\n",
            "20 allowed       2,148\n",
            "21 the           1,103\n",
            "22 two           1,160\n",
            "23 men           1,441\n",
            "24 ample        23,336\n",
            "25 opportunity   3,767\n",
            "26 to            1,106\n",
            "27 diver        23,448\n",
            "28 ##ge          2,176\n",
            "29 from          1,121\n",
            "30 their         1,147\n",
            "31 prepared      4,029\n",
            "32 talking       2,520\n",
            "33 points        1,827\n",
            "34 .               119\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 35\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 35 x 3072\n",
            "Summed embeddings shape is: 35 x 768\n",
            "0 [CLS]           101\n",
            "1 The           1,109\n",
            "2 tone          3,586\n",
            "3 remained      1,915\n",
            "4 \"               107\n",
            "5 respect       4,161\n",
            "6 ##ful         2,365\n",
            "7 and           1,105\n",
            "8 straightforward 21,546\n",
            "9 ,               117\n",
            "10 \"               107\n",
            "11 the           1,103\n",
            "12 officials     3,878\n",
            "13 said          1,163\n",
            "14 .               119\n",
            "15 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 16\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 16 x 3072\n",
            "Summed embeddings shape is: 16 x 768\n",
            "0 The           1,109\n",
            "1 tone          3,586\n",
            "2 remained      1,915\n",
            "3 \"               107\n",
            "4 respect       4,161\n",
            "5 ##ful         2,365\n",
            "6 and           1,105\n",
            "7 straightforward 21,546\n",
            "8 ,               117\n",
            "9 \"               107\n",
            "10 the           1,103\n",
            "11 officials     3,878\n",
            "12 said          1,163\n",
            "13 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 14\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 14 x 3072\n",
            "Summed embeddings shape is: 14 x 768\n",
            "0 [CLS]           101\n",
            "1 But           1,252\n",
            "2 the           1,103\n",
            "3 highly        3,023\n",
            "4 anticipated  11,526\n",
            "5 summit        7,966\n",
            "6 yielded      18,826\n",
            "7 no            1,185\n",
            "8 major         1,558\n",
            "9 breakthrough 15,036\n",
            "10 ##s           1,116\n",
            "11 -               118\n",
            "12 -               118\n",
            "13 none          3,839\n",
            "14 were          1,127\n",
            "15 expected      2,637\n",
            "16 ahead         3,075\n",
            "17 of            1,104\n",
            "18 time          1,159\n",
            "19 -               118\n",
            "20 -               118\n",
            "21 and           1,105\n",
            "22 officials     3,878\n",
            "23 dismissed     6,714\n",
            "24 the           1,103\n",
            "25 notion        9,162\n",
            "26 the           1,103\n",
            "27 summit        7,966\n",
            "28 was           1,108\n",
            "29 intended      3,005\n",
            "30 to            1,106\n",
            "31 ease          7,166\n",
            "32 what          1,184\n",
            "33 has           1,144\n",
            "34 become        1,561\n",
            "35 an            1,126\n",
            "36 increasingly  5,672\n",
            "37 tense         8,901\n",
            "38 relationship  2,398\n",
            "39 .               119\n",
            "40 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 41\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 41 x 3072\n",
            "Summed embeddings shape is: 41 x 768\n",
            "0 But           1,252\n",
            "1 the           1,103\n",
            "2 highly        3,023\n",
            "3 anticipated  11,526\n",
            "4 summit        7,966\n",
            "5 yielded      18,826\n",
            "6 no            1,185\n",
            "7 major         1,558\n",
            "8 breakthrough 15,036\n",
            "9 ##s           1,116\n",
            "10 -               118\n",
            "11 -               118\n",
            "12 none          3,839\n",
            "13 were          1,127\n",
            "14 expected      2,637\n",
            "15 ahead         3,075\n",
            "16 of            1,104\n",
            "17 time          1,159\n",
            "18 -               118\n",
            "19 -               118\n",
            "20 and           1,105\n",
            "21 officials     3,878\n",
            "22 dismissed     6,714\n",
            "23 the           1,103\n",
            "24 notion        9,162\n",
            "25 the           1,103\n",
            "26 summit        7,966\n",
            "27 was           1,108\n",
            "28 intended      3,005\n",
            "29 to            1,106\n",
            "30 ease          7,166\n",
            "31 what          1,184\n",
            "32 has           1,144\n",
            "33 become        1,561\n",
            "34 an            1,126\n",
            "35 increasingly  5,672\n",
            "36 tense         8,901\n",
            "37 relationship  2,398\n",
            "38 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 39\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 39 x 3072\n",
            "Summed embeddings shape is: 39 x 768\n",
            "0 [CLS]           101\n",
            "1 \"               107\n",
            "2 I               146\n",
            "3 don           1,274\n",
            "4 '               112\n",
            "5 t               189\n",
            "6 think         1,341\n",
            "7 the           1,103\n",
            "8 purpose       3,007\n",
            "9 was           1,108\n",
            "10 particularly  2,521\n",
            "11 to            1,106\n",
            "12 ease          7,166\n",
            "13 tensions     14,696\n",
            "14 ,               117\n",
            "15 or            1,137\n",
            "16 that          1,115\n",
            "17 that          1,115\n",
            "18 was           1,108\n",
            "19 the           1,103\n",
            "20 result        1,871\n",
            "21 .               119\n",
            "22 We            1,284\n",
            "23 want          1,328\n",
            "24 to            1,106\n",
            "25 make          1,294\n",
            "26 sure          1,612\n",
            "27 the           1,103\n",
            "28 competition   2,208\n",
            "29 is            1,110\n",
            "30 re            1,231\n",
            "31 ##sp         20,080\n",
            "32 ##ons         4,199\n",
            "33 ##ibly       15,298\n",
            "34 managed       2,374\n",
            "35 ,               117\n",
            "36 that          1,115\n",
            "37 we            1,195\n",
            "38 have          1,138\n",
            "39 ways          3,242\n",
            "40 to            1,106\n",
            "41 do            1,202\n",
            "42 that          1,115\n",
            "43 .               119\n",
            "44 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 45\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 45 x 3072\n",
            "Summed embeddings shape is: 45 x 768\n",
            "0 \"               107\n",
            "1 I               146\n",
            "2 don           1,274\n",
            "3 '               112\n",
            "4 t               189\n",
            "5 think         1,341\n",
            "6 the           1,103\n",
            "7 purpose       3,007\n",
            "8 was           1,108\n",
            "9 particularly  2,521\n",
            "10 to            1,106\n",
            "11 ease          7,166\n",
            "12 tensions     14,696\n",
            "13 ,               117\n",
            "14 or            1,137\n",
            "15 that          1,115\n",
            "16 that          1,115\n",
            "17 was           1,108\n",
            "18 the           1,103\n",
            "19 result        1,871\n",
            "20 .               119\n",
            "21 We            1,284\n",
            "22 want          1,328\n",
            "23 to            1,106\n",
            "24 make          1,294\n",
            "25 sure          1,612\n",
            "26 the           1,103\n",
            "27 competition   2,208\n",
            "28 is            1,110\n",
            "29 re            1,231\n",
            "30 ##sp         20,080\n",
            "31 ##ons         4,199\n",
            "32 ##ibly       15,298\n",
            "33 managed       2,374\n",
            "34 ,               117\n",
            "35 that          1,115\n",
            "36 we            1,195\n",
            "37 have          1,138\n",
            "38 ways          3,242\n",
            "39 to            1,106\n",
            "40 do            1,202\n",
            "41 that          1,115\n",
            "42 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 43\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 43 x 3072\n",
            "Summed embeddings shape is: 43 x 768\n",
            "0 [CLS]           101\n",
            "1 The           1,109\n",
            "2 President     1,697\n",
            "3 '               112\n",
            "4 s               188\n",
            "5 been          1,151\n",
            "6 quite         2,385\n",
            "7 clear         2,330\n",
            "8 he            1,119\n",
            "9 '               112\n",
            "10 s               188\n",
            "11 going         1,280\n",
            "12 to            1,106\n",
            "13 engage        8,306\n",
            "14 in            1,107\n",
            "15 that          1,115\n",
            "16 stiff        11,111\n",
            "17 competition   2,208\n",
            "18 ,               117\n",
            "19 \"               107\n",
            "20 the           1,103\n",
            "21 senior        2,682\n",
            "22 administration  3,469\n",
            "23 official      2,078\n",
            "24 said          1,163\n",
            "25 afterward    11,343\n",
            "26 .               119\n",
            "27 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 28\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 28 x 3072\n",
            "Summed embeddings shape is: 28 x 768\n",
            "0 The           1,109\n",
            "1 President     1,697\n",
            "2 '               112\n",
            "3 s               188\n",
            "4 been          1,151\n",
            "5 quite         2,385\n",
            "6 clear         2,330\n",
            "7 he            1,119\n",
            "8 '               112\n",
            "9 s               188\n",
            "10 going         1,280\n",
            "11 to            1,106\n",
            "12 engage        8,306\n",
            "13 in            1,107\n",
            "14 that          1,115\n",
            "15 stiff        11,111\n",
            "16 competition   2,208\n",
            "17 ,               117\n",
            "18 \"               107\n",
            "19 the           1,103\n",
            "20 senior        2,682\n",
            "21 administration  3,469\n",
            "22 official      2,078\n",
            "23 said          1,163\n",
            "24 afterward    11,343\n",
            "25 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 26\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 26 x 3072\n",
            "Summed embeddings shape is: 26 x 768\n",
            "0 [CLS]           101\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 is            1,110\n",
            "4 fond         15,263\n",
            "5 of            1,104\n",
            "6 citing        9,036\n",
            "7 the           1,103\n",
            "8 dozens       10,366\n",
            "9 of            1,104\n",
            "10 hours         2,005\n",
            "11 and           1,105\n",
            "12 thousands     4,674\n",
            "13 of            1,104\n",
            "14 miles         1,829\n",
            "15 he            1,119\n",
            "16 clock         4,705\n",
            "17 ##ed          1,174\n",
            "18 with          1,114\n",
            "19 Xi           20,802\n",
            "20 when          1,165\n",
            "21 both          1,241\n",
            "22 were          1,127\n",
            "23 serving       2,688\n",
            "24 as            1,112\n",
            "25 their         1,147\n",
            "26 country       1,583\n",
            "27 '               112\n",
            "28 s               188\n",
            "29 vice          4,711\n",
            "30 presidents   17,702\n",
            "31 .               119\n",
            "32 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 33\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 33 x 3072\n",
            "Summed embeddings shape is: 33 x 768\n",
            "0 B               139\n",
            "1 ##iden       26,859\n",
            "2 is            1,110\n",
            "3 fond         15,263\n",
            "4 of            1,104\n",
            "5 citing        9,036\n",
            "6 the           1,103\n",
            "7 dozens       10,366\n",
            "8 of            1,104\n",
            "9 hours         2,005\n",
            "10 and           1,105\n",
            "11 thousands     4,674\n",
            "12 of            1,104\n",
            "13 miles         1,829\n",
            "14 he            1,119\n",
            "15 clock         4,705\n",
            "16 ##ed          1,174\n",
            "17 with          1,114\n",
            "18 Xi           20,802\n",
            "19 when          1,165\n",
            "20 both          1,241\n",
            "21 were          1,127\n",
            "22 serving       2,688\n",
            "23 as            1,112\n",
            "24 their         1,147\n",
            "25 country       1,583\n",
            "26 '               112\n",
            "27 s               188\n",
            "28 vice          4,711\n",
            "29 presidents   17,702\n",
            "30 .               119\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 31\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 31 x 3072\n",
            "Summed embeddings shape is: 31 x 768\n",
            "0 [CLS]           101\n",
            "1 He            1,124\n",
            "2 '               112\n",
            "3 s               188\n",
            "4 claimed       2,694\n",
            "5 to            1,106\n",
            "6 have          1,138\n",
            "7 spent         2,097\n",
            "8 more          1,167\n",
            "9 time          1,159\n",
            "10 with          1,114\n",
            "11 the           1,103\n",
            "12 Chinese       1,922\n",
            "13 president     2,084\n",
            "14 than          1,190\n",
            "15 any           1,251\n",
            "16 other         1,168\n",
            "17 world         1,362\n",
            "18 leader        2,301\n",
            "19 .               119\n",
            "20 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 21\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 21 x 3072\n",
            "Summed embeddings shape is: 21 x 768\n",
            "0 He            1,124\n",
            "1 '               112\n",
            "2 s               188\n",
            "3 claimed       2,694\n",
            "4 to            1,106\n",
            "5 have          1,138\n",
            "6 spent         2,097\n",
            "7 more          1,167\n",
            "8 time          1,159\n",
            "9 with          1,114\n",
            "10 the           1,103\n",
            "11 Chinese       1,922\n",
            "12 president     2,084\n",
            "13 than          1,190\n",
            "14 any           1,251\n",
            "15 other         1,168\n",
            "16 world         1,362\n",
            "17 leader        2,301\n",
            "18 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 19\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 19 x 3072\n",
            "Summed embeddings shape is: 19 x 768\n",
            "0 [CLS]           101\n",
            "1 But           1,252\n",
            "2 things        1,614\n",
            "3 have          1,138\n",
            "4 changed       2,014\n",
            "5 since         1,290\n",
            "6 B               139\n",
            "7 ##iden       26,859\n",
            "8 ,               117\n",
            "9 as            1,112\n",
            "10 he            1,119\n",
            "11 likes         7,407\n",
            "12 to            1,106\n",
            "13 recall        9,148\n",
            "14 ,               117\n",
            "15 was           1,108\n",
            "16 dining        7,659\n",
            "17 with          1,114\n",
            "18 Xi           20,802\n",
            "19 on            1,113\n",
            "20 the           1,103\n",
            "21 Tibetan      12,046\n",
            "22 Plateau      17,069\n",
            "23 and           1,105\n",
            "24 describing    7,645\n",
            "25 the           1,103\n",
            "26 United        1,244\n",
            "27 States        1,311\n",
            "28 in            1,107\n",
            "29 one           1,141\n",
            "30 word          1,937\n",
            "31 :               131\n",
            "32 \"               107\n",
            "33 possibilities 12,435\n",
            "34 .               119\n",
            "35 \"               107\n",
            "36 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 37\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 37 x 3072\n",
            "Summed embeddings shape is: 37 x 768\n",
            "0 But           1,252\n",
            "1 things        1,614\n",
            "2 have          1,138\n",
            "3 changed       2,014\n",
            "4 since         1,290\n",
            "5 B               139\n",
            "6 ##iden       26,859\n",
            "7 ,               117\n",
            "8 as            1,112\n",
            "9 he            1,119\n",
            "10 likes         7,407\n",
            "11 to            1,106\n",
            "12 recall        9,148\n",
            "13 ,               117\n",
            "14 was           1,108\n",
            "15 dining        7,659\n",
            "16 with          1,114\n",
            "17 Xi           20,802\n",
            "18 on            1,113\n",
            "19 the           1,103\n",
            "20 Tibetan      12,046\n",
            "21 Plateau      17,069\n",
            "22 and           1,105\n",
            "23 describing    7,645\n",
            "24 the           1,103\n",
            "25 United        1,244\n",
            "26 States        1,311\n",
            "27 in            1,107\n",
            "28 one           1,141\n",
            "29 word          1,937\n",
            "30 :               131\n",
            "31 \"               107\n",
            "32 possibilities 12,435\n",
            "33 .               119\n",
            "34 \"               107\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 35\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 35 x 3072\n",
            "Summed embeddings shape is: 35 x 768\n",
            "0 [CLS]           101\n",
            "1 Now           1,986\n",
            "2 ,               117\n",
            "3 the           1,103\n",
            "4 world         1,362\n",
            "5 '               112\n",
            "6 s               188\n",
            "7 two           1,160\n",
            "8 largest       2,026\n",
            "9 economies    21,553\n",
            "10 are           1,132\n",
            "11 engaged       4,349\n",
            "12 in            1,107\n",
            "13 fierce        9,250\n",
            "14 tensions     14,696\n",
            "15 on            1,113\n",
            "16 trade         2,597\n",
            "17 ,               117\n",
            "18 military      1,764\n",
            "19 aggression   16,843\n",
            "20 and           1,105\n",
            "21 human         1,769\n",
            "22 rights        2,266\n",
            "23 .               119\n",
            "24 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 25\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 25 x 3072\n",
            "Summed embeddings shape is: 25 x 768\n",
            "0 Now           1,986\n",
            "1 ,               117\n",
            "2 the           1,103\n",
            "3 world         1,362\n",
            "4 '               112\n",
            "5 s               188\n",
            "6 two           1,160\n",
            "7 largest       2,026\n",
            "8 economies    21,553\n",
            "9 are           1,132\n",
            "10 engaged       4,349\n",
            "11 in            1,107\n",
            "12 fierce        9,250\n",
            "13 tensions     14,696\n",
            "14 on            1,113\n",
            "15 trade         2,597\n",
            "16 ,               117\n",
            "17 military      1,764\n",
            "18 aggression   16,843\n",
            "19 and           1,105\n",
            "20 human         1,769\n",
            "21 rights        2,266\n",
            "22 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 23\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 23 x 3072\n",
            "Summed embeddings shape is: 23 x 768\n",
            "0 [CLS]           101\n",
            "1 And           1,262\n",
            "2 B               139\n",
            "3 ##iden       26,859\n",
            "4 ,               117\n",
            "5 who           1,150\n",
            "6 initiated     7,087\n",
            "7 Monday        6,356\n",
            "8 evening       3,440\n",
            "9 '               112\n",
            "10 s               188\n",
            "11 virtual       8,496\n",
            "12 summit        7,966\n",
            "13 ,               117\n",
            "14 finds         4,090\n",
            "15 himself       1,471\n",
            "16 in            1,107\n",
            "17 a               170\n",
            "18 high          1,344\n",
            "19 -               118\n",
            "20 wire          7,700\n",
            "21 act           2,496\n",
            "22 with          1,114\n",
            "23 China         1,975\n",
            "24 '               112\n",
            "25 s               188\n",
            "26 most          1,211\n",
            "27 powerful      3,110\n",
            "28 leader        2,301\n",
            "29 in            1,107\n",
            "30 decades       4,397\n",
            "31 .               119\n",
            "32 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 33\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 33 x 3072\n",
            "Summed embeddings shape is: 33 x 768\n",
            "0 And           1,262\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 ,               117\n",
            "4 who           1,150\n",
            "5 initiated     7,087\n",
            "6 Monday        6,356\n",
            "7 evening       3,440\n",
            "8 '               112\n",
            "9 s               188\n",
            "10 virtual       8,496\n",
            "11 summit        7,966\n",
            "12 ,               117\n",
            "13 finds         4,090\n",
            "14 himself       1,471\n",
            "15 in            1,107\n",
            "16 a               170\n",
            "17 high          1,344\n",
            "18 -               118\n",
            "19 wire          7,700\n",
            "20 act           2,496\n",
            "21 with          1,114\n",
            "22 China         1,975\n",
            "23 '               112\n",
            "24 s               188\n",
            "25 most          1,211\n",
            "26 powerful      3,110\n",
            "27 leader        2,301\n",
            "28 in            1,107\n",
            "29 decades       4,397\n",
            "30 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 31\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 31 x 3072\n",
            "Summed embeddings shape is: 31 x 768\n",
            "0 [CLS]           101\n",
            "1 During        1,507\n",
            "2 the           1,103\n",
            "3 summit        7,966\n",
            "4 ,               117\n",
            "5 each          1,296\n",
            "6 man           1,299\n",
            "7 recounted    25,546\n",
            "8 stories       2,801\n",
            "9 from          1,121\n",
            "10 their         1,147\n",
            "11 time          1,159\n",
            "12 traveling     6,934\n",
            "13 with          1,114\n",
            "14 each          1,296\n",
            "15 other         1,168\n",
            "16 ,               117\n",
            "17 sometimes     2,121\n",
            "18 q               186\n",
            "19 ##uo         11,848\n",
            "20 ##ting        1,916\n",
            "21 each          1,296\n",
            "22 other         1,168\n",
            "23 '               112\n",
            "24 s               188\n",
            "25 words         1,734\n",
            "26 from          1,121\n",
            "27 that          1,115\n",
            "28 era           3,386\n",
            "29 ,               117\n",
            "30 the           1,103\n",
            "31 senior        2,682\n",
            "32 administration  3,469\n",
            "33 official      2,078\n",
            "34 said          1,163\n",
            "35 .               119\n",
            "36 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 37\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 37 x 3072\n",
            "Summed embeddings shape is: 37 x 768\n",
            "0 During        1,507\n",
            "1 the           1,103\n",
            "2 summit        7,966\n",
            "3 ,               117\n",
            "4 each          1,296\n",
            "5 man           1,299\n",
            "6 recounted    25,546\n",
            "7 stories       2,801\n",
            "8 from          1,121\n",
            "9 their         1,147\n",
            "10 time          1,159\n",
            "11 traveling     6,934\n",
            "12 with          1,114\n",
            "13 each          1,296\n",
            "14 other         1,168\n",
            "15 ,               117\n",
            "16 sometimes     2,121\n",
            "17 q               186\n",
            "18 ##uo         11,848\n",
            "19 ##ting        1,916\n",
            "20 each          1,296\n",
            "21 other         1,168\n",
            "22 '               112\n",
            "23 s               188\n",
            "24 words         1,734\n",
            "25 from          1,121\n",
            "26 that          1,115\n",
            "27 era           3,386\n",
            "28 ,               117\n",
            "29 the           1,103\n",
            "30 senior        2,682\n",
            "31 administration  3,469\n",
            "32 official      2,078\n",
            "33 said          1,163\n",
            "34 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 35\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 35 x 3072\n",
            "Summed embeddings shape is: 35 x 768\n",
            "0 [CLS]           101\n",
            "1 As            1,249\n",
            "2 the           1,103\n",
            "3 talks         7,430\n",
            "4 were          1,127\n",
            "5 getting       2,033\n",
            "6 underway     14,910\n",
            "7 ,               117\n",
            "8 B               139\n",
            "9 ##iden       26,859\n",
            "10 said          1,163\n",
            "11 he            1,119\n",
            "12 was           1,108\n",
            "13 expecting     7,805\n",
            "14 to            1,106\n",
            "15 discuss       6,265\n",
            "16 a               170\n",
            "17 wide          2,043\n",
            "18 -               118\n",
            "19 ranging       7,032\n",
            "20 and           1,105\n",
            "21 sub           4,841\n",
            "22 ##stant      16,566\n",
            "23 ##ive         2,109\n",
            "24 agenda       12,932\n",
            "25 .               119\n",
            "26 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 27\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 27 x 3072\n",
            "Summed embeddings shape is: 27 x 768\n",
            "0 As            1,249\n",
            "1 the           1,103\n",
            "2 talks         7,430\n",
            "3 were          1,127\n",
            "4 getting       2,033\n",
            "5 underway     14,910\n",
            "6 ,               117\n",
            "7 B               139\n",
            "8 ##iden       26,859\n",
            "9 said          1,163\n",
            "10 he            1,119\n",
            "11 was           1,108\n",
            "12 expecting     7,805\n",
            "13 to            1,106\n",
            "14 discuss       6,265\n",
            "15 a               170\n",
            "16 wide          2,043\n",
            "17 -               118\n",
            "18 ranging       7,032\n",
            "19 and           1,105\n",
            "20 sub           4,841\n",
            "21 ##stant      16,566\n",
            "22 ##ive         2,109\n",
            "23 agenda       12,932\n",
            "24 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 25\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 25 x 3072\n",
            "Summed embeddings shape is: 25 x 768\n",
            "0 [CLS]           101\n",
            "1 \"               107\n",
            "2 As            1,249\n",
            "3 I               146\n",
            "4 '               112\n",
            "5 ve            1,396\n",
            "6 said          1,163\n",
            "7 before        1,196\n",
            "8 ,               117\n",
            "9 it            1,122\n",
            "10 seems         3,093\n",
            "11 to            1,106\n",
            "12 me            1,143\n",
            "13 our           1,412\n",
            "14 responsibility  4,812\n",
            "15 as            1,112\n",
            "16 leaders       3,478\n",
            "17 of            1,104\n",
            "18 China         1,975\n",
            "19 and           1,105\n",
            "20 the           1,103\n",
            "21 United        1,244\n",
            "22 States        1,311\n",
            "23 is            1,110\n",
            "24 to            1,106\n",
            "25 ensure        4,989\n",
            "26 the           1,103\n",
            "27 competition   2,208\n",
            "28 between       1,206\n",
            "29 our           1,412\n",
            "30 two           1,160\n",
            "31 countries     2,182\n",
            "32 does          1,674\n",
            "33 not           1,136\n",
            "34 ve            1,396\n",
            "35 ##er          1,200\n",
            "36 into          1,154\n",
            "37 conflict      4,139\n",
            "38 ,               117\n",
            "39 either        1,719\n",
            "40 intended      3,005\n",
            "41 or            1,137\n",
            "42 un            8,362\n",
            "43 ##int        10,879\n",
            "44 ##ended      15,399\n",
            "45 .               119\n",
            "46 \"               107\n",
            "47 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 48\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 48 x 3072\n",
            "Summed embeddings shape is: 48 x 768\n",
            "0 \"               107\n",
            "1 As            1,249\n",
            "2 I               146\n",
            "3 '               112\n",
            "4 ve            1,396\n",
            "5 said          1,163\n",
            "6 before        1,196\n",
            "7 ,               117\n",
            "8 it            1,122\n",
            "9 seems         3,093\n",
            "10 to            1,106\n",
            "11 me            1,143\n",
            "12 our           1,412\n",
            "13 responsibility  4,812\n",
            "14 as            1,112\n",
            "15 leaders       3,478\n",
            "16 of            1,104\n",
            "17 China         1,975\n",
            "18 and           1,105\n",
            "19 the           1,103\n",
            "20 United        1,244\n",
            "21 States        1,311\n",
            "22 is            1,110\n",
            "23 to            1,106\n",
            "24 ensure        4,989\n",
            "25 the           1,103\n",
            "26 competition   2,208\n",
            "27 between       1,206\n",
            "28 our           1,412\n",
            "29 two           1,160\n",
            "30 countries     2,182\n",
            "31 does          1,674\n",
            "32 not           1,136\n",
            "33 ve            1,396\n",
            "34 ##er          1,200\n",
            "35 into          1,154\n",
            "36 conflict      4,139\n",
            "37 ,               117\n",
            "38 either        1,719\n",
            "39 intended      3,005\n",
            "40 or            1,137\n",
            "41 un            8,362\n",
            "42 ##int        10,879\n",
            "43 ##ended      15,399\n",
            "44 .               119\n",
            "45 \"               107\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 46\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 46 x 3072\n",
            "Summed embeddings shape is: 46 x 768\n",
            "0 [CLS]           101\n",
            "1 \"               107\n",
            "2 Just          2,066\n",
            "3 simply        2,566\n",
            "4 straightforward 21,546\n",
            "5 competition   2,208\n",
            "6 ,               117\n",
            "7 \"               107\n",
            "8 he            1,119\n",
            "9 said          1,163\n",
            "10 ,               117\n",
            "11 speaking      3,522\n",
            "12 to            1,106\n",
            "13 Xi           20,802\n",
            "14 through       1,194\n",
            "15 a               170\n",
            "16 translator   12,111\n",
            "17 .               119\n",
            "18 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 19\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 19 x 3072\n",
            "Summed embeddings shape is: 19 x 768\n",
            "0 \"               107\n",
            "1 Just          2,066\n",
            "2 simply        2,566\n",
            "3 straightforward 21,546\n",
            "4 competition   2,208\n",
            "5 ,               117\n",
            "6 \"               107\n",
            "7 he            1,119\n",
            "8 said          1,163\n",
            "9 ,               117\n",
            "10 speaking      3,522\n",
            "11 to            1,106\n",
            "12 Xi           20,802\n",
            "13 through       1,194\n",
            "14 a               170\n",
            "15 translator   12,111\n",
            "16 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 17\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 17 x 3072\n",
            "Summed embeddings shape is: 17 x 768\n",
            "0 [CLS]           101\n",
            "1 \"               107\n",
            "2 It            1,135\n",
            "3 seems         3,093\n",
            "4 to            1,106\n",
            "5 me            1,143\n",
            "6 we            1,195\n",
            "7 have          1,138\n",
            "8 to            1,106\n",
            "9 establish     4,586\n",
            "10 some          1,199\n",
            "11 common        1,887\n",
            "12 sense         2,305\n",
            "13 guard         3,542\n",
            "14 ##rail       12,797\n",
            "15 ##s           1,116\n",
            "16 ,               117\n",
            "17 to            1,106\n",
            "18 be            1,129\n",
            "19 clear         2,330\n",
            "20 and           1,105\n",
            "21 honest        7,345\n",
            "22 where         1,187\n",
            "23 we            1,195\n",
            "24 disagree     23,423\n",
            "25 and           1,105\n",
            "26 work          1,250\n",
            "27 together      1,487\n",
            "28 where         1,187\n",
            "29 our           1,412\n",
            "30 interests     4,740\n",
            "31 inter         9,455\n",
            "32 ##sect       26,338\n",
            "33 ,               117\n",
            "34 \"               107\n",
            "35 B               139\n",
            "36 ##iden       26,859\n",
            "37 went          1,355\n",
            "38 on            1,113\n",
            "39 ,               117\n",
            "40 asking        4,107\n",
            "41 to            1,106\n",
            "42 communicate  10,621\n",
            "43 \"               107\n",
            "44 honestly     12,051\n",
            "45 and           1,105\n",
            "46 directly      2,626\n",
            "47 \"               107\n",
            "48 over          1,166\n",
            "49 the           1,103\n",
            "50 range         2,079\n",
            "51 of            1,104\n",
            "52 topics        7,662\n",
            "53 up            1,146\n",
            "54 for           1,111\n",
            "55 discussion    6,145\n",
            "56 .               119\n",
            "57 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 58\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 58 x 3072\n",
            "Summed embeddings shape is: 58 x 768\n",
            "0 \"               107\n",
            "1 It            1,135\n",
            "2 seems         3,093\n",
            "3 to            1,106\n",
            "4 me            1,143\n",
            "5 we            1,195\n",
            "6 have          1,138\n",
            "7 to            1,106\n",
            "8 establish     4,586\n",
            "9 some          1,199\n",
            "10 common        1,887\n",
            "11 sense         2,305\n",
            "12 guard         3,542\n",
            "13 ##rail       12,797\n",
            "14 ##s           1,116\n",
            "15 ,               117\n",
            "16 to            1,106\n",
            "17 be            1,129\n",
            "18 clear         2,330\n",
            "19 and           1,105\n",
            "20 honest        7,345\n",
            "21 where         1,187\n",
            "22 we            1,195\n",
            "23 disagree     23,423\n",
            "24 and           1,105\n",
            "25 work          1,250\n",
            "26 together      1,487\n",
            "27 where         1,187\n",
            "28 our           1,412\n",
            "29 interests     4,740\n",
            "30 inter         9,455\n",
            "31 ##sect       26,338\n",
            "32 ,               117\n",
            "33 \"               107\n",
            "34 B               139\n",
            "35 ##iden       26,859\n",
            "36 went          1,355\n",
            "37 on            1,113\n",
            "38 ,               117\n",
            "39 asking        4,107\n",
            "40 to            1,106\n",
            "41 communicate  10,621\n",
            "42 \"               107\n",
            "43 honestly     12,051\n",
            "44 and           1,105\n",
            "45 directly      2,626\n",
            "46 \"               107\n",
            "47 over          1,166\n",
            "48 the           1,103\n",
            "49 range         2,079\n",
            "50 of            1,104\n",
            "51 topics        7,662\n",
            "52 up            1,146\n",
            "53 for           1,111\n",
            "54 discussion    6,145\n",
            "55 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 56\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 56 x 3072\n",
            "Summed embeddings shape is: 56 x 768\n",
            "0 [CLS]           101\n",
            "1 \"               107\n",
            "2 We            1,284\n",
            "3 never         1,309\n",
            "4 walk          2,647\n",
            "5 away          1,283\n",
            "6 wondering     6,123\n",
            "7 what          1,184\n",
            "8 the           1,103\n",
            "9 other         1,168\n",
            "10 man           1,299\n",
            "11 is            1,110\n",
            "12 thinking      2,422\n",
            "13 ,               117\n",
            "14 \"               107\n",
            "15 he            1,119\n",
            "16 said          1,163\n",
            "17 .               119\n",
            "18 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 19\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 19 x 3072\n",
            "Summed embeddings shape is: 19 x 768\n",
            "0 \"               107\n",
            "1 We            1,284\n",
            "2 never         1,309\n",
            "3 walk          2,647\n",
            "4 away          1,283\n",
            "5 wondering     6,123\n",
            "6 what          1,184\n",
            "7 the           1,103\n",
            "8 other         1,168\n",
            "9 man           1,299\n",
            "10 is            1,110\n",
            "11 thinking      2,422\n",
            "12 ,               117\n",
            "13 \"               107\n",
            "14 he            1,119\n",
            "15 said          1,163\n",
            "16 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 17\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 17 x 3072\n",
            "Summed embeddings shape is: 17 x 768\n",
            "0 [CLS]           101\n",
            "1 Afterward    26,869\n",
            "2 ,               117\n",
            "3 the           1,103\n",
            "4 White         2,061\n",
            "5 House         1,585\n",
            "6 said          1,163\n",
            "7 B               139\n",
            "8 ##iden       26,859\n",
            "9 raised        2,120\n",
            "10 concerns      5,365\n",
            "11 about         1,164\n",
            "12 human         1,769\n",
            "13 rights        2,266\n",
            "14 abuses       23,711\n",
            "15 against       1,222\n",
            "16 the           1,103\n",
            "17 U               158\n",
            "18 ##y           1,183\n",
            "19 ##gh          5,084\n",
            "20 ##ur          2,149\n",
            "21 minority      7,309\n",
            "22 in            1,107\n",
            "23 the           1,103\n",
            "24 western       2,466\n",
            "25 Xi           20,802\n",
            "26 ##nji        21,440\n",
            "27 ##ang         4,993\n",
            "28 Provence     21,818\n",
            "29 and           1,105\n",
            "30 in            1,107\n",
            "31 Tibet        13,659\n",
            "32 .               119\n",
            "33 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 34\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 34 x 3072\n",
            "Summed embeddings shape is: 34 x 768\n",
            "0 Afterward    26,869\n",
            "1 ,               117\n",
            "2 the           1,103\n",
            "3 White         2,061\n",
            "4 House         1,585\n",
            "5 said          1,163\n",
            "6 B               139\n",
            "7 ##iden       26,859\n",
            "8 raised        2,120\n",
            "9 concerns      5,365\n",
            "10 about         1,164\n",
            "11 human         1,769\n",
            "12 rights        2,266\n",
            "13 abuses       23,711\n",
            "14 against       1,222\n",
            "15 the           1,103\n",
            "16 U               158\n",
            "17 ##y           1,183\n",
            "18 ##gh          5,084\n",
            "19 ##ur          2,149\n",
            "20 minority      7,309\n",
            "21 in            1,107\n",
            "22 the           1,103\n",
            "23 western       2,466\n",
            "24 Xi           20,802\n",
            "25 ##nji        21,440\n",
            "26 ##ang         4,993\n",
            "27 Provence     21,818\n",
            "28 and           1,105\n",
            "29 in            1,107\n",
            "30 Tibet        13,659\n",
            "31 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 32\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 32 x 3072\n",
            "Summed embeddings shape is: 32 x 768\n",
            "0 [CLS]           101\n",
            "1 Taiwan        6,036\n",
            "2 ,               117\n",
            "3 which         1,134\n",
            "4 has           1,144\n",
            "5 been          1,151\n",
            "6 a               170\n",
            "7 source        2,674\n",
            "8 of            1,104\n",
            "9 increased     2,569\n",
            "10 tension       6,646\n",
            "11 in            1,107\n",
            "12 recent        2,793\n",
            "13 months        1,808\n",
            "14 ,               117\n",
            "15 was           1,108\n",
            "16 a               170\n",
            "17 topic         8,366\n",
            "18 of            1,104\n",
            "19 extensive     4,154\n",
            "20 discussion    6,145\n",
            "21 during        1,219\n",
            "22 the           1,103\n",
            "23 summit        7,966\n",
            "24 .               119\n",
            "25 [SEP]           102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 26\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 26 x 3072\n",
            "Summed embeddings shape is: 26 x 768\n",
            "0 Taiwan        6,036\n",
            "1 ,               117\n",
            "2 which         1,134\n",
            "3 has           1,144\n",
            "4 been          1,151\n",
            "5 a               170\n",
            "6 source        2,674\n",
            "7 of            1,104\n",
            "8 increased     2,569\n",
            "9 tension       6,646\n",
            "10 in            1,107\n",
            "11 recent        2,793\n",
            "12 months        1,808\n",
            "13 ,               117\n",
            "14 was           1,108\n",
            "15 a               170\n",
            "16 topic         8,366\n",
            "17 of            1,104\n",
            "18 extensive     4,154\n",
            "19 discussion    6,145\n",
            "20 during        1,219\n",
            "21 the           1,103\n",
            "22 summit        7,966\n",
            "23 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 24 x 3072\n",
            "Summed embeddings shape is: 24 x 768\n",
            "0 [CLS]           101\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 stressed     13,713\n",
            "4 the           1,103\n",
            "5 importance    4,495\n",
            "6 of            1,104\n",
            "7 the           1,103\n",
            "8 \"               107\n",
            "9 One           1,448\n",
            "10 China         1,975\n",
            "11 \"               107\n",
            "12 policy        2,818\n",
            "13 ,               117\n",
            "14 and           1,105\n",
            "15 was           1,108\n",
            "16 direct        2,904\n",
            "17 in            1,107\n",
            "18 his           1,117\n",
            "19 concerns      5,365\n",
            "20 about         1,164\n",
            "21 Chinese       1,922\n",
            "22 behavior      4,658\n",
            "23 that          1,115\n",
            "24 threatens    18,241\n",
            "25 stability     9,397\n",
            "26 in            1,107\n",
            "27 the           1,103\n",
            "28 Taiwan        6,036\n",
            "29 s               188\n",
            "30 ##tra         4,487\n",
            "31 ##it          2,875\n",
            "32 .               119\n",
            "33 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 34\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 34 x 3072\n",
            "Summed embeddings shape is: 34 x 768\n",
            "0 B               139\n",
            "1 ##iden       26,859\n",
            "2 stressed     13,713\n",
            "3 the           1,103\n",
            "4 importance    4,495\n",
            "5 of            1,104\n",
            "6 the           1,103\n",
            "7 \"               107\n",
            "8 One           1,448\n",
            "9 China         1,975\n",
            "10 \"               107\n",
            "11 policy        2,818\n",
            "12 ,               117\n",
            "13 and           1,105\n",
            "14 was           1,108\n",
            "15 direct        2,904\n",
            "16 in            1,107\n",
            "17 his           1,117\n",
            "18 concerns      5,365\n",
            "19 about         1,164\n",
            "20 Chinese       1,922\n",
            "21 behavior      4,658\n",
            "22 that          1,115\n",
            "23 threatens    18,241\n",
            "24 stability     9,397\n",
            "25 in            1,107\n",
            "26 the           1,103\n",
            "27 Taiwan        6,036\n",
            "28 s               188\n",
            "29 ##tra         4,487\n",
            "30 ##it          2,875\n",
            "31 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 32\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 32 x 3072\n",
            "Summed embeddings shape is: 32 x 768\n",
            "0 [CLS]           101\n",
            "1 But           1,252\n",
            "2 he            1,119\n",
            "3 did           1,225\n",
            "4 not           1,136\n",
            "5 set           1,383\n",
            "6 any           1,251\n",
            "7 of            1,104\n",
            "8 the           1,103\n",
            "9 new           1,207\n",
            "10 \"               107\n",
            "11 guard         3,542\n",
            "12 ##rail       12,797\n",
            "13 ##s           1,116\n",
            "14 \"               107\n",
            "15 referenced   15,780\n",
            "16 at            1,120\n",
            "17 the           1,103\n",
            "18 start         1,838\n",
            "19 of            1,104\n",
            "20 the           1,103\n",
            "21 talks         7,430\n",
            "22 .               119\n",
            "23 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 24 x 3072\n",
            "Summed embeddings shape is: 24 x 768\n",
            "0 But           1,252\n",
            "1 he            1,119\n",
            "2 did           1,225\n",
            "3 not           1,136\n",
            "4 set           1,383\n",
            "5 any           1,251\n",
            "6 of            1,104\n",
            "7 the           1,103\n",
            "8 new           1,207\n",
            "9 \"               107\n",
            "10 guard         3,542\n",
            "11 ##rail       12,797\n",
            "12 ##s           1,116\n",
            "13 \"               107\n",
            "14 referenced   15,780\n",
            "15 at            1,120\n",
            "16 the           1,103\n",
            "17 start         1,838\n",
            "18 of            1,104\n",
            "19 the           1,103\n",
            "20 talks         7,430\n",
            "21 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 22\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 22 x 3072\n",
            "Summed embeddings shape is: 22 x 768\n",
            "0 [CLS]           101\n",
            "1 On            1,212\n",
            "2 Co            3,291\n",
            "3 ##vid        18,312\n",
            "4 -               118\n",
            "5 19            1,627\n",
            "6 ,               117\n",
            "7 B               139\n",
            "8 ##iden       26,859\n",
            "9 re            1,231\n",
            "10 ##af          9,823\n",
            "11 ##firmed     22,750\n",
            "12 the           1,103\n",
            "13 importance    4,495\n",
            "14 of            1,104\n",
            "15 transparency 21,595\n",
            "16 in            1,107\n",
            "17 preventing   10,878\n",
            "18 future        2,174\n",
            "19 outbreak      8,010\n",
            "20 ##s           1,116\n",
            "21 of            1,104\n",
            "22 disease       3,653\n",
            "23 ,               117\n",
            "24 a               170\n",
            "25 nod           6,873\n",
            "26 to            1,106\n",
            "27 China         1,975\n",
            "28 '               112\n",
            "29 s               188\n",
            "30 unwilling    17,026\n",
            "31 ##ness        1,757\n",
            "32 to            1,106\n",
            "33 allow         2,621\n",
            "34 an            1,126\n",
            "35 international  1,835\n",
            "36 investigation  4,449\n",
            "37 into          1,154\n",
            "38 the           1,103\n",
            "39 origins       7,564\n",
            "40 of            1,104\n",
            "41 the           1,103\n",
            "42 current       1,954\n",
            "43 pan          13,316\n",
            "44 ##de          2,007\n",
            "45 ##mic         7,257\n",
            "46 .               119\n",
            "47 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 48\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 48 x 3072\n",
            "Summed embeddings shape is: 48 x 768\n",
            "0 On            1,212\n",
            "1 Co            3,291\n",
            "2 ##vid        18,312\n",
            "3 -               118\n",
            "4 19            1,627\n",
            "5 ,               117\n",
            "6 B               139\n",
            "7 ##iden       26,859\n",
            "8 re            1,231\n",
            "9 ##af          9,823\n",
            "10 ##firmed     22,750\n",
            "11 the           1,103\n",
            "12 importance    4,495\n",
            "13 of            1,104\n",
            "14 transparency 21,595\n",
            "15 in            1,107\n",
            "16 preventing   10,878\n",
            "17 future        2,174\n",
            "18 outbreak      8,010\n",
            "19 ##s           1,116\n",
            "20 of            1,104\n",
            "21 disease       3,653\n",
            "22 ,               117\n",
            "23 a               170\n",
            "24 nod           6,873\n",
            "25 to            1,106\n",
            "26 China         1,975\n",
            "27 '               112\n",
            "28 s               188\n",
            "29 unwilling    17,026\n",
            "30 ##ness        1,757\n",
            "31 to            1,106\n",
            "32 allow         2,621\n",
            "33 an            1,126\n",
            "34 international  1,835\n",
            "35 investigation  4,449\n",
            "36 into          1,154\n",
            "37 the           1,103\n",
            "38 origins       7,564\n",
            "39 of            1,104\n",
            "40 the           1,103\n",
            "41 current       1,954\n",
            "42 pan          13,316\n",
            "43 ##de          2,007\n",
            "44 ##mic         7,257\n",
            "45 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 46\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 46 x 3072\n",
            "Summed embeddings shape is: 46 x 768\n",
            "0 [CLS]           101\n",
            "1 And           1,262\n",
            "2 he            1,119\n",
            "3 raised        2,120\n",
            "4 areas         1,877\n",
            "5 where         1,187\n",
            "6 the           1,103\n",
            "7 US            1,646\n",
            "8 and           1,105\n",
            "9 China         1,975\n",
            "10 can           1,169\n",
            "11 cooperate    19,204\n",
            "12 ,               117\n",
            "13 including     1,259\n",
            "14 on            1,113\n",
            "15 climate       4,530\n",
            "16 change        1,849\n",
            "17 .               119\n",
            "18 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 19\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 19 x 3072\n",
            "Summed embeddings shape is: 19 x 768\n",
            "0 And           1,262\n",
            "1 he            1,119\n",
            "2 raised        2,120\n",
            "3 areas         1,877\n",
            "4 where         1,187\n",
            "5 the           1,103\n",
            "6 US            1,646\n",
            "7 and           1,105\n",
            "8 China         1,975\n",
            "9 can           1,169\n",
            "10 cooperate    19,204\n",
            "11 ,               117\n",
            "12 including     1,259\n",
            "13 on            1,113\n",
            "14 climate       4,530\n",
            "15 change        1,849\n",
            "16 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 17\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 17 x 3072\n",
            "Summed embeddings shape is: 17 x 768\n",
            "0 [CLS]           101\n",
            "1 The           1,109\n",
            "2 two           1,160\n",
            "3 countries     2,182\n",
            "4 recently      3,055\n",
            "5 surprised     3,753\n",
            "6 observers    15,793\n",
            "7 at            1,120\n",
            "8 the           1,103\n",
            "9 CO           18,732\n",
            "10 ##P           2,101\n",
            "11 ##26         25,129\n",
            "12 climate       4,530\n",
            "13 talks         7,430\n",
            "14 in            1,107\n",
            "15 Scotland      3,030\n",
            "16 with          1,114\n",
            "17 a               170\n",
            "18 joint         4,091\n",
            "19 pledge       20,335\n",
            "20 to            1,106\n",
            "21 cut           2,195\n",
            "22 emissions    12,349\n",
            "23 .               119\n",
            "24 [SEP]           102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 25\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 25 x 3072\n",
            "Summed embeddings shape is: 25 x 768\n",
            "0 The           1,109\n",
            "1 two           1,160\n",
            "2 countries     2,182\n",
            "3 recently      3,055\n",
            "4 surprised     3,753\n",
            "5 observers    15,793\n",
            "6 at            1,120\n",
            "7 the           1,103\n",
            "8 CO           18,732\n",
            "9 ##P           2,101\n",
            "10 ##26         25,129\n",
            "11 climate       4,530\n",
            "12 talks         7,430\n",
            "13 in            1,107\n",
            "14 Scotland      3,030\n",
            "15 with          1,114\n",
            "16 a               170\n",
            "17 joint         4,091\n",
            "18 pledge       20,335\n",
            "19 to            1,106\n",
            "20 cut           2,195\n",
            "21 emissions    12,349\n",
            "22 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 23\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 23 x 3072\n",
            "Summed embeddings shape is: 23 x 768\n",
            "0 [CLS]           101\n",
            "1 After         1,258\n",
            "2 speculation  12,828\n",
            "3 Xi           20,802\n",
            "4 might         1,547\n",
            "5 use           1,329\n",
            "6 the           1,103\n",
            "7 meeting       2,309\n",
            "8 to            1,106\n",
            "9 invite       13,967\n",
            "10 B               139\n",
            "11 ##iden       26,859\n",
            "12 to            1,106\n",
            "13 the           1,103\n",
            "14 upcoming      8,851\n",
            "15 Beijing       6,671\n",
            "16 Winter        4,591\n",
            "17 Olympics      2,932\n",
            "18 ,               117\n",
            "19 officials     3,878\n",
            "20 said          1,163\n",
            "21 the           1,103\n",
            "22 topic         8,366\n",
            "23 did           1,225\n",
            "24 not           1,136\n",
            "25 arise        14,368\n",
            "26 .               119\n",
            "27 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 28\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 28 x 3072\n",
            "Summed embeddings shape is: 28 x 768\n",
            "0 After         1,258\n",
            "1 speculation  12,828\n",
            "2 Xi           20,802\n",
            "3 might         1,547\n",
            "4 use           1,329\n",
            "5 the           1,103\n",
            "6 meeting       2,309\n",
            "7 to            1,106\n",
            "8 invite       13,967\n",
            "9 B               139\n",
            "10 ##iden       26,859\n",
            "11 to            1,106\n",
            "12 the           1,103\n",
            "13 upcoming      8,851\n",
            "14 Beijing       6,671\n",
            "15 Winter        4,591\n",
            "16 Olympics      2,932\n",
            "17 ,               117\n",
            "18 officials     3,878\n",
            "19 said          1,163\n",
            "20 the           1,103\n",
            "21 topic         8,366\n",
            "22 did           1,225\n",
            "23 not           1,136\n",
            "24 arise        14,368\n",
            "25 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 26\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 26 x 3072\n",
            "Summed embeddings shape is: 26 x 768\n",
            "0 [CLS]           101\n",
            "1 White         2,061\n",
            "2 House         1,585\n",
            "3 officials     3,878\n",
            "4 had           1,125\n",
            "5 hoped         4,320\n",
            "6 a               170\n",
            "7 large         1,415\n",
            "8 South         1,375\n",
            "9 Lawn         26,189\n",
            "10 signing       6,086\n",
            "11 ceremony      4,502\n",
            "12 for           1,111\n",
            "13 a               170\n",
            "14 massive       4,672\n",
            "15 new           1,207\n",
            "16 public        1,470\n",
            "17 works         1,759\n",
            "18 package       7,305\n",
            "19 ,               117\n",
            "20 which         1,134\n",
            "21 occurred      3,296\n",
            "22 a               170\n",
            "23 few           1,374\n",
            "24 hours         2,005\n",
            "25 before        1,196\n",
            "26 B               139\n",
            "27 ##iden       26,859\n",
            "28 '               112\n",
            "29 s               188\n",
            "30 virtual       8,496\n",
            "31 summit        7,966\n",
            "32 ,               117\n",
            "33 would         1,156\n",
            "34 help          1,494\n",
            "35 signal        4,344\n",
            "36 progress      5,070\n",
            "37 on            1,113\n",
            "38 the           1,103\n",
            "39 main          1,514\n",
            "40 under         1,223\n",
            "41 ##pin         6,709\n",
            "42 ##ning        3,381\n",
            "43 of            1,104\n",
            "44 his           1,117\n",
            "45 foreign       2,880\n",
            "46 policy        2,818\n",
            "47 :               131\n",
            "48 proving      15,975\n",
            "49 demo         11,238\n",
            "50 ##c           1,665\n",
            "51 ##rac        19,366\n",
            "52 ##ies         1,905\n",
            "53 can           1,169\n",
            "54 deliver       7,852\n",
            "55 more          1,167\n",
            "56 effectively   5,877\n",
            "57 than          1,190\n",
            "58 auto         12,365\n",
            "59 ##c           1,665\n",
            "60 ##rac        19,366\n",
            "61 ##ies         1,905\n",
            "62 like          1,176\n",
            "63 China         1,975\n",
            "64 .               119\n",
            "65 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 66\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 66 x 3072\n",
            "Summed embeddings shape is: 66 x 768\n",
            "0 White         2,061\n",
            "1 House         1,585\n",
            "2 officials     3,878\n",
            "3 had           1,125\n",
            "4 hoped         4,320\n",
            "5 a               170\n",
            "6 large         1,415\n",
            "7 South         1,375\n",
            "8 Lawn         26,189\n",
            "9 signing       6,086\n",
            "10 ceremony      4,502\n",
            "11 for           1,111\n",
            "12 a               170\n",
            "13 massive       4,672\n",
            "14 new           1,207\n",
            "15 public        1,470\n",
            "16 works         1,759\n",
            "17 package       7,305\n",
            "18 ,               117\n",
            "19 which         1,134\n",
            "20 occurred      3,296\n",
            "21 a               170\n",
            "22 few           1,374\n",
            "23 hours         2,005\n",
            "24 before        1,196\n",
            "25 B               139\n",
            "26 ##iden       26,859\n",
            "27 '               112\n",
            "28 s               188\n",
            "29 virtual       8,496\n",
            "30 summit        7,966\n",
            "31 ,               117\n",
            "32 would         1,156\n",
            "33 help          1,494\n",
            "34 signal        4,344\n",
            "35 progress      5,070\n",
            "36 on            1,113\n",
            "37 the           1,103\n",
            "38 main          1,514\n",
            "39 under         1,223\n",
            "40 ##pin         6,709\n",
            "41 ##ning        3,381\n",
            "42 of            1,104\n",
            "43 his           1,117\n",
            "44 foreign       2,880\n",
            "45 policy        2,818\n",
            "46 :               131\n",
            "47 proving      15,975\n",
            "48 demo         11,238\n",
            "49 ##c           1,665\n",
            "50 ##rac        19,366\n",
            "51 ##ies         1,905\n",
            "52 can           1,169\n",
            "53 deliver       7,852\n",
            "54 more          1,167\n",
            "55 effectively   5,877\n",
            "56 than          1,190\n",
            "57 auto         12,365\n",
            "58 ##c           1,665\n",
            "59 ##rac        19,366\n",
            "60 ##ies         1,905\n",
            "61 like          1,176\n",
            "62 China         1,975\n",
            "63 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 64\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 64 x 3072\n",
            "Summed embeddings shape is: 64 x 768\n",
            "0 [CLS]           101\n",
            "1 He            1,124\n",
            "2 planned       2,919\n",
            "3 to            1,106\n",
            "4 detail        6,505\n",
            "5 the           1,103\n",
            "6 new           1,207\n",
            "7 infrastructure  6,557\n",
            "8 package       7,305\n",
            "9 to            1,106\n",
            "10 Xi           20,802\n",
            "11 .               119\n",
            "12 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 13\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 13 x 3072\n",
            "Summed embeddings shape is: 13 x 768\n",
            "0 He            1,124\n",
            "1 planned       2,919\n",
            "2 to            1,106\n",
            "3 detail        6,505\n",
            "4 the           1,103\n",
            "5 new           1,207\n",
            "6 infrastructure  6,557\n",
            "7 package       7,305\n",
            "8 to            1,106\n",
            "9 Xi           20,802\n",
            "10 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 11\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 11 x 3072\n",
            "Summed embeddings shape is: 11 x 768\n",
            "0 [CLS]           101\n",
            "1 The           1,109\n",
            "2 fact          1,864\n",
            "3 the           1,103\n",
            "4 bill          4,550\n",
            "5 was           1,108\n",
            "6 passed        2,085\n",
            "7 with          1,114\n",
            "8 help          1,494\n",
            "9 from          1,121\n",
            "10 some          1,199\n",
            "11 Republicans  11,115\n",
            "12 -               118\n",
            "13 -               118\n",
            "14 fulfilling   24,560\n",
            "15 B               139\n",
            "16 ##iden       26,859\n",
            "17 '               112\n",
            "18 s               188\n",
            "19 promise       4,437\n",
            "20 to            1,106\n",
            "21 work          1,250\n",
            "22 across        1,506\n",
            "23 party         1,710\n",
            "24 lines         2,442\n",
            "25 -               118\n",
            "26 -               118\n",
            "27 helps         6,618\n",
            "28 sustain      16,974\n",
            "29 his           1,117\n",
            "30 pledge       20,335\n",
            "31 to            1,106\n",
            "32 prove         5,424\n",
            "33 demo         11,238\n",
            "34 ##c           1,665\n",
            "35 ##rac        19,366\n",
            "36 ##ies         1,905\n",
            "37 can           1,169\n",
            "38 work          1,250\n",
            "39 ,               117\n",
            "40 according     2,452\n",
            "41 to            1,106\n",
            "42 the           1,103\n",
            "43 officials     3,878\n",
            "44 .               119\n",
            "45 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 46\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 46 x 3072\n",
            "Summed embeddings shape is: 46 x 768\n",
            "0 The           1,109\n",
            "1 fact          1,864\n",
            "2 the           1,103\n",
            "3 bill          4,550\n",
            "4 was           1,108\n",
            "5 passed        2,085\n",
            "6 with          1,114\n",
            "7 help          1,494\n",
            "8 from          1,121\n",
            "9 some          1,199\n",
            "10 Republicans  11,115\n",
            "11 -               118\n",
            "12 -               118\n",
            "13 fulfilling   24,560\n",
            "14 B               139\n",
            "15 ##iden       26,859\n",
            "16 '               112\n",
            "17 s               188\n",
            "18 promise       4,437\n",
            "19 to            1,106\n",
            "20 work          1,250\n",
            "21 across        1,506\n",
            "22 party         1,710\n",
            "23 lines         2,442\n",
            "24 -               118\n",
            "25 -               118\n",
            "26 helps         6,618\n",
            "27 sustain      16,974\n",
            "28 his           1,117\n",
            "29 pledge       20,335\n",
            "30 to            1,106\n",
            "31 prove         5,424\n",
            "32 demo         11,238\n",
            "33 ##c           1,665\n",
            "34 ##rac        19,366\n",
            "35 ##ies         1,905\n",
            "36 can           1,169\n",
            "37 work          1,250\n",
            "38 ,               117\n",
            "39 according     2,452\n",
            "40 to            1,106\n",
            "41 the           1,103\n",
            "42 officials     3,878\n",
            "43 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 44\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 44 x 3072\n",
            "Summed embeddings shape is: 44 x 768\n",
            "0 [CLS]           101\n",
            "1 Yet           6,355\n",
            "2 he            1,119\n",
            "3 still         1,253\n",
            "4 entered       2,242\n",
            "5 the           1,103\n",
            "6 talks         7,430\n",
            "7 at            1,120\n",
            "8 a               170\n",
            "9 politically  10,966\n",
            "10 weakened     12,041\n",
            "11 moment        1,721\n",
            "12 .               119\n",
            "13 [SEP]           102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 14\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 14 x 3072\n",
            "Summed embeddings shape is: 14 x 768\n",
            "0 Yet           6,355\n",
            "1 he            1,119\n",
            "2 still         1,253\n",
            "3 entered       2,242\n",
            "4 the           1,103\n",
            "5 talks         7,430\n",
            "6 at            1,120\n",
            "7 a               170\n",
            "8 politically  10,966\n",
            "9 weakened     12,041\n",
            "10 moment        1,721\n",
            "11 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 12\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 12 x 3072\n",
            "Summed embeddings shape is: 12 x 768\n",
            "0 [CLS]           101\n",
            "1 His           1,230\n",
            "2 party         1,710\n",
            "3 fare         14,550\n",
            "4 ##d           1,181\n",
            "5 poorly        9,874\n",
            "6 in            1,107\n",
            "7 off           1,228\n",
            "8 -               118\n",
            "9 year          1,214\n",
            "10 elections     3,212\n",
            "11 this          1,142\n",
            "12 month         2,370\n",
            "13 in            1,107\n",
            "14 Virginia      2,550\n",
            "15 ,               117\n",
            "16 and           1,105\n",
            "17 polls        15,995\n",
            "18 continue      2,760\n",
            "19 to            1,106\n",
            "20 show          1,437\n",
            "21 his           1,117\n",
            "22 approval      5,684\n",
            "23 rating        5,261\n",
            "24 at            1,120\n",
            "25 some          1,199\n",
            "26 of            1,104\n",
            "27 the           1,103\n",
            "28 lowest        6,905\n",
            "29 levels        3,001\n",
            "30 of            1,104\n",
            "31 his           1,117\n",
            "32 presidency   11,223\n",
            "33 .               119\n",
            "34 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 35\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 35 x 3072\n",
            "Summed embeddings shape is: 35 x 768\n",
            "0 His           1,230\n",
            "1 party         1,710\n",
            "2 fare         14,550\n",
            "3 ##d           1,181\n",
            "4 poorly        9,874\n",
            "5 in            1,107\n",
            "6 off           1,228\n",
            "7 -               118\n",
            "8 year          1,214\n",
            "9 elections     3,212\n",
            "10 this          1,142\n",
            "11 month         2,370\n",
            "12 in            1,107\n",
            "13 Virginia      2,550\n",
            "14 ,               117\n",
            "15 and           1,105\n",
            "16 polls        15,995\n",
            "17 continue      2,760\n",
            "18 to            1,106\n",
            "19 show          1,437\n",
            "20 his           1,117\n",
            "21 approval      5,684\n",
            "22 rating        5,261\n",
            "23 at            1,120\n",
            "24 some          1,199\n",
            "25 of            1,104\n",
            "26 the           1,103\n",
            "27 lowest        6,905\n",
            "28 levels        3,001\n",
            "29 of            1,104\n",
            "30 his           1,117\n",
            "31 presidency   11,223\n",
            "32 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 33\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 33 x 3072\n",
            "Summed embeddings shape is: 33 x 768\n",
            "0 [CLS]           101\n",
            "1 That          1,337\n",
            "2 is            1,110\n",
            "3 in            1,107\n",
            "4 sharp         4,295\n",
            "5 contrast      5,014\n",
            "6 to            1,106\n",
            "7 Xi           20,802\n",
            "8 ,               117\n",
            "9 whose         2,133\n",
            "10 consolidation 20,994\n",
            "11 of            1,104\n",
            "12 power         1,540\n",
            "13 in            1,107\n",
            "14 China         1,975\n",
            "15 was           1,108\n",
            "16 cement       12,527\n",
            "17 ##ed          1,174\n",
            "18 last          1,314\n",
            "19 week          1,989\n",
            "20 when          1,165\n",
            "21 the           1,103\n",
            "22 Chinese       1,922\n",
            "23 Communist     5,248\n",
            "24 Party         1,786\n",
            "25 adopted       3,399\n",
            "26 a               170\n",
            "27 landmark     10,755\n",
            "28 resolution    6,021\n",
            "29 el            8,468\n",
            "30 ##eva        12,853\n",
            "31 ##ting        1,916\n",
            "32 him           1,140\n",
            "33 in            1,107\n",
            "34 stature      23,432\n",
            "35 to            1,106\n",
            "36 his           1,117\n",
            "37 two           1,160\n",
            "38 most          1,211\n",
            "39 powerful      3,110\n",
            "40 predecessors 17,543\n",
            "41 -               118\n",
            "42 -               118\n",
            "43 Mao          16,922\n",
            "44 Z               163\n",
            "45 ##ed          1,174\n",
            "46 ##ong         4,553\n",
            "47 and           1,105\n",
            "48 Den          14,760\n",
            "49 ##g           1,403\n",
            "50 Xiao         22,238\n",
            "51 ##ping        2,624\n",
            "52 .               119\n",
            "53 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 54\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 54 x 3072\n",
            "Summed embeddings shape is: 54 x 768\n",
            "0 That          1,337\n",
            "1 is            1,110\n",
            "2 in            1,107\n",
            "3 sharp         4,295\n",
            "4 contrast      5,014\n",
            "5 to            1,106\n",
            "6 Xi           20,802\n",
            "7 ,               117\n",
            "8 whose         2,133\n",
            "9 consolidation 20,994\n",
            "10 of            1,104\n",
            "11 power         1,540\n",
            "12 in            1,107\n",
            "13 China         1,975\n",
            "14 was           1,108\n",
            "15 cement       12,527\n",
            "16 ##ed          1,174\n",
            "17 last          1,314\n",
            "18 week          1,989\n",
            "19 when          1,165\n",
            "20 the           1,103\n",
            "21 Chinese       1,922\n",
            "22 Communist     5,248\n",
            "23 Party         1,786\n",
            "24 adopted       3,399\n",
            "25 a               170\n",
            "26 landmark     10,755\n",
            "27 resolution    6,021\n",
            "28 el            8,468\n",
            "29 ##eva        12,853\n",
            "30 ##ting        1,916\n",
            "31 him           1,140\n",
            "32 in            1,107\n",
            "33 stature      23,432\n",
            "34 to            1,106\n",
            "35 his           1,117\n",
            "36 two           1,160\n",
            "37 most          1,211\n",
            "38 powerful      3,110\n",
            "39 predecessors 17,543\n",
            "40 -               118\n",
            "41 -               118\n",
            "42 Mao          16,922\n",
            "43 Z               163\n",
            "44 ##ed          1,174\n",
            "45 ##ong         4,553\n",
            "46 and           1,105\n",
            "47 Den          14,760\n",
            "48 ##g           1,403\n",
            "49 Xiao         22,238\n",
            "50 ##ping        2,624\n",
            "51 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 52\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 52 x 3072\n",
            "Summed embeddings shape is: 52 x 768\n",
            "0 [CLS]           101\n",
            "1 Official      9,018\n",
            "2 ##s           1,116\n",
            "3 said          1,163\n",
            "4 the           1,103\n",
            "5 up            1,146\n",
            "6 ##grading    20,407\n",
            "7 of            1,104\n",
            "8 Xi           20,802\n",
            "9 '               112\n",
            "10 s               188\n",
            "11 status        2,781\n",
            "12 only          1,178\n",
            "13 enhanced      9,927\n",
            "14 the           1,103\n",
            "15 imp          24,034\n",
            "16 ##erative    21,126\n",
            "17 of            1,104\n",
            "18 a               170\n",
            "19 face          1,339\n",
            "20 -               118\n",
            "21 to            1,106\n",
            "22 -               118\n",
            "23 face          1,339\n",
            "24 with          1,114\n",
            "25 B               139\n",
            "26 ##iden       26,859\n",
            "27 .               119\n",
            "28 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 29\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 29 x 3072\n",
            "Summed embeddings shape is: 29 x 768\n",
            "0 Official      9,018\n",
            "1 ##s           1,116\n",
            "2 said          1,163\n",
            "3 the           1,103\n",
            "4 up            1,146\n",
            "5 ##grading    20,407\n",
            "6 of            1,104\n",
            "7 Xi           20,802\n",
            "8 '               112\n",
            "9 s               188\n",
            "10 status        2,781\n",
            "11 only          1,178\n",
            "12 enhanced      9,927\n",
            "13 the           1,103\n",
            "14 imp          24,034\n",
            "15 ##erative    21,126\n",
            "16 of            1,104\n",
            "17 a               170\n",
            "18 face          1,339\n",
            "19 -               118\n",
            "20 to            1,106\n",
            "21 -               118\n",
            "22 face          1,339\n",
            "23 with          1,114\n",
            "24 B               139\n",
            "25 ##iden       26,859\n",
            "26 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 27\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 27 x 3072\n",
            "Summed embeddings shape is: 27 x 768\n",
            "0 [CLS]           101\n",
            "1 Nearly       16,992\n",
            "2 every         1,451\n",
            "3 issue         2,486\n",
            "4 B               139\n",
            "5 ##iden       26,859\n",
            "6 is            1,110\n",
            "7 focused       3,378\n",
            "8 on            1,113\n",
            "9 ,               117\n",
            "10 domestic      4,500\n",
            "11 ##ally        2,716\n",
            "12 and           1,105\n",
            "13 internationally  7,460\n",
            "14 ,               117\n",
            "15 has           1,144\n",
            "16 a               170\n",
            "17 ne           24,928\n",
            "18 ##x           1,775\n",
            "19 ##us          1,361\n",
            "20 to            1,106\n",
            "21 China         1,975\n",
            "22 .               119\n",
            "23 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 24 x 3072\n",
            "Summed embeddings shape is: 24 x 768\n",
            "0 Nearly       16,992\n",
            "1 every         1,451\n",
            "2 issue         2,486\n",
            "3 B               139\n",
            "4 ##iden       26,859\n",
            "5 is            1,110\n",
            "6 focused       3,378\n",
            "7 on            1,113\n",
            "8 ,               117\n",
            "9 domestic      4,500\n",
            "10 ##ally        2,716\n",
            "11 and           1,105\n",
            "12 internationally  7,460\n",
            "13 ,               117\n",
            "14 has           1,144\n",
            "15 a               170\n",
            "16 ne           24,928\n",
            "17 ##x           1,775\n",
            "18 ##us          1,361\n",
            "19 to            1,106\n",
            "20 China         1,975\n",
            "21 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 22\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 22 x 3072\n",
            "Summed embeddings shape is: 22 x 768\n",
            "0 [CLS]           101\n",
            "1 Supply       17,153\n",
            "2 chain         4,129\n",
            "3 issues        2,492\n",
            "4 that          1,115\n",
            "5 are           1,132\n",
            "6 driving       3,759\n",
            "7 inflation    15,503\n",
            "8 at            1,120\n",
            "9 home          1,313\n",
            "10 can           1,169\n",
            "11 be            1,129\n",
            "12 traced        9,286\n",
            "13 in            1,107\n",
            "14 part          1,226\n",
            "15 to            1,106\n",
            "16 shortages    25,630\n",
            "17 in            1,107\n",
            "18 Chinese       1,922\n",
            "19 plants        3,546\n",
            "20 .               119\n",
            "21 [SEP]           102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 22\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 22 x 3072\n",
            "Summed embeddings shape is: 22 x 768\n",
            "0 Supply       17,153\n",
            "1 chain         4,129\n",
            "2 issues        2,492\n",
            "3 that          1,115\n",
            "4 are           1,132\n",
            "5 driving       3,759\n",
            "6 inflation    15,503\n",
            "7 at            1,120\n",
            "8 home          1,313\n",
            "9 can           1,169\n",
            "10 be            1,129\n",
            "11 traced        9,286\n",
            "12 in            1,107\n",
            "13 part          1,226\n",
            "14 to            1,106\n",
            "15 shortages    25,630\n",
            "16 in            1,107\n",
            "17 Chinese       1,922\n",
            "18 plants        3,546\n",
            "19 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 20\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 20 x 3072\n",
            "Summed embeddings shape is: 20 x 768\n",
            "0 [CLS]           101\n",
            "1 Combat       10,314\n",
            "2 ##ting        1,916\n",
            "3 climate       4,530\n",
            "4 change        1,849\n",
            "5 requires      5,315\n",
            "6 buy           4,417\n",
            "7 -               118\n",
            "8 in            1,107\n",
            "9 from          1,121\n",
            "10 Xi           20,802\n",
            "11 ,               117\n",
            "12 who           1,150\n",
            "13 has           1,144\n",
            "14 shown         2,602\n",
            "15 some          1,199\n",
            "16 willingness  21,623\n",
            "17 to            1,106\n",
            "18 partner       3,547\n",
            "19 with          1,114\n",
            "20 B               139\n",
            "21 ##iden       26,859\n",
            "22 on            1,113\n",
            "23 the           1,103\n",
            "24 issue         2,486\n",
            "25 .               119\n",
            "26 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 27\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 27 x 3072\n",
            "Summed embeddings shape is: 27 x 768\n",
            "0 Combat       10,314\n",
            "1 ##ting        1,916\n",
            "2 climate       4,530\n",
            "3 change        1,849\n",
            "4 requires      5,315\n",
            "5 buy           4,417\n",
            "6 -               118\n",
            "7 in            1,107\n",
            "8 from          1,121\n",
            "9 Xi           20,802\n",
            "10 ,               117\n",
            "11 who           1,150\n",
            "12 has           1,144\n",
            "13 shown         2,602\n",
            "14 some          1,199\n",
            "15 willingness  21,623\n",
            "16 to            1,106\n",
            "17 partner       3,547\n",
            "18 with          1,114\n",
            "19 B               139\n",
            "20 ##iden       26,859\n",
            "21 on            1,113\n",
            "22 the           1,103\n",
            "23 issue         2,486\n",
            "24 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 25\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 25 x 3072\n",
            "Summed embeddings shape is: 25 x 768\n",
            "0 [CLS]           101\n",
            "1 Managing     16,522\n",
            "2 global        4,265\n",
            "3 trouble       3,819\n",
            "4 -               118\n",
            "5 spots         7,152\n",
            "6 like          1,176\n",
            "7 North         1,456\n",
            "8 Korea         3,577\n",
            "9 and           1,105\n",
            "10 Iran          3,398\n",
            "11 each          1,296\n",
            "12 involves      6,808\n",
            "13 coordination 14,501\n",
            "14 with          1,114\n",
            "15 Beijing       6,671\n",
            "16 .               119\n",
            "17 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 18\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 18 x 3072\n",
            "Summed embeddings shape is: 18 x 768\n",
            "0 Managing     16,522\n",
            "1 global        4,265\n",
            "2 trouble       3,819\n",
            "3 -               118\n",
            "4 spots         7,152\n",
            "5 like          1,176\n",
            "6 North         1,456\n",
            "7 Korea         3,577\n",
            "8 and           1,105\n",
            "9 Iran          3,398\n",
            "10 each          1,296\n",
            "11 involves      6,808\n",
            "12 coordination 14,501\n",
            "13 with          1,114\n",
            "14 Beijing       6,671\n",
            "15 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 16\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 16 x 3072\n",
            "Summed embeddings shape is: 16 x 768\n",
            "0 [CLS]           101\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 is            1,110\n",
            "4 a               170\n",
            "5 fan           5,442\n",
            "6 of            1,104\n",
            "7 in            1,107\n",
            "8 -               118\n",
            "9 person        1,825\n",
            "10 meetings      5,845\n",
            "11 and           1,105\n",
            "12 complained   10,790\n",
            "13 early         1,346\n",
            "14 in            1,107\n",
            "15 his           1,117\n",
            "16 presidency   11,223\n",
            "17 that          1,115\n",
            "18 virtual       8,496\n",
            "19 summit        7,966\n",
            "20 ##s           1,116\n",
            "21 -               118\n",
            "22 -               118\n",
            "23 where         1,187\n",
            "24 foreign       2,880\n",
            "25 leaders       3,478\n",
            "26 are           1,132\n",
            "27 patch        10,085\n",
            "28 ##ed          1,174\n",
            "29 in            1,107\n",
            "30 on            1,113\n",
            "31 video         1,888\n",
            "32 screens      12,472\n",
            "33 -               118\n",
            "34 -               118\n",
            "35 could         1,180\n",
            "36 not           1,136\n",
            "37 replica      16,498\n",
            "38 ##te          1,566\n",
            "39 the           1,103\n",
            "40 chemistry     8,117\n",
            "41 of            1,104\n",
            "42 sitting       2,807\n",
            "43 face          1,339\n",
            "44 -               118\n",
            "45 to            1,106\n",
            "46 -               118\n",
            "47 face          1,339\n",
            "48 .               119\n",
            "49 American      1,237\n",
            "50 officials     3,878\n",
            "51 say           1,474\n",
            "52 leader        2,301\n",
            "53 -               118\n",
            "54 to            1,106\n",
            "55 -               118\n",
            "56 leader        2,301\n",
            "57 meetings      5,845\n",
            "58 are           1,132\n",
            "59 even          1,256\n",
            "60 more          1,167\n",
            "61 important     1,696\n",
            "62 with          1,114\n",
            "63 Xi           20,802\n",
            "64 ,               117\n",
            "65 whose         2,133\n",
            "66 inner         5,047\n",
            "67 circle        4,726\n",
            "68 has           1,144\n",
            "69 become        1,561\n",
            "70 smaller       2,964\n",
            "71 and           1,105\n",
            "72 smaller       2,964\n",
            "73 and           1,105\n",
            "74 who           1,150\n",
            "75 now           1,208\n",
            "76 w               192\n",
            "77 ##ield       12,350\n",
            "78 ##s           1,116\n",
            "79 a               170\n",
            "80 historic      3,432\n",
            "81 level         1,634\n",
            "82 of            1,104\n",
            "83 power         1,540\n",
            "84 .               119\n",
            "85 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 86\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 86 x 3072\n",
            "Summed embeddings shape is: 86 x 768\n",
            "0 B               139\n",
            "1 ##iden       26,859\n",
            "2 is            1,110\n",
            "3 a               170\n",
            "4 fan           5,442\n",
            "5 of            1,104\n",
            "6 in            1,107\n",
            "7 -               118\n",
            "8 person        1,825\n",
            "9 meetings      5,845\n",
            "10 and           1,105\n",
            "11 complained   10,790\n",
            "12 early         1,346\n",
            "13 in            1,107\n",
            "14 his           1,117\n",
            "15 presidency   11,223\n",
            "16 that          1,115\n",
            "17 virtual       8,496\n",
            "18 summit        7,966\n",
            "19 ##s           1,116\n",
            "20 -               118\n",
            "21 -               118\n",
            "22 where         1,187\n",
            "23 foreign       2,880\n",
            "24 leaders       3,478\n",
            "25 are           1,132\n",
            "26 patch        10,085\n",
            "27 ##ed          1,174\n",
            "28 in            1,107\n",
            "29 on            1,113\n",
            "30 video         1,888\n",
            "31 screens      12,472\n",
            "32 -               118\n",
            "33 -               118\n",
            "34 could         1,180\n",
            "35 not           1,136\n",
            "36 replica      16,498\n",
            "37 ##te          1,566\n",
            "38 the           1,103\n",
            "39 chemistry     8,117\n",
            "40 of            1,104\n",
            "41 sitting       2,807\n",
            "42 face          1,339\n",
            "43 -               118\n",
            "44 to            1,106\n",
            "45 -               118\n",
            "46 face          1,339\n",
            "47 .               119\n",
            "48 American      1,237\n",
            "49 officials     3,878\n",
            "50 say           1,474\n",
            "51 leader        2,301\n",
            "52 -               118\n",
            "53 to            1,106\n",
            "54 -               118\n",
            "55 leader        2,301\n",
            "56 meetings      5,845\n",
            "57 are           1,132\n",
            "58 even          1,256\n",
            "59 more          1,167\n",
            "60 important     1,696\n",
            "61 with          1,114\n",
            "62 Xi           20,802\n",
            "63 ,               117\n",
            "64 whose         2,133\n",
            "65 inner         5,047\n",
            "66 circle        4,726\n",
            "67 has           1,144\n",
            "68 become        1,561\n",
            "69 smaller       2,964\n",
            "70 and           1,105\n",
            "71 smaller       2,964\n",
            "72 and           1,105\n",
            "73 who           1,150\n",
            "74 now           1,208\n",
            "75 w               192\n",
            "76 ##ield       12,350\n",
            "77 ##s           1,116\n",
            "78 a               170\n",
            "79 historic      3,432\n",
            "80 level         1,634\n",
            "81 of            1,104\n",
            "82 power         1,540\n",
            "83 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 84\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 84 x 3072\n",
            "Summed embeddings shape is: 84 x 768\n",
            "0 [CLS]           101\n",
            "1 Over          3,278\n",
            "2 the           1,103\n",
            "3 summer        2,247\n",
            "4 ,               117\n",
            "5 aide         17,512\n",
            "6 ##s           1,116\n",
            "7 were          1,127\n",
            "8 hopeful      19,953\n",
            "9 of            1,104\n",
            "10 setting       3,545\n",
            "11 up            1,146\n",
            "12 a               170\n",
            "13 meeting       2,309\n",
            "14 between       1,206\n",
            "15 the           1,103\n",
            "16 two           1,160\n",
            "17 men           1,441\n",
            "18 on            1,113\n",
            "19 the           1,103\n",
            "20 side          1,334\n",
            "21 ##lines      10,443\n",
            "22 of            1,104\n",
            "23 this          1,142\n",
            "24 year          1,214\n",
            "25 '               112\n",
            "26 s               188\n",
            "27 Group         1,990\n",
            "28 of            1,104\n",
            "29 20            1,406\n",
            "30 summit        7,966\n",
            "31 in            1,107\n",
            "32 Rome          3,352\n",
            "33 .               119\n",
            "34 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 35\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 35 x 3072\n",
            "Summed embeddings shape is: 35 x 768\n",
            "0 Over          3,278\n",
            "1 the           1,103\n",
            "2 summer        2,247\n",
            "3 ,               117\n",
            "4 aide         17,512\n",
            "5 ##s           1,116\n",
            "6 were          1,127\n",
            "7 hopeful      19,953\n",
            "8 of            1,104\n",
            "9 setting       3,545\n",
            "10 up            1,146\n",
            "11 a               170\n",
            "12 meeting       2,309\n",
            "13 between       1,206\n",
            "14 the           1,103\n",
            "15 two           1,160\n",
            "16 men           1,441\n",
            "17 on            1,113\n",
            "18 the           1,103\n",
            "19 side          1,334\n",
            "20 ##lines      10,443\n",
            "21 of            1,104\n",
            "22 this          1,142\n",
            "23 year          1,214\n",
            "24 '               112\n",
            "25 s               188\n",
            "26 Group         1,990\n",
            "27 of            1,104\n",
            "28 20            1,406\n",
            "29 summit        7,966\n",
            "30 in            1,107\n",
            "31 Rome          3,352\n",
            "32 .               119\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 33\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 33 x 3072\n",
            "Summed embeddings shape is: 33 x 768\n",
            "0 [CLS]           101\n",
            "1 But           1,252\n",
            "2 Xi           20,802\n",
            "3 has           1,144\n",
            "4 not           1,136\n",
            "5 left          1,286\n",
            "6 China         1,975\n",
            "7 in            1,107\n",
            "8 nearly        2,212\n",
            "9 two           1,160\n",
            "10 years         1,201\n",
            "11 ,               117\n",
            "12 partly        6,146\n",
            "13 over          1,166\n",
            "14 Co            3,291\n",
            "15 ##vid        18,312\n",
            "16 -               118\n",
            "17 19            1,627\n",
            "18 concerns      5,365\n",
            "19 .               119\n",
            "20 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 21\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 21 x 3072\n",
            "Summed embeddings shape is: 21 x 768\n",
            "0 But           1,252\n",
            "1 Xi           20,802\n",
            "2 has           1,144\n",
            "3 not           1,136\n",
            "4 left          1,286\n",
            "5 China         1,975\n",
            "6 in            1,107\n",
            "7 nearly        2,212\n",
            "8 two           1,160\n",
            "9 years         1,201\n",
            "10 ,               117\n",
            "11 partly        6,146\n",
            "12 over          1,166\n",
            "13 Co            3,291\n",
            "14 ##vid        18,312\n",
            "15 -               118\n",
            "16 19            1,627\n",
            "17 concerns      5,365\n",
            "18 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 19\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 19 x 3072\n",
            "Summed embeddings shape is: 19 x 768\n",
            "0 [CLS]           101\n",
            "1 So            1,573\n",
            "2 B               139\n",
            "3 ##iden       26,859\n",
            "4 settled       3,035\n",
            "5 on            1,113\n",
            "6 a               170\n",
            "7 virtual       8,496\n",
            "8 summit        7,966\n",
            "9 instead       1,939\n",
            "10 as            1,112\n",
            "11 a               170\n",
            "12 way           1,236\n",
            "13 to            1,106\n",
            "14 advance       4,657\n",
            "15 his           1,117\n",
            "16 two           1,160\n",
            "17 previous      2,166\n",
            "18 phone         2,179\n",
            "19 conversations 12,705\n",
            "20 with          1,114\n",
            "21 Xi           20,802\n",
            "22 .               119\n",
            "23 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 24\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 24 x 3072\n",
            "Summed embeddings shape is: 24 x 768\n",
            "0 So            1,573\n",
            "1 B               139\n",
            "2 ##iden       26,859\n",
            "3 settled       3,035\n",
            "4 on            1,113\n",
            "5 a               170\n",
            "6 virtual       8,496\n",
            "7 summit        7,966\n",
            "8 instead       1,939\n",
            "9 as            1,112\n",
            "10 a               170\n",
            "11 way           1,236\n",
            "12 to            1,106\n",
            "13 advance       4,657\n",
            "14 his           1,117\n",
            "15 two           1,160\n",
            "16 previous      2,166\n",
            "17 phone         2,179\n",
            "18 conversations 12,705\n",
            "19 with          1,114\n",
            "20 Xi           20,802\n",
            "21 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 22\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 22 x 3072\n",
            "Summed embeddings shape is: 22 x 768\n",
            "0 [CLS]           101\n",
            "1 \"               107\n",
            "2 There         1,247\n",
            "3 is            1,110\n",
            "4 something     1,380\n",
            "5 different     1,472\n",
            "6 about         1,164\n",
            "7 actually      2,140\n",
            "8 seeing        3,195\n",
            "9 someone       1,800\n",
            "10 physically    8,027\n",
            "11 ,               117\n",
            "12 about         1,164\n",
            "13 the           1,103\n",
            "14 depth         5,415\n",
            "15 of            1,104\n",
            "16 the           1,103\n",
            "17 conversation  3,771\n",
            "18 you           1,128\n",
            "19 can           1,169\n",
            "20 have          1,138\n",
            "21 ,               117\n",
            "22 versus        6,055\n",
            "23 just          1,198\n",
            "24 on            1,113\n",
            "25 a               170\n",
            "26 regular       2,366\n",
            "27 phone         2,179\n",
            "28 line          1,413\n",
            "29 ,               117\n",
            "30 \"               107\n",
            "31 the           1,103\n",
            "32 senior        2,682\n",
            "33 administration  3,469\n",
            "34 official      2,078\n",
            "35 said          1,163\n",
            "36 earlier       2,206\n",
            "37 ,               117\n",
            "38 describing    7,645\n",
            "39 different     1,472\n",
            "40 ways          3,242\n",
            "41 of            1,104\n",
            "42 preparing     7,963\n",
            "43 for           1,111\n",
            "44 a               170\n",
            "45 video         1,888\n",
            "46 conference    3,511\n",
            "47 compared      3,402\n",
            "48 to            1,106\n",
            "49 just          1,198\n",
            "50 a               170\n",
            "51 phone         2,179\n",
            "52 conversation  3,771\n",
            "53 .               119\n",
            "54 [SEP]           102\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 55\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 55 x 3072\n",
            "Summed embeddings shape is: 55 x 768\n",
            "0 \"               107\n",
            "1 There         1,247\n",
            "2 is            1,110\n",
            "3 something     1,380\n",
            "4 different     1,472\n",
            "5 about         1,164\n",
            "6 actually      2,140\n",
            "7 seeing        3,195\n",
            "8 someone       1,800\n",
            "9 physically    8,027\n",
            "10 ,               117\n",
            "11 about         1,164\n",
            "12 the           1,103\n",
            "13 depth         5,415\n",
            "14 of            1,104\n",
            "15 the           1,103\n",
            "16 conversation  3,771\n",
            "17 you           1,128\n",
            "18 can           1,169\n",
            "19 have          1,138\n",
            "20 ,               117\n",
            "21 versus        6,055\n",
            "22 just          1,198\n",
            "23 on            1,113\n",
            "24 a               170\n",
            "25 regular       2,366\n",
            "26 phone         2,179\n",
            "27 line          1,413\n",
            "28 ,               117\n",
            "29 \"               107\n",
            "30 the           1,103\n",
            "31 senior        2,682\n",
            "32 administration  3,469\n",
            "33 official      2,078\n",
            "34 said          1,163\n",
            "35 earlier       2,206\n",
            "36 ,               117\n",
            "37 describing    7,645\n",
            "38 different     1,472\n",
            "39 ways          3,242\n",
            "40 of            1,104\n",
            "41 preparing     7,963\n",
            "42 for           1,111\n",
            "43 a               170\n",
            "44 video         1,888\n",
            "45 conference    3,511\n",
            "46 compared      3,402\n",
            "47 to            1,106\n",
            "48 just          1,198\n",
            "49 a               170\n",
            "50 phone         2,179\n",
            "51 conversation  3,771\n",
            "52 .               119\n",
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 53\n",
            "Number of hidden units: 768\n",
            "Concatenated embeddings shape is: 53 x 3072\n",
            "Summed embeddings shape is: 53 x 768\n"
          ]
        }
      ],
      "source": [
        "bert_sentence_ids = {}\n",
        "bert_cat_embeddings = {}\n",
        "bert_sum_embeddings = {}\n",
        "roberta_sentence_ids = {}\n",
        "roberta_cat_embeddings = {}\n",
        "roberta_sum_embeddings = {}\n",
        "\n",
        "# get BERT and RoBERTa emebddings for every token\n",
        "for sentence in training_data:\n",
        "    bert_tokenized_text, seq_embedding, pooled_embedding, bert_hidden_states = \\\n",
        "        get_sentence_embedding(sentence, bert_base_cased_tokenizer, bert_base_cased_model)\n",
        "    bert_sentence_ids[len(bert_sentence_ids)] = bert_tokenized_text\n",
        "    token_embeddings = torch.squeeze(torch.stack(bert_hidden_states, dim=0), dim=1).permute(1,0,2)\n",
        "    c_emb, s_emb = get_token_embeddings(token_embeddings, 4, _)\n",
        "    bert_cat_embeddings[len(bert_cat_embeddings)] = c_emb\n",
        "    bert_sum_embeddings[len(bert_sum_embeddings)] = s_emb\n",
        "\n",
        "    roberta_tokenized_text, seq_embedding, pooled_embedding, roberta_hidden_states = \\\n",
        "        get_sentence_embedding(sentence, bert_base_cased_tokenizer, roberta_base_model) # using BERT tokenizer with RoBERTa\n",
        "    roberta_sentence_ids[len(roberta_sentence_ids)] = roberta_tokenized_text\n",
        "    token_embeddings = torch.squeeze(torch.stack(roberta_hidden_states, dim=0), dim=1).permute(1,0,2)\n",
        "    c_emb, s_emb = get_token_embeddings(token_embeddings, 4, _)\n",
        "    roberta_cat_embeddings[len(roberta_cat_embeddings)] = c_emb\n",
        "    roberta_sum_embeddings[len(roberta_sum_embeddings)] = s_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "352cd201",
      "metadata": {
        "id": "352cd201",
        "outputId": "8c84ee2c-9754-4ed6-faf7-099f99f277c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({0: [6],\n",
              "  1: [],\n",
              "  2: [],\n",
              "  3: [3],\n",
              "  4: [3],\n",
              "  5: [],\n",
              "  6: [3],\n",
              "  7: [],\n",
              "  8: [],\n",
              "  9: [3],\n",
              "  10: [6],\n",
              "  11: [3],\n",
              "  12: [],\n",
              "  13: [],\n",
              "  14: [],\n",
              "  15: [],\n",
              "  16: [],\n",
              "  17: [22],\n",
              "  18: [23],\n",
              "  19: [33],\n",
              "  20: [],\n",
              "  21: [],\n",
              "  22: [17, 53],\n",
              "  23: [13],\n",
              "  24: [],\n",
              "  25: [],\n",
              "  26: [],\n",
              "  27: [],\n",
              "  28: [],\n",
              "  29: [],\n",
              "  30: [2],\n",
              "  31: [],\n",
              "  32: [7],\n",
              "  33: [],\n",
              "  34: [3],\n",
              "  35: [],\n",
              "  36: [9],\n",
              "  37: [],\n",
              "  38: [],\n",
              "  39: [36],\n",
              "  40: [],\n",
              "  41: [8],\n",
              "  42: [],\n",
              "  43: [2],\n",
              "  44: [],\n",
              "  45: [8],\n",
              "  46: [],\n",
              "  47: [],\n",
              "  48: [11],\n",
              "  49: [27],\n",
              "  50: [],\n",
              "  51: [16],\n",
              "  52: [],\n",
              "  53: [],\n",
              "  54: [],\n",
              "  55: [26],\n",
              "  56: [5],\n",
              "  57: [],\n",
              "  58: [21],\n",
              "  59: [],\n",
              "  60: [2],\n",
              "  61: [],\n",
              "  62: [],\n",
              "  63: [3],\n",
              "  64: []},\n",
              " {0: [5],\n",
              "  1: [],\n",
              "  2: [],\n",
              "  3: [2],\n",
              "  4: [2],\n",
              "  5: [],\n",
              "  6: [2],\n",
              "  7: [],\n",
              "  8: [],\n",
              "  9: [2],\n",
              "  10: [5],\n",
              "  11: [2],\n",
              "  12: [],\n",
              "  13: [],\n",
              "  14: [],\n",
              "  15: [],\n",
              "  16: [],\n",
              "  17: [21],\n",
              "  18: [22],\n",
              "  19: [32],\n",
              "  20: [],\n",
              "  21: [],\n",
              "  22: [16, 52],\n",
              "  23: [12],\n",
              "  24: [],\n",
              "  25: [],\n",
              "  26: [],\n",
              "  27: [],\n",
              "  28: [],\n",
              "  29: [],\n",
              "  30: [1],\n",
              "  31: [],\n",
              "  32: [6],\n",
              "  33: [],\n",
              "  34: [2],\n",
              "  35: [],\n",
              "  36: [8],\n",
              "  37: [],\n",
              "  38: [],\n",
              "  39: [35],\n",
              "  40: [],\n",
              "  41: [7],\n",
              "  42: [],\n",
              "  43: [1],\n",
              "  44: [],\n",
              "  45: [7],\n",
              "  46: [],\n",
              "  47: [],\n",
              "  48: [10],\n",
              "  49: [26],\n",
              "  50: [],\n",
              "  51: [15],\n",
              "  52: [],\n",
              "  53: [],\n",
              "  54: [],\n",
              "  55: [25],\n",
              "  56: [4],\n",
              "  57: [],\n",
              "  58: [20],\n",
              "  59: [],\n",
              "  60: [1],\n",
              "  61: [],\n",
              "  62: [],\n",
              "  63: [2],\n",
              "  64: []})"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "where_is(\"##iden\",bert_sentence_ids),where_is(\"##iden\",roberta_sentence_ids),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e435c0",
      "metadata": {
        "id": "30e435c0",
        "outputId": "ac2d2e9d-8fb0-4b94-f63d-73b2277751b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.91460913 0.92009926 0.9304398 0.9241062 0.9202597 0.9177884 0.90847474 \n",
            " 0.84164673 0.784188 0.8110357 0.8553922 0.8009101 0.79874104 0.7772011\n"
          ]
        }
      ],
      "source": [
        "# compare BERT embeddings of ##iden\n",
        "print(cos_sim(bert_sum_embeddings[0][6],bert_sum_embeddings[3][3]),\\\n",
        "cos_sim(bert_sum_embeddings[0][6],bert_sum_embeddings[4][3]),\\\n",
        "cos_sim(bert_sum_embeddings[0][6],bert_sum_embeddings[6][3]),\\\n",
        "cos_sim(bert_sum_embeddings[0][6],bert_sum_embeddings[9][3]),\\\n",
        "cos_sim(bert_sum_embeddings[0][6],bert_sum_embeddings[10][6]),\\\n",
        "cos_sim(bert_sum_embeddings[0][6],bert_sum_embeddings[11][3]),\\\n",
        "cos_sim(bert_sum_embeddings[0][6],bert_sum_embeddings[17][22]),\"\\n\",\\\n",
        "# compare RoBERTa embeddings of ##iden\n",
        "cos_sim(roberta_sum_embeddings[0][6],roberta_sum_embeddings[3][5]),\\\n",
        "cos_sim(roberta_sum_embeddings[0][6],roberta_sum_embeddings[4][5]),\\\n",
        "cos_sim(roberta_sum_embeddings[0][6],roberta_sum_embeddings[6][5]),\\\n",
        "cos_sim(roberta_sum_embeddings[0][6],roberta_sum_embeddings[9][5]),\\\n",
        "cos_sim(roberta_sum_embeddings[0][6],roberta_sum_embeddings[10][8]),\\\n",
        "cos_sim(roberta_sum_embeddings[0][6],roberta_sum_embeddings[11][5]),\\\n",
        "cos_sim(roberta_sum_embeddings[0][6],roberta_sum_embeddings[17][24]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1cbf802",
      "metadata": {
        "id": "c1cbf802"
      },
      "source": [
        "If I were using the RoBERTa tokenizer to get RoBERTa embeddings, the RoBERTa embeddings would actually be a lot more similar to each other (~.97).  But BERT and RoBERTa tokenize differently (e.g., RoBERTa keeps \"Biden\" as a whole word), and in this case I want them to be directly comparable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f2f2ce",
      "metadata": {
        "id": "29f2f2ce",
        "outputId": "9d704db2-255b-40fa-c1ae-2a0cbd2ca43d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: [],\n",
              " 1: [],\n",
              " 2: [],\n",
              " 3: [],\n",
              " 4: [9],\n",
              " 5: [],\n",
              " 6: [],\n",
              " 7: [],\n",
              " 8: [22],\n",
              " 9: [],\n",
              " 10: [12, 50],\n",
              " 11: [],\n",
              " 12: [],\n",
              " 13: [],\n",
              " 14: [25],\n",
              " 15: [],\n",
              " 16: [],\n",
              " 17: [],\n",
              " 18: [],\n",
              " 19: [],\n",
              " 20: [],\n",
              " 21: [],\n",
              " 22: [],\n",
              " 23: [],\n",
              " 24: [],\n",
              " 25: [],\n",
              " 26: [],\n",
              " 27: [],\n",
              " 28: [],\n",
              " 29: [],\n",
              " 30: [],\n",
              " 31: [],\n",
              " 32: [26],\n",
              " 33: [],\n",
              " 34: [],\n",
              " 35: [],\n",
              " 36: [],\n",
              " 37: [21],\n",
              " 38: [],\n",
              " 39: [],\n",
              " 40: [],\n",
              " 41: [],\n",
              " 42: [],\n",
              " 43: [],\n",
              " 44: [],\n",
              " 45: [],\n",
              " 46: [],\n",
              " 47: [],\n",
              " 48: [],\n",
              " 49: [],\n",
              " 50: [],\n",
              " 51: [],\n",
              " 52: [],\n",
              " 53: [],\n",
              " 54: [],\n",
              " 55: [],\n",
              " 56: [],\n",
              " 57: [],\n",
              " 58: [],\n",
              " 59: [],\n",
              " 60: [],\n",
              " 61: [],\n",
              " 62: [],\n",
              " 63: [],\n",
              " 64: []}"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "where_is(\"United\",bert_sentence_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac61783a",
      "metadata": {
        "id": "ac61783a",
        "outputId": "0281ca09-833a-4fd9-89f1-369e6fc573e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.9290528, 0.9420772, 0.9309909, 0.9294519)"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compare BERT embeddings of \"United\"\n",
        "cos_sim(bert_sum_embeddings[4][9],bert_sum_embeddings[8][22]),\\\n",
        "cos_sim(bert_sum_embeddings[4][9],bert_sum_embeddings[10][12]),\\\n",
        "cos_sim(bert_sum_embeddings[4][9],bert_sum_embeddings[10][50]),\\\n",
        "cos_sim(bert_sum_embeddings[4][9],bert_sum_embeddings[14][25])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ad5e3fd",
      "metadata": {
        "id": "5ad5e3fd"
      },
      "source": [
        "I don't want to choose an arbitary instance as the \"default\" embedding for a term. I'd much rather compare individual instances to a single stable representation.\n",
        "\n",
        "A robust approach could use PCA to find this, but if you use only the first principle component for a set of vectors, you end up very close to the mean, so here we'll just take the average.\n",
        "\n",
        "First, I will find all instancs of a term using the `where_is` function, get contextualized embeddings for every one of those, and then take the mean of those vectors so I have something separate to compare each vector to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bf5876",
      "metadata": {
        "id": "e1bf5876"
      },
      "outputs": [],
      "source": [
        "def get_all_instances(token, model=\"bert\"):\n",
        "    if model == \"bert\":\n",
        "        occurrences = where_is(token,bert_sentence_ids)\n",
        "    elif model == \"roberta\":\n",
        "        occurrences = where_is(token,roberta_sentence_ids)\n",
        "    instances = []\n",
        "    for sid in occurrences:\n",
        "        if occurrences[sid] != []:\n",
        "            instances += list(product([sid],occurrences[sid]))\n",
        "    return instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e27fc3c",
      "metadata": {
        "id": "2e27fc3c"
      },
      "outputs": [],
      "source": [
        "def get_contextual_embeddings(token, model=\"bert\", emb_type=\"sum\", inv=False):\n",
        "    instances = get_all_instances(token, model)\n",
        "\n",
        "    # can get the inverse of the vector too if I want; for later\n",
        "    inv_multiplier = -1 if inv else 1\n",
        "\n",
        "    context_embeddings = []\n",
        "    for instance in instances:\n",
        "        if model == \"bert\":\n",
        "            if emb_type == \"sum\":\n",
        "                context_embeddings.append(inv_multiplier*bert_sum_embeddings[instance[0]][instance[1]].numpy())\n",
        "            elif emb_type == \"cat\":\n",
        "                context_embeddings.append(inv_multiplier*bert_cat_embeddings[instance[0]][instance[1]].numpy())\n",
        "        elif model == \"roberta\":\n",
        "            if emb_type == \"sum\":\n",
        "                context_embeddings.append(inv_multiplier*roberta_sum_embeddings[instance[0]][instance[1]].numpy())\n",
        "            elif emb_type == \"cat\":\n",
        "                context_embeddings.append(inv_multiplier*roberta_cat_embeddings[instance[0]][instance[1]].numpy())\n",
        "\n",
        "    return context_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba27460",
      "metadata": {
        "id": "3ba27460"
      },
      "outputs": [],
      "source": [
        "def mean_emb(token, model=\"bert\", emb_type=\"sum\"):\n",
        "    context_embeddings = get_contextual_embeddings(token,model,emb_type)\n",
        "\n",
        "    return np.mean(np.array(context_embeddings), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd589e54",
      "metadata": {
        "id": "fd589e54"
      },
      "outputs": [],
      "source": [
        "mean = mean_emb(\"United\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e50aa3",
      "metadata": {
        "id": "17e50aa3"
      },
      "source": [
        "Compare all \"United\" embeddings to the mean \"United\" embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d93c4aa",
      "metadata": {
        "id": "8d93c4aa",
        "outputId": "8d921187-80bf-4659-e4bf-c99b7b6eadfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9719128\n",
            "0.9647045\n",
            "0.959468\n",
            "0.95756286\n",
            "0.9653022\n",
            "0.931456\n",
            "0.9415491\n"
          ]
        }
      ],
      "source": [
        "for instance in get_all_instances(\"United\"):\n",
        "    print(cos_sim(mean,bert_sum_embeddings[instance[0]][instance[1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd073f46",
      "metadata": {
        "id": "dd073f46"
      },
      "source": [
        "Pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123658d0",
      "metadata": {
        "id": "123658d0"
      },
      "source": [
        "Now I'll wrap all of the above in a single helper function.  I can choose which model I want, BERT or RoBERTa, and how I want to represent the embedding, as a sum or concatenation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7e2afb",
      "metadata": {
        "id": "7a7e2afb"
      },
      "outputs": [],
      "source": [
        "def sim_to_mean(token, model=\"bert\", emb_type=\"sum\"):\n",
        "    instances = get_all_instances(token, model)\n",
        "\n",
        "    mean = mean_emb(token, model, emb_type)\n",
        "\n",
        "    for instance in instances:\n",
        "        if model == \"bert\":\n",
        "            if emb_type == \"sum\":\n",
        "                print(cos_sim(mean,bert_sum_embeddings[instance[0]][instance[1]]))\n",
        "            elif emb_type == \"cat\":\n",
        "                print(cos_sim(mean,bert_cat_embeddings[instance[0]][instance[1]]))\n",
        "        elif model == \"roberta\":\n",
        "            if emb_type == \"sum\":\n",
        "                print(cos_sim(mean,roberta_sum_embeddings[instance[0]][instance[1]]))\n",
        "            elif emb_type == \"cat\":\n",
        "                print(cos_sim(mean,roberta_cat_embeddings[instance[0]][instance[1]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85cc4640",
      "metadata": {
        "id": "85cc4640",
        "outputId": "80c896a5-46d7-429f-a3e6-b4e8e2f50216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9719128\n",
            "0.9647045\n",
            "0.959468\n",
            "0.95756286\n",
            "0.9653022\n",
            "0.931456\n",
            "0.9415491\n"
          ]
        }
      ],
      "source": [
        "sim_to_mean(\"United\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e93bc0",
      "metadata": {
        "id": "f3e93bc0",
        "outputId": "3725a018-0e03-472f-8ca9-a75aeff84fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.92461336\n",
            "0.9723553\n",
            "0.9572589\n",
            "0.96328247\n",
            "0.95900244\n",
            "0.9421863\n",
            "0.9695808\n",
            "0.9623033\n",
            "0.93271834\n",
            "0.9140068\n",
            "0.9552182\n",
            "0.9459734\n",
            "0.9745208\n",
            "0.97186553\n",
            "0.9610865\n",
            "0.96674526\n",
            "0.97269666\n",
            "0.9328612\n",
            "0.96595454\n",
            "0.9662421\n",
            "0.9664111\n",
            "0.9426807\n",
            "0.95189667\n",
            "0.9635226\n",
            "0.90105516\n",
            "0.8843179\n",
            "0.9553506\n",
            "0.96678907\n",
            "0.9555821\n"
          ]
        }
      ],
      "source": [
        "sim_to_mean(\"##iden\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32bc22d8",
      "metadata": {
        "id": "32bc22d8",
        "outputId": "31cadd84-dde4-41c8-8204-303fb10b21e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8241619\n",
            "0.9437714\n",
            "0.8353652\n",
            "0.8468057\n",
            "0.93103707\n",
            "0.9438541\n",
            "0.89052725\n",
            "0.9605425\n",
            "0.9657128\n",
            "0.89657295\n",
            "0.8295743\n",
            "0.95314676\n",
            "0.9503279\n",
            "0.9419902\n",
            "0.9388308\n",
            "0.95990366\n",
            "0.9616684\n",
            "0.9480257\n",
            "0.96647745\n"
          ]
        }
      ],
      "source": [
        "sim_to_mean(\"Xi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522ccdba",
      "metadata": {
        "id": "522ccdba",
        "outputId": "cf2ca036-1b3c-440c-c592-d43f9c0a0996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.88467735\n",
            "0.88141763\n",
            "0.9558845\n",
            "0.961143\n",
            "0.95358026\n",
            "0.8853544\n",
            "0.9538163\n",
            "0.95456153\n",
            "0.9167086\n",
            "0.9487444\n",
            "0.9481702\n",
            "0.9454616\n"
          ]
        }
      ],
      "source": [
        "sim_to_mean(\"China\",emb_type=\"sum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ee739c",
      "metadata": {
        "id": "f1ee739c",
        "outputId": "6458f397-e176-4287-976d-6fd56b1d4a36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.70051956\n",
            "0.62334305\n",
            "0.87728256\n",
            "0.8430073\n",
            "0.79919314\n",
            "0.8027341\n",
            "0.7815922\n",
            "0.8469708\n",
            "0.7490677\n",
            "0.8300163\n",
            "0.733182\n",
            "0.8621789\n",
            "0.8045163\n",
            "0.8634441\n",
            "0.8613811\n",
            "0.8105525\n",
            "0.8704169\n",
            "0.7515808\n",
            "0.74817795\n",
            "0.7633075\n",
            "0.70917803\n",
            "0.73050004\n",
            "0.7773533\n",
            "0.7182495\n",
            "0.77823085\n",
            "0.77277875\n",
            "0.76600134\n",
            "0.7859439\n",
            "0.8093319\n",
            "0.7997593\n",
            "0.7486369\n",
            "0.87380826\n",
            "0.7770131\n",
            "0.84049505\n",
            "0.8230272\n",
            "0.7598989\n",
            "0.8235913\n",
            "0.84415495\n",
            "0.6986584\n",
            "0.74573195\n",
            "0.84185857\n",
            "0.8241085\n",
            "0.84988445\n",
            "0.75655305\n",
            "0.7709929\n",
            "0.80323595\n",
            "0.76833653\n",
            "0.8240651\n",
            "0.7328832\n",
            "0.8269663\n",
            "0.793611\n"
          ]
        }
      ],
      "source": [
        "# a common word like \"of\" should not define a very narrow subspace\n",
        "sim_to_mean(\"of\",emb_type=\"cat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1583d00b",
      "metadata": {
        "id": "1583d00b",
        "outputId": "70c0ec43-3b3f-4e0f-ba7c-4bfef3dba19a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6375637"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ##iden is not very similar to Xi\n",
        "cos_sim(mean_emb(\"##iden\"),mean_emb(\"Xi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435775eb",
      "metadata": {
        "id": "435775eb",
        "outputId": "1a1a9bf9-9511-42e8-9260-f3439e117a7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9063593"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# United is very similar to States\n",
        "cos_sim(mean_emb(\"United\"),mean_emb(\"States\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01767bf8",
      "metadata": {
        "id": "01767bf8",
        "outputId": "bd49492c-7add-4152-f324-6a93a09914c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.78541607"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cos_sim(mean_emb(\"States\"),mean_emb(\"China\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d578d925",
      "metadata": {
        "id": "d578d925",
        "outputId": "12dadc3f-44ac-4016-9995-9053f7400d9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.61760527, 0.40623903)"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Joseph is at least more similar to ##iden than Xi is to Jin\n",
        "cos_sim(mean_emb(\"Joseph\"),mean_emb(\"##iden\")),cos_sim(mean_emb(\"Xi\"),mean_emb(\"Jin\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dfe3979",
      "metadata": {
        "id": "2dfe3979",
        "outputId": "4504e18b-fb59-4f56-a654-f07081739ff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7115031, 0.7162513)"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cos_sim(mean_emb(\"President\"),mean_emb(\"##iden\")),cos_sim(mean_emb(\"President\"),mean_emb(\"Xi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16c5d59",
      "metadata": {
        "id": "c16c5d59",
        "outputId": "4e914e87-ba44-4060-8f5e-f3ed3087500f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.6284444, 0.6032391)"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ##iden is not much more similar to United than he is to China; might be dealing with some MWE issues here\n",
        "cos_sim(mean_emb(\"##iden\"),mean_emb(\"United\")),cos_sim(mean_emb(\"##iden\"),mean_emb(\"China\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6efddb66",
      "metadata": {
        "id": "6efddb66",
        "outputId": "945099ed-ef89-40a2-e2f0-bb3aa7ae151a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.6523542, 0.848916)"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Xi is quite similar to China\n",
        "cos_sim(mean_emb(\"Xi\"),mean_emb(\"United\")),cos_sim(mean_emb(\"Xi\"),mean_emb(\"China\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c80755cc",
      "metadata": {
        "id": "c80755cc",
        "outputId": "bb6fbfa8-d629-46ec-8c0a-76031d8e2d35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.32803056, 0.5568467)"
            ]
          },
          "execution_count": 177,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# of course Joseph is more similar to China than Xi is to a period\n",
        "cos_sim(mean_emb(\"Xi\"),mean_emb(\".\")),cos_sim(mean_emb(\"Joseph\"),mean_emb(\"China\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d18bcdb",
      "metadata": {
        "id": "0d18bcdb",
        "outputId": "024d4e8f-dee1-49d7-d709-616188077bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT:\n",
            "0.92461336\n",
            "0.9723553\n",
            "0.9572589\n",
            "0.96328247\n",
            "0.95900244\n",
            "0.9421863\n",
            "0.9695808\n",
            "0.9623033\n",
            "0.93271834\n",
            "0.9140068\n",
            "0.9552182\n",
            "0.9459734\n",
            "0.9745208\n",
            "0.97186553\n",
            "0.9610865\n",
            "0.96674526\n",
            "0.97269666\n",
            "0.9328612\n",
            "0.96595454\n",
            "0.9662421\n",
            "0.9664111\n",
            "0.9426807\n",
            "0.95189667\n",
            "0.9635226\n",
            "0.90105516\n",
            "0.8843179\n",
            "0.9553506\n",
            "0.96678907\n",
            "0.9555821\n",
            "\n",
            "RoBERTa:\n",
            "0.960526\n",
            "0.9683521\n",
            "0.966242\n",
            "0.95562327\n",
            "0.95661134\n",
            "0.9604931\n",
            "0.9662562\n",
            "0.961686\n",
            "0.94541365\n",
            "0.9570515\n",
            "0.91226876\n",
            "0.88853544\n",
            "0.96050996\n",
            "0.87953126\n",
            "0.9536133\n",
            "0.95463395\n",
            "0.9568807\n",
            "0.942709\n",
            "0.94543827\n",
            "0.8089897\n",
            "0.95956546\n",
            "0.956536\n",
            "0.95420426\n",
            "0.96062356\n",
            "0.9355163\n",
            "0.9645269\n",
            "0.80249476\n",
            "0.9117955\n",
            "0.95476866\n"
          ]
        }
      ],
      "source": [
        "# compare BERT and RoBERTa embeddings for ##iden\n",
        "print(\"BERT:\")\n",
        "sim_to_mean(\"##iden\")\n",
        "print(\"\\nRoBERTa:\")\n",
        "sim_to_mean(\"##iden\", model=\"roberta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d28e0d",
      "metadata": {
        "id": "e2d28e0d",
        "outputId": "e8f5908b-2df0-4b47-9f44-f9a73039cc94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT:\n",
            "0.8241619\n",
            "0.9437714\n",
            "0.8353652\n",
            "0.8468057\n",
            "0.93103707\n",
            "0.9438541\n",
            "0.89052725\n",
            "0.9605425\n",
            "0.9657128\n",
            "0.89657295\n",
            "0.8295743\n",
            "0.95314676\n",
            "0.9503279\n",
            "0.9419902\n",
            "0.9388308\n",
            "0.95990366\n",
            "0.9616684\n",
            "0.9480257\n",
            "0.96647745\n",
            "\n",
            "RoBERTa:\n",
            "0.94226915\n",
            "0.92813927\n",
            "0.95794165\n",
            "0.9557436\n",
            "0.92567503\n",
            "0.93396616\n",
            "0.9433925\n",
            "0.9418549\n",
            "0.83810735\n",
            "0.9177662\n",
            "0.9484554\n",
            "0.9482459\n",
            "0.9258455\n",
            "0.9413105\n",
            "0.8888482\n",
            "0.9469824\n",
            "0.9135624\n",
            "0.8649158\n",
            "0.89532006\n"
          ]
        }
      ],
      "source": [
        "# and for Xi\n",
        "print(\"BERT:\")\n",
        "sim_to_mean(\"Xi\")\n",
        "print(\"\\nRoBERTa:\")\n",
        "sim_to_mean(\"Xi\", model=\"roberta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3df40e9",
      "metadata": {
        "id": "b3df40e9",
        "outputId": "47070abf-b509-4f30-e3d4-bb18ac12d8ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-0.017536873, -0.020948494)"
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now mean token embeddings across models\n",
        "cos_sim(mean_emb(\"##iden\"),mean_emb(\"##iden\", model=\"roberta\")),cos_sim(mean_emb(\"Xi\"),mean_emb(\"Xi\", model=\"roberta\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3688a936",
      "metadata": {
        "id": "3688a936"
      },
      "source": [
        "**Not directly comparable!!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}